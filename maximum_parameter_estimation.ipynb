{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qai_hub as hub\n",
    "import torch\n",
    "from torchvision.models import mobilenet_v2\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchviz\n",
    "import torchsummary\n",
    "\n",
    "# Creates a simple scalable CNN model to test the maximum amount of RAM and layers supported by the QAI Hub\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, imgRes, convChannels, fcChannels):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # Assuming 3 input channels (RGB images)\n",
    "\n",
    "        imgWidth, imgHeight = imgRes\n",
    "\n",
    "        inChannels = 3\n",
    "\n",
    "        # Creates convolutional layers with the number of channels specified in the convChannels list\n",
    "\n",
    "        self.convLayers = []\n",
    "\n",
    "        for convLayer in convChannels:\n",
    "            self.convLayers.append(nn.Conv2d(in_channels=inChannels, out_channels=convLayer, kernel_size=3, stride=1, padding=1))\n",
    "            inChannels = convLayer\n",
    "\n",
    "        #self.convLayers = nn.ModuleList(convLayers)\n",
    "\n",
    "        self.convLayers = nn.Sequential(*self.convLayers)\n",
    "\n",
    "        self.fcLayers = []\n",
    "\n",
    "        # Creates fully connected layers with the number of neurons specified in the fcChannels list\n",
    "\n",
    "        inNeurons = convChannels[-1] * imgWidth * imgHeight\n",
    "\n",
    "        for fcLayer in fcChannels:\n",
    "            self.fcLayers.append(nn.Linear(inNeurons, fcLayer))\n",
    "            self.fcLayers.append(nn.ReLU())\n",
    "            inNeurons = fcLayer\n",
    "\n",
    "        self.fcLayers = nn.Sequential(*self.fcLayers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convLayers(x)\n",
    "\n",
    "        # Reshapes the output of the convolutional layers to be fed into the fully connected layers\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.fcLayers(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Instantiate model\n",
    "\n",
    "imgRes = [224, 224]\n",
    "convChannels = [16, 32, 64, 64, 32, 16]\n",
    "fcChannels = [64]\n",
    "\n",
    "\n",
    "model = SimpleCNN(imgRes, convChannels, fcChannels)\n",
    "\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (convLayers): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (fcLayers): Sequential(\n",
      "    (0): Linear(in_features=802816, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 224, 224]             448\n",
      "            Conv2d-2         [-1, 32, 224, 224]           4,640\n",
      "            Conv2d-3         [-1, 64, 224, 224]          18,496\n",
      "            Conv2d-4         [-1, 64, 224, 224]          36,928\n",
      "            Conv2d-5         [-1, 32, 224, 224]          18,464\n",
      "            Conv2d-6         [-1, 16, 224, 224]           4,624\n",
      "            Linear-7                   [-1, 64]      51,380,288\n",
      "              ReLU-8                   [-1, 64]               0\n",
      "================================================================\n",
      "Total params: 51,463,888\n",
      "Trainable params: 51,463,888\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 85.75\n",
      "Params size (MB): 196.32\n",
      "Estimated Total Size (MB): 282.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "\n",
    "summary = torchsummary.summary(model, (3, imgRes[0], imgRes[1]))\n",
    "\n",
    "# Print model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.container.Sequential'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(model.convLayers))\n",
    "\n",
    "model.convLayers[0].weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves model to file\n",
    "\n",
    "input_shape = (1, 3, 224, 224)\n",
    "example_input = torch.rand(input_shape)\n",
    "traced_torch_model = torch.jit.trace(model, example_input)\n",
    "torch.jit.save(traced_torch_model, 'testModel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading tmp282dmnsz.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m██████████\u001b[0m| 196M/196M [02:47<00:00, 1.23MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (j5q06v14p) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/j5q06v14p/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Compile model\n",
    "compile_job = hub.submit_compile_job(\n",
    "    model=traced_torch_model,\n",
    "    device=hub.Device(\"Snapdragon 8 Elite QRD\"),\n",
    "    input_specs=dict(image=input_shape),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled profile job (jgl4vl885) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jgl4vl885/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_model = compile_job.get_target_model()\n",
    "\n",
    "profile_job = hub.submit_profile_job(\n",
    "    model=target_model,\n",
    "    device=hub.Device(\"Snapdragon 8 Elite QRD\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading dataset: 154kB [00:01, 144kB/s]                             872kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled inference job (jp3nj67l5) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jp3nj67l5/\n",
      "\n",
      "Waiting for inference job (jp3nj67l5) completion. Type Ctrl+C to stop waiting at any time.\n",
      "    ✅ SUCCESS                          \u0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tmp90ivxp7d.h5: 100%|\u001b[34m██████████\u001b[0m| 13.9k/13.9k [00:00<00:00, 36.1kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Run inference on cloud-hosted device\n",
    "sample_image_url = (\n",
    "    \"https://qaihub-public-assets.s3.us-west-2.amazonaws.com/apidoc/input_image1.jpg\"\n",
    ")\n",
    "response = requests.get(sample_image_url, stream=True)\n",
    "response.raw.decode_content = True\n",
    "image = Image.open(response.raw).resize((224, 224))\n",
    "input_array = np.expand_dims(\n",
    "    np.transpose(np.array(image, dtype=np.float32) / 255.0, (2, 0, 1)), axis=0\n",
    ")\n",
    "\n",
    "# Run inference using the on-device model on the input image\n",
    "inference_job = hub.submit_inference_job(\n",
    "    model=target_model,\n",
    "    device=hub.Device(\"Snapdragon 8 Elite QRD\"),\n",
    "    inputs=dict(image=[input_array]),\n",
    ")\n",
    "on_device_output = inference_job.download_output_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
