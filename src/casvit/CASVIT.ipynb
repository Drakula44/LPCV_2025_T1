{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sveska za importovanje i pravljenje jobova za CAS-VIT mrežu\n",
    "\n",
    "CAS-ViT se zasniva na Visual Transformer arhitekturi, više informacija na linku.\n",
    "\n",
    "https://paperswithcode.com/paper/cas-vit-convolutional-additive-self-attention\n",
    "\n",
    "(samo sam sortirao po broju parametara i uzeo ovu random mrezu, nemam pojma da li je dobra, i rad za sada ne razumem)\n",
    "\n",
    "Ova sveska uz prateće fajlove automatski skida weightove za CAS-ViT model zadate veličine, i posle taj model šalje na AIHUB za testiranje.\n",
    "\n",
    "Za sada sam pokrenuo samo xs model, koji se izvršava super brzo i ima manje parametara od MobileNetV2, doduše mnogo više layera koji su raspoređeni na više COMPUTE unita, što je koliko ja razumem dobro.\n",
    "\n",
    "Ne znam da li je dobro da modele čuvamo na GIT-u (Drakula neka kaže kako bi to trebalo), pa sam sve fajlove koje se generišu stavio u gitignore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/centar15-desktop1/LPCV_2025_T1/.venv/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/centar15-desktop1/LPCV_2025_T1/.venv/lib/python3.12/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "2025-02-13 16:17:54.890061: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-13 16:17:54.897588: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739459874.906371  261751 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739459874.909005  261751 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-13 16:17:54.919377: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "import torch\n",
    "import rcvit\n",
    "import torchsummary\n",
    "import os\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(ROOT_DIR)\n",
    "from utils import helper, input_getter, qai_hub_jobs, tfhelper\n",
    "\n",
    "import skimage as ski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CASVIT_s.pth already exists.\n"
     ]
    }
   ],
   "source": [
    "# Model selection (options: \"xs\", \"s\", \"m\", \"t\")\n",
    "\n",
    "modelID = \"s\"\n",
    "\n",
    "driveFileIDs = dict()\n",
    "\n",
    "driveFileIDs[\"xs\"] = \"16wKcwF6QMW5w_lyPYnDKjMNuoxQDfrLK\"\n",
    "driveFileIDs[\"s\"]  = \"1facFRq8s8oelYUtK1fj3fcfdoWoKDBQQ\"\n",
    "driveFileIDs[\"m\"]  = \"13sQpSEf0h_uuh0jRy9V0yIW6ZsbDpVGy\"\n",
    "driveFileIDs[\"t\"]  = \"1NqoIUPbwBC91RTjTUvubAbOfGqo1VYT0\"\n",
    "\n",
    "networks = dict()\n",
    "networks[\"xs\"] = rcvit.rcvit_xs\n",
    "networks[\"s\"]  = rcvit.rcvit_s\n",
    "networks[\"m\"]  = rcvit.rcvit_m\n",
    "networks[\"t\"]  = rcvit.rcvit_t\n",
    "\n",
    "file_id = driveFileIDs[modelID]\n",
    "\n",
    "modelName = \"CASVIT_{}\".format(modelID)\n",
    "\n",
    "file_path = modelName + \".pth\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", file_path, quiet=False)\n",
    "    print(\"Model downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"File {file_path} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RCViT(\n",
      "  (patch_embed): Sequential(\n",
      "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (network): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (norm1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "                (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(48, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "                (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(48, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "          (proj): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (norm1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "                (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(48, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "                (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(48, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "          (proj): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (norm1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "                (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(48, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "                (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(48, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "          (proj): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Embedding(\n",
      "      (proj): Conv2d(48, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Embedding(\n",
      "      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Embedding(\n",
      "      (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "            (3): GELU(approximate='none')\n",
      "            (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU(inplace=True)\n",
      "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (4): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (2): Sigmoid()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (head): Linear(in_features=256, out_features=1000, bias=True)\n",
      "  (dist_head): Linear(in_features=256, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net: rcvit.RCViT = networks[modelID]()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 112, 112]             672\n",
      "       BatchNorm2d-2         [-1, 24, 112, 112]              48\n",
      "              ReLU-3         [-1, 24, 112, 112]               0\n",
      "            Conv2d-4           [-1, 48, 56, 56]          10,416\n",
      "       BatchNorm2d-5           [-1, 48, 56, 56]              96\n",
      "              ReLU-6           [-1, 48, 56, 56]               0\n",
      "            Conv2d-7           [-1, 48, 56, 56]           2,352\n",
      "       BatchNorm2d-8           [-1, 48, 56, 56]              96\n",
      "            Conv2d-9           [-1, 48, 56, 56]             480\n",
      "             GELU-10           [-1, 48, 56, 56]               0\n",
      "           Conv2d-11           [-1, 48, 56, 56]           2,352\n",
      " LocalIntegration-12           [-1, 48, 56, 56]               0\n",
      "      BatchNorm2d-13           [-1, 48, 56, 56]              96\n",
      "           Conv2d-14          [-1, 144, 56, 56]           6,912\n",
      "           Conv2d-15           [-1, 48, 56, 56]             480\n",
      "      BatchNorm2d-16           [-1, 48, 56, 56]              96\n",
      "             ReLU-17           [-1, 48, 56, 56]               0\n",
      "           Conv2d-18            [-1, 1, 56, 56]              48\n",
      "          Sigmoid-19            [-1, 1, 56, 56]               0\n",
      " SpatialOperation-20           [-1, 48, 56, 56]               0\n",
      "AdaptiveAvgPool2d-21             [-1, 48, 1, 1]               0\n",
      "           Conv2d-22             [-1, 48, 1, 1]           2,304\n",
      "          Sigmoid-23             [-1, 48, 1, 1]               0\n",
      " ChannelOperation-24           [-1, 48, 56, 56]               0\n",
      "           Conv2d-25           [-1, 48, 56, 56]             480\n",
      "      BatchNorm2d-26           [-1, 48, 56, 56]              96\n",
      "             ReLU-27           [-1, 48, 56, 56]               0\n",
      "           Conv2d-28            [-1, 1, 56, 56]              48\n",
      "          Sigmoid-29            [-1, 1, 56, 56]               0\n",
      " SpatialOperation-30           [-1, 48, 56, 56]               0\n",
      "AdaptiveAvgPool2d-31             [-1, 48, 1, 1]               0\n",
      "           Conv2d-32             [-1, 48, 1, 1]           2,304\n",
      "          Sigmoid-33             [-1, 48, 1, 1]               0\n",
      " ChannelOperation-34           [-1, 48, 56, 56]               0\n",
      "           Conv2d-35           [-1, 48, 56, 56]             480\n",
      "           Conv2d-36           [-1, 48, 56, 56]             480\n",
      "          Dropout-37           [-1, 48, 56, 56]               0\n",
      "AdditiveTokenMixer-38           [-1, 48, 56, 56]               0\n",
      "         Identity-39           [-1, 48, 56, 56]               0\n",
      "      BatchNorm2d-40           [-1, 48, 56, 56]              96\n",
      "           Conv2d-41          [-1, 192, 56, 56]           9,408\n",
      "             GELU-42          [-1, 192, 56, 56]               0\n",
      "          Dropout-43          [-1, 192, 56, 56]               0\n",
      "           Conv2d-44           [-1, 48, 56, 56]           9,264\n",
      "          Dropout-45           [-1, 48, 56, 56]               0\n",
      "              Mlp-46           [-1, 48, 56, 56]               0\n",
      "         Identity-47           [-1, 48, 56, 56]               0\n",
      "    AdditiveBlock-48           [-1, 48, 56, 56]               0\n",
      "           Conv2d-49           [-1, 48, 56, 56]           2,352\n",
      "      BatchNorm2d-50           [-1, 48, 56, 56]              96\n",
      "           Conv2d-51           [-1, 48, 56, 56]             480\n",
      "             GELU-52           [-1, 48, 56, 56]               0\n",
      "           Conv2d-53           [-1, 48, 56, 56]           2,352\n",
      " LocalIntegration-54           [-1, 48, 56, 56]               0\n",
      "      BatchNorm2d-55           [-1, 48, 56, 56]              96\n",
      "           Conv2d-56          [-1, 144, 56, 56]           6,912\n",
      "           Conv2d-57           [-1, 48, 56, 56]             480\n",
      "      BatchNorm2d-58           [-1, 48, 56, 56]              96\n",
      "             ReLU-59           [-1, 48, 56, 56]               0\n",
      "           Conv2d-60            [-1, 1, 56, 56]              48\n",
      "          Sigmoid-61            [-1, 1, 56, 56]               0\n",
      " SpatialOperation-62           [-1, 48, 56, 56]               0\n",
      "AdaptiveAvgPool2d-63             [-1, 48, 1, 1]               0\n",
      "           Conv2d-64             [-1, 48, 1, 1]           2,304\n",
      "          Sigmoid-65             [-1, 48, 1, 1]               0\n",
      " ChannelOperation-66           [-1, 48, 56, 56]               0\n",
      "           Conv2d-67           [-1, 48, 56, 56]             480\n",
      "      BatchNorm2d-68           [-1, 48, 56, 56]              96\n",
      "             ReLU-69           [-1, 48, 56, 56]               0\n",
      "           Conv2d-70            [-1, 1, 56, 56]              48\n",
      "          Sigmoid-71            [-1, 1, 56, 56]               0\n",
      " SpatialOperation-72           [-1, 48, 56, 56]               0\n",
      "AdaptiveAvgPool2d-73             [-1, 48, 1, 1]               0\n",
      "           Conv2d-74             [-1, 48, 1, 1]           2,304\n",
      "          Sigmoid-75             [-1, 48, 1, 1]               0\n",
      " ChannelOperation-76           [-1, 48, 56, 56]               0\n",
      "           Conv2d-77           [-1, 48, 56, 56]             480\n",
      "           Conv2d-78           [-1, 48, 56, 56]             480\n",
      "          Dropout-79           [-1, 48, 56, 56]               0\n",
      "AdditiveTokenMixer-80           [-1, 48, 56, 56]               0\n",
      "         Identity-81           [-1, 48, 56, 56]               0\n",
      "      BatchNorm2d-82           [-1, 48, 56, 56]              96\n",
      "           Conv2d-83          [-1, 192, 56, 56]           9,408\n",
      "             GELU-84          [-1, 192, 56, 56]               0\n",
      "          Dropout-85          [-1, 192, 56, 56]               0\n",
      "           Conv2d-86           [-1, 48, 56, 56]           9,264\n",
      "          Dropout-87           [-1, 48, 56, 56]               0\n",
      "              Mlp-88           [-1, 48, 56, 56]               0\n",
      "         Identity-89           [-1, 48, 56, 56]               0\n",
      "    AdditiveBlock-90           [-1, 48, 56, 56]               0\n",
      "           Conv2d-91           [-1, 48, 56, 56]           2,352\n",
      "      BatchNorm2d-92           [-1, 48, 56, 56]              96\n",
      "           Conv2d-93           [-1, 48, 56, 56]             480\n",
      "             GELU-94           [-1, 48, 56, 56]               0\n",
      "           Conv2d-95           [-1, 48, 56, 56]           2,352\n",
      " LocalIntegration-96           [-1, 48, 56, 56]               0\n",
      "      BatchNorm2d-97           [-1, 48, 56, 56]              96\n",
      "           Conv2d-98          [-1, 144, 56, 56]           6,912\n",
      "           Conv2d-99           [-1, 48, 56, 56]             480\n",
      "     BatchNorm2d-100           [-1, 48, 56, 56]              96\n",
      "            ReLU-101           [-1, 48, 56, 56]               0\n",
      "          Conv2d-102            [-1, 1, 56, 56]              48\n",
      "         Sigmoid-103            [-1, 1, 56, 56]               0\n",
      "SpatialOperation-104           [-1, 48, 56, 56]               0\n",
      "AdaptiveAvgPool2d-105             [-1, 48, 1, 1]               0\n",
      "          Conv2d-106             [-1, 48, 1, 1]           2,304\n",
      "         Sigmoid-107             [-1, 48, 1, 1]               0\n",
      "ChannelOperation-108           [-1, 48, 56, 56]               0\n",
      "          Conv2d-109           [-1, 48, 56, 56]             480\n",
      "     BatchNorm2d-110           [-1, 48, 56, 56]              96\n",
      "            ReLU-111           [-1, 48, 56, 56]               0\n",
      "          Conv2d-112            [-1, 1, 56, 56]              48\n",
      "         Sigmoid-113            [-1, 1, 56, 56]               0\n",
      "SpatialOperation-114           [-1, 48, 56, 56]               0\n",
      "AdaptiveAvgPool2d-115             [-1, 48, 1, 1]               0\n",
      "          Conv2d-116             [-1, 48, 1, 1]           2,304\n",
      "         Sigmoid-117             [-1, 48, 1, 1]               0\n",
      "ChannelOperation-118           [-1, 48, 56, 56]               0\n",
      "          Conv2d-119           [-1, 48, 56, 56]             480\n",
      "          Conv2d-120           [-1, 48, 56, 56]             480\n",
      "         Dropout-121           [-1, 48, 56, 56]               0\n",
      "AdditiveTokenMixer-122           [-1, 48, 56, 56]               0\n",
      "        Identity-123           [-1, 48, 56, 56]               0\n",
      "     BatchNorm2d-124           [-1, 48, 56, 56]              96\n",
      "          Conv2d-125          [-1, 192, 56, 56]           9,408\n",
      "            GELU-126          [-1, 192, 56, 56]               0\n",
      "         Dropout-127          [-1, 192, 56, 56]               0\n",
      "          Conv2d-128           [-1, 48, 56, 56]           9,264\n",
      "         Dropout-129           [-1, 48, 56, 56]               0\n",
      "             Mlp-130           [-1, 48, 56, 56]               0\n",
      "        Identity-131           [-1, 48, 56, 56]               0\n",
      "   AdditiveBlock-132           [-1, 48, 56, 56]               0\n",
      "          Conv2d-133           [-1, 64, 28, 28]          27,712\n",
      "     BatchNorm2d-134           [-1, 64, 28, 28]             128\n",
      "       Embedding-135           [-1, 64, 28, 28]               0\n",
      "          Conv2d-136           [-1, 64, 28, 28]           4,160\n",
      "     BatchNorm2d-137           [-1, 64, 28, 28]             128\n",
      "          Conv2d-138           [-1, 64, 28, 28]             640\n",
      "            GELU-139           [-1, 64, 28, 28]               0\n",
      "          Conv2d-140           [-1, 64, 28, 28]           4,160\n",
      "LocalIntegration-141           [-1, 64, 28, 28]               0\n",
      "     BatchNorm2d-142           [-1, 64, 28, 28]             128\n",
      "          Conv2d-143          [-1, 192, 28, 28]          12,288\n",
      "          Conv2d-144           [-1, 64, 28, 28]             640\n",
      "     BatchNorm2d-145           [-1, 64, 28, 28]             128\n",
      "            ReLU-146           [-1, 64, 28, 28]               0\n",
      "          Conv2d-147            [-1, 1, 28, 28]              64\n",
      "         Sigmoid-148            [-1, 1, 28, 28]               0\n",
      "SpatialOperation-149           [-1, 64, 28, 28]               0\n",
      "AdaptiveAvgPool2d-150             [-1, 64, 1, 1]               0\n",
      "          Conv2d-151             [-1, 64, 1, 1]           4,096\n",
      "         Sigmoid-152             [-1, 64, 1, 1]               0\n",
      "ChannelOperation-153           [-1, 64, 28, 28]               0\n",
      "          Conv2d-154           [-1, 64, 28, 28]             640\n",
      "     BatchNorm2d-155           [-1, 64, 28, 28]             128\n",
      "            ReLU-156           [-1, 64, 28, 28]               0\n",
      "          Conv2d-157            [-1, 1, 28, 28]              64\n",
      "         Sigmoid-158            [-1, 1, 28, 28]               0\n",
      "SpatialOperation-159           [-1, 64, 28, 28]               0\n",
      "AdaptiveAvgPool2d-160             [-1, 64, 1, 1]               0\n",
      "          Conv2d-161             [-1, 64, 1, 1]           4,096\n",
      "         Sigmoid-162             [-1, 64, 1, 1]               0\n",
      "ChannelOperation-163           [-1, 64, 28, 28]               0\n",
      "          Conv2d-164           [-1, 64, 28, 28]             640\n",
      "          Conv2d-165           [-1, 64, 28, 28]             640\n",
      "         Dropout-166           [-1, 64, 28, 28]               0\n",
      "AdditiveTokenMixer-167           [-1, 64, 28, 28]               0\n",
      "        Identity-168           [-1, 64, 28, 28]               0\n",
      "     BatchNorm2d-169           [-1, 64, 28, 28]             128\n",
      "          Conv2d-170          [-1, 256, 28, 28]          16,640\n",
      "            GELU-171          [-1, 256, 28, 28]               0\n",
      "         Dropout-172          [-1, 256, 28, 28]               0\n",
      "          Conv2d-173           [-1, 64, 28, 28]          16,448\n",
      "         Dropout-174           [-1, 64, 28, 28]               0\n",
      "             Mlp-175           [-1, 64, 28, 28]               0\n",
      "        Identity-176           [-1, 64, 28, 28]               0\n",
      "   AdditiveBlock-177           [-1, 64, 28, 28]               0\n",
      "          Conv2d-178           [-1, 64, 28, 28]           4,160\n",
      "     BatchNorm2d-179           [-1, 64, 28, 28]             128\n",
      "          Conv2d-180           [-1, 64, 28, 28]             640\n",
      "            GELU-181           [-1, 64, 28, 28]               0\n",
      "          Conv2d-182           [-1, 64, 28, 28]           4,160\n",
      "LocalIntegration-183           [-1, 64, 28, 28]               0\n",
      "     BatchNorm2d-184           [-1, 64, 28, 28]             128\n",
      "          Conv2d-185          [-1, 192, 28, 28]          12,288\n",
      "          Conv2d-186           [-1, 64, 28, 28]             640\n",
      "     BatchNorm2d-187           [-1, 64, 28, 28]             128\n",
      "            ReLU-188           [-1, 64, 28, 28]               0\n",
      "          Conv2d-189            [-1, 1, 28, 28]              64\n",
      "         Sigmoid-190            [-1, 1, 28, 28]               0\n",
      "SpatialOperation-191           [-1, 64, 28, 28]               0\n",
      "AdaptiveAvgPool2d-192             [-1, 64, 1, 1]               0\n",
      "          Conv2d-193             [-1, 64, 1, 1]           4,096\n",
      "         Sigmoid-194             [-1, 64, 1, 1]               0\n",
      "ChannelOperation-195           [-1, 64, 28, 28]               0\n",
      "          Conv2d-196           [-1, 64, 28, 28]             640\n",
      "     BatchNorm2d-197           [-1, 64, 28, 28]             128\n",
      "            ReLU-198           [-1, 64, 28, 28]               0\n",
      "          Conv2d-199            [-1, 1, 28, 28]              64\n",
      "         Sigmoid-200            [-1, 1, 28, 28]               0\n",
      "SpatialOperation-201           [-1, 64, 28, 28]               0\n",
      "AdaptiveAvgPool2d-202             [-1, 64, 1, 1]               0\n",
      "          Conv2d-203             [-1, 64, 1, 1]           4,096\n",
      "         Sigmoid-204             [-1, 64, 1, 1]               0\n",
      "ChannelOperation-205           [-1, 64, 28, 28]               0\n",
      "          Conv2d-206           [-1, 64, 28, 28]             640\n",
      "          Conv2d-207           [-1, 64, 28, 28]             640\n",
      "         Dropout-208           [-1, 64, 28, 28]               0\n",
      "AdditiveTokenMixer-209           [-1, 64, 28, 28]               0\n",
      "        Identity-210           [-1, 64, 28, 28]               0\n",
      "     BatchNorm2d-211           [-1, 64, 28, 28]             128\n",
      "          Conv2d-212          [-1, 256, 28, 28]          16,640\n",
      "            GELU-213          [-1, 256, 28, 28]               0\n",
      "         Dropout-214          [-1, 256, 28, 28]               0\n",
      "          Conv2d-215           [-1, 64, 28, 28]          16,448\n",
      "         Dropout-216           [-1, 64, 28, 28]               0\n",
      "             Mlp-217           [-1, 64, 28, 28]               0\n",
      "        Identity-218           [-1, 64, 28, 28]               0\n",
      "   AdditiveBlock-219           [-1, 64, 28, 28]               0\n",
      "          Conv2d-220           [-1, 64, 28, 28]           4,160\n",
      "     BatchNorm2d-221           [-1, 64, 28, 28]             128\n",
      "          Conv2d-222           [-1, 64, 28, 28]             640\n",
      "            GELU-223           [-1, 64, 28, 28]               0\n",
      "          Conv2d-224           [-1, 64, 28, 28]           4,160\n",
      "LocalIntegration-225           [-1, 64, 28, 28]               0\n",
      "     BatchNorm2d-226           [-1, 64, 28, 28]             128\n",
      "          Conv2d-227          [-1, 192, 28, 28]          12,288\n",
      "          Conv2d-228           [-1, 64, 28, 28]             640\n",
      "     BatchNorm2d-229           [-1, 64, 28, 28]             128\n",
      "            ReLU-230           [-1, 64, 28, 28]               0\n",
      "          Conv2d-231            [-1, 1, 28, 28]              64\n",
      "         Sigmoid-232            [-1, 1, 28, 28]               0\n",
      "SpatialOperation-233           [-1, 64, 28, 28]               0\n",
      "AdaptiveAvgPool2d-234             [-1, 64, 1, 1]               0\n",
      "          Conv2d-235             [-1, 64, 1, 1]           4,096\n",
      "         Sigmoid-236             [-1, 64, 1, 1]               0\n",
      "ChannelOperation-237           [-1, 64, 28, 28]               0\n",
      "          Conv2d-238           [-1, 64, 28, 28]             640\n",
      "     BatchNorm2d-239           [-1, 64, 28, 28]             128\n",
      "            ReLU-240           [-1, 64, 28, 28]               0\n",
      "          Conv2d-241            [-1, 1, 28, 28]              64\n",
      "         Sigmoid-242            [-1, 1, 28, 28]               0\n",
      "SpatialOperation-243           [-1, 64, 28, 28]               0\n",
      "AdaptiveAvgPool2d-244             [-1, 64, 1, 1]               0\n",
      "          Conv2d-245             [-1, 64, 1, 1]           4,096\n",
      "         Sigmoid-246             [-1, 64, 1, 1]               0\n",
      "ChannelOperation-247           [-1, 64, 28, 28]               0\n",
      "          Conv2d-248           [-1, 64, 28, 28]             640\n",
      "          Conv2d-249           [-1, 64, 28, 28]             640\n",
      "         Dropout-250           [-1, 64, 28, 28]               0\n",
      "AdditiveTokenMixer-251           [-1, 64, 28, 28]               0\n",
      "        Identity-252           [-1, 64, 28, 28]               0\n",
      "     BatchNorm2d-253           [-1, 64, 28, 28]             128\n",
      "          Conv2d-254          [-1, 256, 28, 28]          16,640\n",
      "            GELU-255          [-1, 256, 28, 28]               0\n",
      "         Dropout-256          [-1, 256, 28, 28]               0\n",
      "          Conv2d-257           [-1, 64, 28, 28]          16,448\n",
      "         Dropout-258           [-1, 64, 28, 28]               0\n",
      "             Mlp-259           [-1, 64, 28, 28]               0\n",
      "        Identity-260           [-1, 64, 28, 28]               0\n",
      "   AdditiveBlock-261           [-1, 64, 28, 28]               0\n",
      "          Conv2d-262          [-1, 128, 14, 14]          73,856\n",
      "     BatchNorm2d-263          [-1, 128, 14, 14]             256\n",
      "       Embedding-264          [-1, 128, 14, 14]               0\n",
      "          Conv2d-265          [-1, 128, 14, 14]          16,512\n",
      "     BatchNorm2d-266          [-1, 128, 14, 14]             256\n",
      "          Conv2d-267          [-1, 128, 14, 14]           1,280\n",
      "            GELU-268          [-1, 128, 14, 14]               0\n",
      "          Conv2d-269          [-1, 128, 14, 14]          16,512\n",
      "LocalIntegration-270          [-1, 128, 14, 14]               0\n",
      "     BatchNorm2d-271          [-1, 128, 14, 14]             256\n",
      "          Conv2d-272          [-1, 384, 14, 14]          49,152\n",
      "          Conv2d-273          [-1, 128, 14, 14]           1,280\n",
      "     BatchNorm2d-274          [-1, 128, 14, 14]             256\n",
      "            ReLU-275          [-1, 128, 14, 14]               0\n",
      "          Conv2d-276            [-1, 1, 14, 14]             128\n",
      "         Sigmoid-277            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-278          [-1, 128, 14, 14]               0\n",
      "AdaptiveAvgPool2d-279            [-1, 128, 1, 1]               0\n",
      "          Conv2d-280            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-281            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-282          [-1, 128, 14, 14]               0\n",
      "          Conv2d-283          [-1, 128, 14, 14]           1,280\n",
      "     BatchNorm2d-284          [-1, 128, 14, 14]             256\n",
      "            ReLU-285          [-1, 128, 14, 14]               0\n",
      "          Conv2d-286            [-1, 1, 14, 14]             128\n",
      "         Sigmoid-287            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-288          [-1, 128, 14, 14]               0\n",
      "AdaptiveAvgPool2d-289            [-1, 128, 1, 1]               0\n",
      "          Conv2d-290            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-291            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-292          [-1, 128, 14, 14]               0\n",
      "          Conv2d-293          [-1, 128, 14, 14]           1,280\n",
      "          Conv2d-294          [-1, 128, 14, 14]           1,280\n",
      "         Dropout-295          [-1, 128, 14, 14]               0\n",
      "AdditiveTokenMixer-296          [-1, 128, 14, 14]               0\n",
      "        Identity-297          [-1, 128, 14, 14]               0\n",
      "     BatchNorm2d-298          [-1, 128, 14, 14]             256\n",
      "          Conv2d-299          [-1, 512, 14, 14]          66,048\n",
      "            GELU-300          [-1, 512, 14, 14]               0\n",
      "         Dropout-301          [-1, 512, 14, 14]               0\n",
      "          Conv2d-302          [-1, 128, 14, 14]          65,664\n",
      "         Dropout-303          [-1, 128, 14, 14]               0\n",
      "             Mlp-304          [-1, 128, 14, 14]               0\n",
      "        Identity-305          [-1, 128, 14, 14]               0\n",
      "   AdditiveBlock-306          [-1, 128, 14, 14]               0\n",
      "          Conv2d-307          [-1, 128, 14, 14]          16,512\n",
      "     BatchNorm2d-308          [-1, 128, 14, 14]             256\n",
      "          Conv2d-309          [-1, 128, 14, 14]           1,280\n",
      "            GELU-310          [-1, 128, 14, 14]               0\n",
      "          Conv2d-311          [-1, 128, 14, 14]          16,512\n",
      "LocalIntegration-312          [-1, 128, 14, 14]               0\n",
      "     BatchNorm2d-313          [-1, 128, 14, 14]             256\n",
      "          Conv2d-314          [-1, 384, 14, 14]          49,152\n",
      "          Conv2d-315          [-1, 128, 14, 14]           1,280\n",
      "     BatchNorm2d-316          [-1, 128, 14, 14]             256\n",
      "            ReLU-317          [-1, 128, 14, 14]               0\n",
      "          Conv2d-318            [-1, 1, 14, 14]             128\n",
      "         Sigmoid-319            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-320          [-1, 128, 14, 14]               0\n",
      "AdaptiveAvgPool2d-321            [-1, 128, 1, 1]               0\n",
      "          Conv2d-322            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-323            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-324          [-1, 128, 14, 14]               0\n",
      "          Conv2d-325          [-1, 128, 14, 14]           1,280\n",
      "     BatchNorm2d-326          [-1, 128, 14, 14]             256\n",
      "            ReLU-327          [-1, 128, 14, 14]               0\n",
      "          Conv2d-328            [-1, 1, 14, 14]             128\n",
      "         Sigmoid-329            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-330          [-1, 128, 14, 14]               0\n",
      "AdaptiveAvgPool2d-331            [-1, 128, 1, 1]               0\n",
      "          Conv2d-332            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-333            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-334          [-1, 128, 14, 14]               0\n",
      "          Conv2d-335          [-1, 128, 14, 14]           1,280\n",
      "          Conv2d-336          [-1, 128, 14, 14]           1,280\n",
      "         Dropout-337          [-1, 128, 14, 14]               0\n",
      "AdditiveTokenMixer-338          [-1, 128, 14, 14]               0\n",
      "        Identity-339          [-1, 128, 14, 14]               0\n",
      "     BatchNorm2d-340          [-1, 128, 14, 14]             256\n",
      "          Conv2d-341          [-1, 512, 14, 14]          66,048\n",
      "            GELU-342          [-1, 512, 14, 14]               0\n",
      "         Dropout-343          [-1, 512, 14, 14]               0\n",
      "          Conv2d-344          [-1, 128, 14, 14]          65,664\n",
      "         Dropout-345          [-1, 128, 14, 14]               0\n",
      "             Mlp-346          [-1, 128, 14, 14]               0\n",
      "        Identity-347          [-1, 128, 14, 14]               0\n",
      "   AdditiveBlock-348          [-1, 128, 14, 14]               0\n",
      "          Conv2d-349          [-1, 128, 14, 14]          16,512\n",
      "     BatchNorm2d-350          [-1, 128, 14, 14]             256\n",
      "          Conv2d-351          [-1, 128, 14, 14]           1,280\n",
      "            GELU-352          [-1, 128, 14, 14]               0\n",
      "          Conv2d-353          [-1, 128, 14, 14]          16,512\n",
      "LocalIntegration-354          [-1, 128, 14, 14]               0\n",
      "     BatchNorm2d-355          [-1, 128, 14, 14]             256\n",
      "          Conv2d-356          [-1, 384, 14, 14]          49,152\n",
      "          Conv2d-357          [-1, 128, 14, 14]           1,280\n",
      "     BatchNorm2d-358          [-1, 128, 14, 14]             256\n",
      "            ReLU-359          [-1, 128, 14, 14]               0\n",
      "          Conv2d-360            [-1, 1, 14, 14]             128\n",
      "         Sigmoid-361            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-362          [-1, 128, 14, 14]               0\n",
      "AdaptiveAvgPool2d-363            [-1, 128, 1, 1]               0\n",
      "          Conv2d-364            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-365            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-366          [-1, 128, 14, 14]               0\n",
      "          Conv2d-367          [-1, 128, 14, 14]           1,280\n",
      "     BatchNorm2d-368          [-1, 128, 14, 14]             256\n",
      "            ReLU-369          [-1, 128, 14, 14]               0\n",
      "          Conv2d-370            [-1, 1, 14, 14]             128\n",
      "         Sigmoid-371            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-372          [-1, 128, 14, 14]               0\n",
      "AdaptiveAvgPool2d-373            [-1, 128, 1, 1]               0\n",
      "          Conv2d-374            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-375            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-376          [-1, 128, 14, 14]               0\n",
      "          Conv2d-377          [-1, 128, 14, 14]           1,280\n",
      "          Conv2d-378          [-1, 128, 14, 14]           1,280\n",
      "         Dropout-379          [-1, 128, 14, 14]               0\n",
      "AdditiveTokenMixer-380          [-1, 128, 14, 14]               0\n",
      "        Identity-381          [-1, 128, 14, 14]               0\n",
      "     BatchNorm2d-382          [-1, 128, 14, 14]             256\n",
      "          Conv2d-383          [-1, 512, 14, 14]          66,048\n",
      "            GELU-384          [-1, 512, 14, 14]               0\n",
      "         Dropout-385          [-1, 512, 14, 14]               0\n",
      "          Conv2d-386          [-1, 128, 14, 14]          65,664\n",
      "         Dropout-387          [-1, 128, 14, 14]               0\n",
      "             Mlp-388          [-1, 128, 14, 14]               0\n",
      "        Identity-389          [-1, 128, 14, 14]               0\n",
      "   AdditiveBlock-390          [-1, 128, 14, 14]               0\n",
      "          Conv2d-391          [-1, 128, 14, 14]          16,512\n",
      "     BatchNorm2d-392          [-1, 128, 14, 14]             256\n",
      "          Conv2d-393          [-1, 128, 14, 14]           1,280\n",
      "            GELU-394          [-1, 128, 14, 14]               0\n",
      "          Conv2d-395          [-1, 128, 14, 14]          16,512\n",
      "LocalIntegration-396          [-1, 128, 14, 14]               0\n",
      "     BatchNorm2d-397          [-1, 128, 14, 14]             256\n",
      "          Conv2d-398          [-1, 384, 14, 14]          49,152\n",
      "          Conv2d-399          [-1, 128, 14, 14]           1,280\n",
      "     BatchNorm2d-400          [-1, 128, 14, 14]             256\n",
      "            ReLU-401          [-1, 128, 14, 14]               0\n",
      "          Conv2d-402            [-1, 1, 14, 14]             128\n",
      "         Sigmoid-403            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-404          [-1, 128, 14, 14]               0\n",
      "AdaptiveAvgPool2d-405            [-1, 128, 1, 1]               0\n",
      "          Conv2d-406            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-407            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-408          [-1, 128, 14, 14]               0\n",
      "          Conv2d-409          [-1, 128, 14, 14]           1,280\n",
      "     BatchNorm2d-410          [-1, 128, 14, 14]             256\n",
      "            ReLU-411          [-1, 128, 14, 14]               0\n",
      "          Conv2d-412            [-1, 1, 14, 14]             128\n",
      "         Sigmoid-413            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-414          [-1, 128, 14, 14]               0\n",
      "AdaptiveAvgPool2d-415            [-1, 128, 1, 1]               0\n",
      "          Conv2d-416            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-417            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-418          [-1, 128, 14, 14]               0\n",
      "          Conv2d-419          [-1, 128, 14, 14]           1,280\n",
      "          Conv2d-420          [-1, 128, 14, 14]           1,280\n",
      "         Dropout-421          [-1, 128, 14, 14]               0\n",
      "AdditiveTokenMixer-422          [-1, 128, 14, 14]               0\n",
      "        Identity-423          [-1, 128, 14, 14]               0\n",
      "     BatchNorm2d-424          [-1, 128, 14, 14]             256\n",
      "          Conv2d-425          [-1, 512, 14, 14]          66,048\n",
      "            GELU-426          [-1, 512, 14, 14]               0\n",
      "         Dropout-427          [-1, 512, 14, 14]               0\n",
      "          Conv2d-428          [-1, 128, 14, 14]          65,664\n",
      "         Dropout-429          [-1, 128, 14, 14]               0\n",
      "             Mlp-430          [-1, 128, 14, 14]               0\n",
      "        Identity-431          [-1, 128, 14, 14]               0\n",
      "   AdditiveBlock-432          [-1, 128, 14, 14]               0\n",
      "          Conv2d-433          [-1, 128, 14, 14]          16,512\n",
      "     BatchNorm2d-434          [-1, 128, 14, 14]             256\n",
      "          Conv2d-435          [-1, 128, 14, 14]           1,280\n",
      "            GELU-436          [-1, 128, 14, 14]               0\n",
      "          Conv2d-437          [-1, 128, 14, 14]          16,512\n",
      "LocalIntegration-438          [-1, 128, 14, 14]               0\n",
      "     BatchNorm2d-439          [-1, 128, 14, 14]             256\n",
      "          Conv2d-440          [-1, 384, 14, 14]          49,152\n",
      "          Conv2d-441          [-1, 128, 14, 14]           1,280\n",
      "     BatchNorm2d-442          [-1, 128, 14, 14]             256\n",
      "            ReLU-443          [-1, 128, 14, 14]               0\n",
      "          Conv2d-444            [-1, 1, 14, 14]             128\n",
      "         Sigmoid-445            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-446          [-1, 128, 14, 14]               0\n",
      "AdaptiveAvgPool2d-447            [-1, 128, 1, 1]               0\n",
      "          Conv2d-448            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-449            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-450          [-1, 128, 14, 14]               0\n",
      "          Conv2d-451          [-1, 128, 14, 14]           1,280\n",
      "     BatchNorm2d-452          [-1, 128, 14, 14]             256\n",
      "            ReLU-453          [-1, 128, 14, 14]               0\n",
      "          Conv2d-454            [-1, 1, 14, 14]             128\n",
      "         Sigmoid-455            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-456          [-1, 128, 14, 14]               0\n",
      "AdaptiveAvgPool2d-457            [-1, 128, 1, 1]               0\n",
      "          Conv2d-458            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-459            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-460          [-1, 128, 14, 14]               0\n",
      "          Conv2d-461          [-1, 128, 14, 14]           1,280\n",
      "          Conv2d-462          [-1, 128, 14, 14]           1,280\n",
      "         Dropout-463          [-1, 128, 14, 14]               0\n",
      "AdditiveTokenMixer-464          [-1, 128, 14, 14]               0\n",
      "        Identity-465          [-1, 128, 14, 14]               0\n",
      "     BatchNorm2d-466          [-1, 128, 14, 14]             256\n",
      "          Conv2d-467          [-1, 512, 14, 14]          66,048\n",
      "            GELU-468          [-1, 512, 14, 14]               0\n",
      "         Dropout-469          [-1, 512, 14, 14]               0\n",
      "          Conv2d-470          [-1, 128, 14, 14]          65,664\n",
      "         Dropout-471          [-1, 128, 14, 14]               0\n",
      "             Mlp-472          [-1, 128, 14, 14]               0\n",
      "        Identity-473          [-1, 128, 14, 14]               0\n",
      "   AdditiveBlock-474          [-1, 128, 14, 14]               0\n",
      "          Conv2d-475          [-1, 128, 14, 14]          16,512\n",
      "     BatchNorm2d-476          [-1, 128, 14, 14]             256\n",
      "          Conv2d-477          [-1, 128, 14, 14]           1,280\n",
      "            GELU-478          [-1, 128, 14, 14]               0\n",
      "          Conv2d-479          [-1, 128, 14, 14]          16,512\n",
      "LocalIntegration-480          [-1, 128, 14, 14]               0\n",
      "     BatchNorm2d-481          [-1, 128, 14, 14]             256\n",
      "          Conv2d-482          [-1, 384, 14, 14]          49,152\n",
      "          Conv2d-483          [-1, 128, 14, 14]           1,280\n",
      "     BatchNorm2d-484          [-1, 128, 14, 14]             256\n",
      "            ReLU-485          [-1, 128, 14, 14]               0\n",
      "          Conv2d-486            [-1, 1, 14, 14]             128\n",
      "         Sigmoid-487            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-488          [-1, 128, 14, 14]               0\n",
      "AdaptiveAvgPool2d-489            [-1, 128, 1, 1]               0\n",
      "          Conv2d-490            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-491            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-492          [-1, 128, 14, 14]               0\n",
      "          Conv2d-493          [-1, 128, 14, 14]           1,280\n",
      "     BatchNorm2d-494          [-1, 128, 14, 14]             256\n",
      "            ReLU-495          [-1, 128, 14, 14]               0\n",
      "          Conv2d-496            [-1, 1, 14, 14]             128\n",
      "         Sigmoid-497            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-498          [-1, 128, 14, 14]               0\n",
      "AdaptiveAvgPool2d-499            [-1, 128, 1, 1]               0\n",
      "          Conv2d-500            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-501            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-502          [-1, 128, 14, 14]               0\n",
      "          Conv2d-503          [-1, 128, 14, 14]           1,280\n",
      "          Conv2d-504          [-1, 128, 14, 14]           1,280\n",
      "         Dropout-505          [-1, 128, 14, 14]               0\n",
      "AdditiveTokenMixer-506          [-1, 128, 14, 14]               0\n",
      "        Identity-507          [-1, 128, 14, 14]               0\n",
      "     BatchNorm2d-508          [-1, 128, 14, 14]             256\n",
      "          Conv2d-509          [-1, 512, 14, 14]          66,048\n",
      "            GELU-510          [-1, 512, 14, 14]               0\n",
      "         Dropout-511          [-1, 512, 14, 14]               0\n",
      "          Conv2d-512          [-1, 128, 14, 14]          65,664\n",
      "         Dropout-513          [-1, 128, 14, 14]               0\n",
      "             Mlp-514          [-1, 128, 14, 14]               0\n",
      "        Identity-515          [-1, 128, 14, 14]               0\n",
      "   AdditiveBlock-516          [-1, 128, 14, 14]               0\n",
      "          Conv2d-517            [-1, 256, 7, 7]         295,168\n",
      "     BatchNorm2d-518            [-1, 256, 7, 7]             512\n",
      "       Embedding-519            [-1, 256, 7, 7]               0\n",
      "          Conv2d-520            [-1, 256, 7, 7]          65,792\n",
      "     BatchNorm2d-521            [-1, 256, 7, 7]             512\n",
      "          Conv2d-522            [-1, 256, 7, 7]           2,560\n",
      "            GELU-523            [-1, 256, 7, 7]               0\n",
      "          Conv2d-524            [-1, 256, 7, 7]          65,792\n",
      "LocalIntegration-525            [-1, 256, 7, 7]               0\n",
      "     BatchNorm2d-526            [-1, 256, 7, 7]             512\n",
      "          Conv2d-527            [-1, 768, 7, 7]         196,608\n",
      "          Conv2d-528            [-1, 256, 7, 7]           2,560\n",
      "     BatchNorm2d-529            [-1, 256, 7, 7]             512\n",
      "            ReLU-530            [-1, 256, 7, 7]               0\n",
      "          Conv2d-531              [-1, 1, 7, 7]             256\n",
      "         Sigmoid-532              [-1, 1, 7, 7]               0\n",
      "SpatialOperation-533            [-1, 256, 7, 7]               0\n",
      "AdaptiveAvgPool2d-534            [-1, 256, 1, 1]               0\n",
      "          Conv2d-535            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-536            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-537            [-1, 256, 7, 7]               0\n",
      "          Conv2d-538            [-1, 256, 7, 7]           2,560\n",
      "     BatchNorm2d-539            [-1, 256, 7, 7]             512\n",
      "            ReLU-540            [-1, 256, 7, 7]               0\n",
      "          Conv2d-541              [-1, 1, 7, 7]             256\n",
      "         Sigmoid-542              [-1, 1, 7, 7]               0\n",
      "SpatialOperation-543            [-1, 256, 7, 7]               0\n",
      "AdaptiveAvgPool2d-544            [-1, 256, 1, 1]               0\n",
      "          Conv2d-545            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-546            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-547            [-1, 256, 7, 7]               0\n",
      "          Conv2d-548            [-1, 256, 7, 7]           2,560\n",
      "          Conv2d-549            [-1, 256, 7, 7]           2,560\n",
      "         Dropout-550            [-1, 256, 7, 7]               0\n",
      "AdditiveTokenMixer-551            [-1, 256, 7, 7]               0\n",
      "        Identity-552            [-1, 256, 7, 7]               0\n",
      "     BatchNorm2d-553            [-1, 256, 7, 7]             512\n",
      "          Conv2d-554           [-1, 1024, 7, 7]         263,168\n",
      "            GELU-555           [-1, 1024, 7, 7]               0\n",
      "         Dropout-556           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-557            [-1, 256, 7, 7]         262,400\n",
      "         Dropout-558            [-1, 256, 7, 7]               0\n",
      "             Mlp-559            [-1, 256, 7, 7]               0\n",
      "        Identity-560            [-1, 256, 7, 7]               0\n",
      "   AdditiveBlock-561            [-1, 256, 7, 7]               0\n",
      "          Conv2d-562            [-1, 256, 7, 7]          65,792\n",
      "     BatchNorm2d-563            [-1, 256, 7, 7]             512\n",
      "          Conv2d-564            [-1, 256, 7, 7]           2,560\n",
      "            GELU-565            [-1, 256, 7, 7]               0\n",
      "          Conv2d-566            [-1, 256, 7, 7]          65,792\n",
      "LocalIntegration-567            [-1, 256, 7, 7]               0\n",
      "     BatchNorm2d-568            [-1, 256, 7, 7]             512\n",
      "          Conv2d-569            [-1, 768, 7, 7]         196,608\n",
      "          Conv2d-570            [-1, 256, 7, 7]           2,560\n",
      "     BatchNorm2d-571            [-1, 256, 7, 7]             512\n",
      "            ReLU-572            [-1, 256, 7, 7]               0\n",
      "          Conv2d-573              [-1, 1, 7, 7]             256\n",
      "         Sigmoid-574              [-1, 1, 7, 7]               0\n",
      "SpatialOperation-575            [-1, 256, 7, 7]               0\n",
      "AdaptiveAvgPool2d-576            [-1, 256, 1, 1]               0\n",
      "          Conv2d-577            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-578            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-579            [-1, 256, 7, 7]               0\n",
      "          Conv2d-580            [-1, 256, 7, 7]           2,560\n",
      "     BatchNorm2d-581            [-1, 256, 7, 7]             512\n",
      "            ReLU-582            [-1, 256, 7, 7]               0\n",
      "          Conv2d-583              [-1, 1, 7, 7]             256\n",
      "         Sigmoid-584              [-1, 1, 7, 7]               0\n",
      "SpatialOperation-585            [-1, 256, 7, 7]               0\n",
      "AdaptiveAvgPool2d-586            [-1, 256, 1, 1]               0\n",
      "          Conv2d-587            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-588            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-589            [-1, 256, 7, 7]               0\n",
      "          Conv2d-590            [-1, 256, 7, 7]           2,560\n",
      "          Conv2d-591            [-1, 256, 7, 7]           2,560\n",
      "         Dropout-592            [-1, 256, 7, 7]               0\n",
      "AdditiveTokenMixer-593            [-1, 256, 7, 7]               0\n",
      "        Identity-594            [-1, 256, 7, 7]               0\n",
      "     BatchNorm2d-595            [-1, 256, 7, 7]             512\n",
      "          Conv2d-596           [-1, 1024, 7, 7]         263,168\n",
      "            GELU-597           [-1, 1024, 7, 7]               0\n",
      "         Dropout-598           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-599            [-1, 256, 7, 7]         262,400\n",
      "         Dropout-600            [-1, 256, 7, 7]               0\n",
      "             Mlp-601            [-1, 256, 7, 7]               0\n",
      "        Identity-602            [-1, 256, 7, 7]               0\n",
      "   AdditiveBlock-603            [-1, 256, 7, 7]               0\n",
      "          Conv2d-604            [-1, 256, 7, 7]          65,792\n",
      "     BatchNorm2d-605            [-1, 256, 7, 7]             512\n",
      "          Conv2d-606            [-1, 256, 7, 7]           2,560\n",
      "            GELU-607            [-1, 256, 7, 7]               0\n",
      "          Conv2d-608            [-1, 256, 7, 7]          65,792\n",
      "LocalIntegration-609            [-1, 256, 7, 7]               0\n",
      "     BatchNorm2d-610            [-1, 256, 7, 7]             512\n",
      "          Conv2d-611            [-1, 768, 7, 7]         196,608\n",
      "          Conv2d-612            [-1, 256, 7, 7]           2,560\n",
      "     BatchNorm2d-613            [-1, 256, 7, 7]             512\n",
      "            ReLU-614            [-1, 256, 7, 7]               0\n",
      "          Conv2d-615              [-1, 1, 7, 7]             256\n",
      "         Sigmoid-616              [-1, 1, 7, 7]               0\n",
      "SpatialOperation-617            [-1, 256, 7, 7]               0\n",
      "AdaptiveAvgPool2d-618            [-1, 256, 1, 1]               0\n",
      "          Conv2d-619            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-620            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-621            [-1, 256, 7, 7]               0\n",
      "          Conv2d-622            [-1, 256, 7, 7]           2,560\n",
      "     BatchNorm2d-623            [-1, 256, 7, 7]             512\n",
      "            ReLU-624            [-1, 256, 7, 7]               0\n",
      "          Conv2d-625              [-1, 1, 7, 7]             256\n",
      "         Sigmoid-626              [-1, 1, 7, 7]               0\n",
      "SpatialOperation-627            [-1, 256, 7, 7]               0\n",
      "AdaptiveAvgPool2d-628            [-1, 256, 1, 1]               0\n",
      "          Conv2d-629            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-630            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-631            [-1, 256, 7, 7]               0\n",
      "          Conv2d-632            [-1, 256, 7, 7]           2,560\n",
      "          Conv2d-633            [-1, 256, 7, 7]           2,560\n",
      "         Dropout-634            [-1, 256, 7, 7]               0\n",
      "AdditiveTokenMixer-635            [-1, 256, 7, 7]               0\n",
      "        Identity-636            [-1, 256, 7, 7]               0\n",
      "     BatchNorm2d-637            [-1, 256, 7, 7]             512\n",
      "          Conv2d-638           [-1, 1024, 7, 7]         263,168\n",
      "            GELU-639           [-1, 1024, 7, 7]               0\n",
      "         Dropout-640           [-1, 1024, 7, 7]               0\n",
      "          Conv2d-641            [-1, 256, 7, 7]         262,400\n",
      "         Dropout-642            [-1, 256, 7, 7]               0\n",
      "             Mlp-643            [-1, 256, 7, 7]               0\n",
      "        Identity-644            [-1, 256, 7, 7]               0\n",
      "   AdditiveBlock-645            [-1, 256, 7, 7]               0\n",
      "     BatchNorm2d-646            [-1, 256, 7, 7]             512\n",
      "          Linear-647                 [-1, 1000]         257,000\n",
      "          Linear-648                 [-1, 1000]         257,000\n",
      "================================================================\n",
      "Total params: 5,764,224\n",
      "Trainable params: 5,764,224\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 272.20\n",
      "Params size (MB): 21.99\n",
      "Estimated Total Size (MB): 294.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = net.cuda()\n",
    "\n",
    "net.eval()\n",
    "\n",
    "torchsummary.summary(net, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for RCViT:\n\tsize mismatch for head.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([1000, 256]).\n\tsize mismatch for head.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([1000]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Loading pre-trained weights\u001b[39;00m\n\u001b[1;32m      3\u001b[0m weightDict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/centar15-desktop1/LPCV_2025_T1/src/casvit/models/2025-02-12 18:22:40.578694/model_40epoha_coco_train_batch_size_384.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweightDict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LPCV_2025_T1/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for RCViT:\n\tsize mismatch for head.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([1000, 256]).\n\tsize mismatch for head.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([1000])."
     ]
    }
   ],
   "source": [
    "# Loading pre-trained weights\n",
    "\n",
    "# weightDict = torch.load(\"/home/centar15-desktop1/LPCV_2025_T1/src/casvit/models/2025-02-12 18:22:40.578694/model_40epoha_coco_train_batch_size_384.pth\", map_location=torch.device(\"cpu\"))\n",
    "# net.load_state_dict(weightDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.eval()\n",
    "# input = input_getter.local_image_getter('../../../data/1.jpeg')\n",
    "# net(torch.tensor(input.get_input_torch())).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input.get_input_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(input.get_input_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kada se uradi ovi jobovi na linkovima se moze naci vizuelizacija mreze i informacije za dalju analizu\n",
    "\n",
    "# compile, profile, inference = qai_hub_jobs.compile_profile_inference(net, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = helper.get_imagenet_categories()\n",
    "\n",
    "# # Rezultati dobijeni na cloudu\n",
    "\n",
    "# helper.inference_job_probabilities(inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vraca verovatnoce lokalne PyTorch mreze\n",
    "\n",
    "# helper.print_probablities_from_output(net(input.get_input_torch()), categories=categories, modelname = 'Torch Local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vraca verovatnoce TensorFlow mreze\n",
    "\n",
    "# download_path = tfhelper.download_model_from_compile_job(compile, modelName)\n",
    "# TFModel = tfhelper.TFHelper(download_path)\n",
    "# TFModel.run_inference(input.get_input_numpy())\n",
    "# helper.print_probablities_from_output(TFModel.run_inference(input.get_input_numpy()), categories=categories, modelname = 'TensorFlow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n",
      "NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"using cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"using mps\")\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"using cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from PIL import Image\n",
    "\n",
    "# folderpath = \"/home/centar15-desktop1/Desktop/example_data/images-20250210T115939Z-001/images/\"\n",
    "\n",
    "# #from src.utils.helper import get_imagenet_categories\n",
    "\n",
    "# klas = helper.get_imagenet_categories()\n",
    "\n",
    "\n",
    "# transform2 = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  \n",
    "#     transforms.ToTensor(),  \n",
    "#     # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     transforms.Normalize(mean=[0.4435, 0.3968, 0.3221], std=[0.2266, 0.2126, 0.2013]),  \n",
    "#     transforms.Lambda(lambda x: x.unsqueeze(0).to(device))\n",
    "# ])\n",
    "\n",
    "# image = Image.open(folderpath + f\"{idx}.jpg\")\n",
    "\n",
    "# if image.mode == 'RGBA':\n",
    "#     # Convert the image to RGB (remove alpha channel)\n",
    "#     image = image.convert('RGB')\n",
    "\n",
    "# # image = ski.io.imread(folderpath + f\"{idx}.jpg\")\n",
    "\n",
    "# # print(image.shape)\n",
    "\n",
    "# # image = image[:,:, :3]\n",
    "# # image2 = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "# # print(image.shape)\n",
    "\n",
    "\n",
    "\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "\n",
    "# helper.print_probablities_from_output(net(transform2(image)), klas, 10, 'Transformer')\n",
    "\n",
    "# idx = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from dataset import DatasetReader\n",
    "import matplotlib.pyplot as plt\n",
    "import dataset.utils as utils\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "    transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    transforms.ColorJitter(brightness = 0.5),\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672065\n"
     ]
    }
   ],
   "source": [
    "dataset_coco = DatasetReader.COCODataset(annotation_file='../../datasets/coco/annotations/instances_train2017.json',\n",
    "    image_dir= '../../datasets/coco/train2017',\n",
    "    target_classes=[s.lower() for s in utils.GLOBAL_CLASSES],\n",
    "    transform=transform)\n",
    "\n",
    "\n",
    "root_folder = '../../datasets/imagenet/coco_80'\n",
    "class_names = [s.lower().replace(' ', '_') for s in utils.GLOBAL_CLASSES]\n",
    "dataset_imagenet = DatasetReader.CustomImageFolder(root_dir=root_folder, class_names=class_names, transform=transform)\n",
    "\n",
    "dataset = torch.utils.data.ConcatDataset([dataset_coco, dataset_imagenet])\n",
    "\n",
    "batch_size = 384\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=15, prefetch_factor=4, persistent_workers=True)\n",
    "\n",
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=50):\n",
    "    start_datetime = datetime.datetime.now()\n",
    "    os.mkdir(f'models/{start_datetime}')\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "        asdf = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {running_loss / len(dataloader):.4f}, Time: {asdf:.4f}s\")\n",
    "        if np.mod(epoch, 5) == 0:\n",
    "            torch.save(model.state_dict(), f\"models/{start_datetime}/model_{epoch}epoha_coco_train_batch_size_{batch_size}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.dist = False\n",
    "net.head = nn.Linear(256, 64)\n",
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad = False  # Freeze all layers\n",
    "\n",
    "for param in net.head.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd = torch.load(\"/home/centar15-desktop1/LPCV_2025_T1/src/casvit/models/2025-02-12 18:22:40.578694/model_40epoha_coco_train_batch_size_384.pth\")\n",
    "\n",
    "net.load_state_dict(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [10/1751], Loss: 4.2838\n",
      "Epoch [1/100], Step [20/1751], Loss: 4.2698\n",
      "Epoch [1/100], Step [30/1751], Loss: 4.2477\n",
      "Epoch [1/100], Step [40/1751], Loss: 4.2738\n",
      "Epoch [1/100], Step [50/1751], Loss: 4.2535\n",
      "Epoch [1/100], Step [60/1751], Loss: 4.1810\n",
      "Epoch [1/100], Step [70/1751], Loss: 4.2644\n",
      "Epoch [1/100], Step [80/1751], Loss: 4.2376\n",
      "Epoch [1/100], Step [90/1751], Loss: 4.2018\n",
      "Epoch [1/100], Step [100/1751], Loss: 4.2471\n",
      "Epoch [1/100], Step [110/1751], Loss: 4.1921\n",
      "Epoch [1/100], Step [120/1751], Loss: 4.2083\n",
      "Epoch [1/100], Step [130/1751], Loss: 4.2006\n",
      "Epoch [1/100], Step [140/1751], Loss: 4.2023\n",
      "Epoch [1/100], Step [150/1751], Loss: 4.1738\n",
      "Epoch [1/100], Step [160/1751], Loss: 4.1929\n",
      "Epoch [1/100], Step [170/1751], Loss: 4.1552\n",
      "Epoch [1/100], Step [180/1751], Loss: 4.1591\n",
      "Epoch [1/100], Step [190/1751], Loss: 4.1217\n",
      "Epoch [1/100], Step [200/1751], Loss: 4.0791\n",
      "Epoch [1/100], Step [210/1751], Loss: 4.1804\n",
      "Epoch [1/100], Step [220/1751], Loss: 4.1522\n",
      "Epoch [1/100], Step [230/1751], Loss: 4.1087\n",
      "Epoch [1/100], Step [240/1751], Loss: 4.0620\n",
      "Epoch [1/100], Step [250/1751], Loss: 4.1048\n",
      "Epoch [1/100], Step [260/1751], Loss: 4.0867\n",
      "Epoch [1/100], Step [270/1751], Loss: 4.1413\n",
      "Epoch [1/100], Step [280/1751], Loss: 4.1013\n",
      "Epoch [1/100], Step [290/1751], Loss: 4.0832\n",
      "Epoch [1/100], Step [300/1751], Loss: 4.0488\n",
      "Epoch [1/100], Step [310/1751], Loss: 4.0455\n",
      "Epoch [1/100], Step [320/1751], Loss: 4.0455\n",
      "Epoch [1/100], Step [330/1751], Loss: 4.0659\n",
      "Epoch [1/100], Step [340/1751], Loss: 4.0028\n",
      "Epoch [1/100], Step [350/1751], Loss: 4.0344\n",
      "Epoch [1/100], Step [360/1751], Loss: 4.0237\n",
      "Epoch [1/100], Step [370/1751], Loss: 4.0565\n",
      "Epoch [1/100], Step [380/1751], Loss: 4.0704\n",
      "Epoch [1/100], Step [390/1751], Loss: 4.0252\n",
      "Epoch [1/100], Step [400/1751], Loss: 4.0287\n",
      "Epoch [1/100], Step [410/1751], Loss: 3.9829\n",
      "Epoch [1/100], Step [420/1751], Loss: 4.0602\n",
      "Epoch [1/100], Step [430/1751], Loss: 3.9645\n",
      "Epoch [1/100], Step [440/1751], Loss: 3.9314\n",
      "Epoch [1/100], Step [450/1751], Loss: 3.9665\n",
      "Epoch [1/100], Step [460/1751], Loss: 3.9817\n",
      "Epoch [1/100], Step [470/1751], Loss: 3.9847\n",
      "Epoch [1/100], Step [480/1751], Loss: 3.9728\n",
      "Epoch [1/100], Step [490/1751], Loss: 3.9431\n",
      "Epoch [1/100], Step [500/1751], Loss: 4.0021\n",
      "Epoch [1/100], Step [510/1751], Loss: 3.9221\n",
      "Epoch [1/100], Step [520/1751], Loss: 3.9249\n",
      "Epoch [1/100], Step [530/1751], Loss: 3.9448\n",
      "Epoch [1/100], Step [540/1751], Loss: 3.9120\n",
      "Epoch [1/100], Step [550/1751], Loss: 3.9206\n",
      "Epoch [1/100], Step [560/1751], Loss: 3.8882\n",
      "Epoch [1/100], Step [570/1751], Loss: 3.8855\n",
      "Epoch [1/100], Step [580/1751], Loss: 3.9003\n",
      "Epoch [1/100], Step [590/1751], Loss: 3.8734\n",
      "Epoch [1/100], Step [600/1751], Loss: 3.9052\n",
      "Epoch [1/100], Step [610/1751], Loss: 3.8248\n",
      "Epoch [1/100], Step [620/1751], Loss: 3.8373\n",
      "Epoch [1/100], Step [630/1751], Loss: 3.8347\n",
      "Epoch [1/100], Step [640/1751], Loss: 3.8280\n",
      "Epoch [1/100], Step [650/1751], Loss: 3.8676\n",
      "Epoch [1/100], Step [660/1751], Loss: 3.8042\n",
      "Epoch [1/100], Step [670/1751], Loss: 3.8468\n",
      "Epoch [1/100], Step [680/1751], Loss: 3.8585\n",
      "Epoch [1/100], Step [690/1751], Loss: 3.8013\n",
      "Epoch [1/100], Step [700/1751], Loss: 3.8925\n",
      "Epoch [1/100], Step [710/1751], Loss: 3.8254\n",
      "Epoch [1/100], Step [720/1751], Loss: 3.8176\n",
      "Epoch [1/100], Step [730/1751], Loss: 3.7652\n",
      "Epoch [1/100], Step [740/1751], Loss: 3.8020\n",
      "Epoch [1/100], Step [750/1751], Loss: 3.7309\n",
      "Epoch [1/100], Step [760/1751], Loss: 3.7358\n",
      "Epoch [1/100], Step [770/1751], Loss: 3.7504\n",
      "Epoch [1/100], Step [780/1751], Loss: 3.7550\n",
      "Epoch [1/100], Step [790/1751], Loss: 3.7461\n",
      "Epoch [1/100], Step [800/1751], Loss: 3.7127\n",
      "Epoch [1/100], Step [810/1751], Loss: 3.7935\n",
      "Epoch [1/100], Step [820/1751], Loss: 3.7463\n",
      "Epoch [1/100], Step [830/1751], Loss: 3.7401\n",
      "Epoch [1/100], Step [840/1751], Loss: 3.7617\n",
      "Epoch [1/100], Step [850/1751], Loss: 3.7046\n",
      "Epoch [1/100], Step [860/1751], Loss: 3.6973\n",
      "Epoch [1/100], Step [870/1751], Loss: 3.7007\n",
      "Epoch [1/100], Step [880/1751], Loss: 3.7286\n",
      "Epoch [1/100], Step [890/1751], Loss: 3.6649\n",
      "Epoch [1/100], Step [900/1751], Loss: 3.6688\n",
      "Epoch [1/100], Step [910/1751], Loss: 3.6281\n",
      "Epoch [1/100], Step [920/1751], Loss: 3.6701\n",
      "Epoch [1/100], Step [930/1751], Loss: 3.6683\n",
      "Epoch [1/100], Step [940/1751], Loss: 3.6552\n",
      "Epoch [1/100], Step [950/1751], Loss: 3.5799\n",
      "Epoch [1/100], Step [960/1751], Loss: 3.6616\n",
      "Epoch [1/100], Step [970/1751], Loss: 3.6332\n",
      "Epoch [1/100], Step [980/1751], Loss: 3.5990\n",
      "Epoch [1/100], Step [990/1751], Loss: 3.6136\n",
      "Epoch [1/100], Step [1000/1751], Loss: 3.6356\n",
      "Epoch [1/100], Step [1010/1751], Loss: 3.6039\n",
      "Epoch [1/100], Step [1020/1751], Loss: 3.5773\n",
      "Epoch [1/100], Step [1030/1751], Loss: 3.5843\n",
      "Epoch [1/100], Step [1040/1751], Loss: 3.5742\n",
      "Epoch [1/100], Step [1050/1751], Loss: 3.6375\n",
      "Epoch [1/100], Step [1060/1751], Loss: 3.6481\n",
      "Epoch [1/100], Step [1070/1751], Loss: 3.6337\n",
      "Epoch [1/100], Step [1080/1751], Loss: 3.5575\n",
      "Epoch [1/100], Step [1090/1751], Loss: 3.5598\n",
      "Epoch [1/100], Step [1100/1751], Loss: 3.5394\n",
      "Epoch [1/100], Step [1110/1751], Loss: 3.6128\n",
      "Epoch [1/100], Step [1120/1751], Loss: 3.5253\n",
      "Epoch [1/100], Step [1130/1751], Loss: 3.5173\n",
      "Epoch [1/100], Step [1140/1751], Loss: 3.5518\n",
      "Epoch [1/100], Step [1150/1751], Loss: 3.4903\n",
      "Epoch [1/100], Step [1160/1751], Loss: 3.4823\n",
      "Epoch [1/100], Step [1170/1751], Loss: 3.4730\n",
      "Epoch [1/100], Step [1180/1751], Loss: 3.4557\n",
      "Epoch [1/100], Step [1190/1751], Loss: 3.5165\n",
      "Epoch [1/100], Step [1200/1751], Loss: 3.5160\n",
      "Epoch [1/100], Step [1210/1751], Loss: 3.4109\n",
      "Epoch [1/100], Step [1220/1751], Loss: 3.4875\n",
      "Epoch [1/100], Step [1230/1751], Loss: 3.4808\n",
      "Epoch [1/100], Step [1240/1751], Loss: 3.5262\n",
      "Epoch [1/100], Step [1250/1751], Loss: 3.5577\n",
      "Epoch [1/100], Step [1260/1751], Loss: 3.5087\n",
      "Epoch [1/100], Step [1270/1751], Loss: 3.4329\n",
      "Epoch [1/100], Step [1280/1751], Loss: 3.4323\n",
      "Epoch [1/100], Step [1290/1751], Loss: 3.4837\n",
      "Epoch [1/100], Step [1300/1751], Loss: 3.4704\n",
      "Epoch [1/100], Step [1310/1751], Loss: 3.4276\n",
      "Epoch [1/100], Step [1320/1751], Loss: 3.4492\n",
      "Epoch [1/100], Step [1330/1751], Loss: 3.4251\n",
      "Epoch [1/100], Step [1340/1751], Loss: 3.4328\n",
      "Epoch [1/100], Step [1350/1751], Loss: 3.4210\n",
      "Epoch [1/100], Step [1360/1751], Loss: 3.3445\n",
      "Epoch [1/100], Step [1370/1751], Loss: 3.4954\n",
      "Epoch [1/100], Step [1380/1751], Loss: 3.3622\n",
      "Epoch [1/100], Step [1390/1751], Loss: 3.4539\n",
      "Epoch [1/100], Step [1400/1751], Loss: 3.4426\n",
      "Epoch [1/100], Step [1410/1751], Loss: 3.4981\n",
      "Epoch [1/100], Step [1420/1751], Loss: 3.4333\n",
      "Epoch [1/100], Step [1430/1751], Loss: 3.4224\n",
      "Epoch [1/100], Step [1440/1751], Loss: 3.3550\n",
      "Epoch [1/100], Step [1450/1751], Loss: 3.3290\n",
      "Epoch [1/100], Step [1460/1751], Loss: 3.3232\n",
      "Epoch [1/100], Step [1470/1751], Loss: 3.3179\n",
      "Epoch [1/100], Step [1480/1751], Loss: 3.4261\n",
      "Epoch [1/100], Step [1490/1751], Loss: 3.2885\n",
      "Epoch [1/100], Step [1500/1751], Loss: 3.3962\n",
      "Epoch [1/100], Step [1510/1751], Loss: 3.4081\n",
      "Epoch [1/100], Step [1520/1751], Loss: 3.4646\n",
      "Epoch [1/100], Step [1530/1751], Loss: 3.3196\n",
      "Epoch [1/100], Step [1540/1751], Loss: 3.3309\n",
      "Epoch [1/100], Step [1550/1751], Loss: 3.3180\n",
      "Epoch [1/100], Step [1560/1751], Loss: 3.3160\n",
      "Epoch [1/100], Step [1570/1751], Loss: 3.3453\n",
      "Epoch [1/100], Step [1580/1751], Loss: 3.2488\n",
      "Epoch [1/100], Step [1590/1751], Loss: 3.2661\n",
      "Epoch [1/100], Step [1600/1751], Loss: 3.2399\n",
      "Epoch [1/100], Step [1610/1751], Loss: 3.2628\n",
      "Epoch [1/100], Step [1620/1751], Loss: 3.2776\n",
      "Epoch [1/100], Step [1630/1751], Loss: 3.2355\n",
      "Epoch [1/100], Step [1640/1751], Loss: 3.3608\n",
      "Epoch [1/100], Step [1650/1751], Loss: 3.1844\n",
      "Epoch [1/100], Step [1660/1751], Loss: 3.2993\n",
      "Epoch [1/100], Step [1670/1751], Loss: 3.2626\n",
      "Epoch [1/100], Step [1680/1751], Loss: 3.1982\n",
      "Epoch [1/100], Step [1690/1751], Loss: 3.2129\n",
      "Epoch [1/100], Step [1700/1751], Loss: 3.2218\n",
      "Epoch [1/100], Step [1710/1751], Loss: 3.2380\n",
      "Epoch [1/100], Step [1720/1751], Loss: 3.2983\n",
      "Epoch [1/100], Step [1730/1751], Loss: 3.2950\n",
      "Epoch [1/100], Step [1740/1751], Loss: 3.2458\n",
      "Epoch [1/100], Step [1750/1751], Loss: 3.1822\n",
      "Epoch [1/100], Average Loss: 3.7145, Time: 1619.7914s\n",
      "Epoch [2/100], Step [10/1751], Loss: 3.2347\n",
      "Epoch [2/100], Step [20/1751], Loss: 3.2171\n",
      "Epoch [2/100], Step [30/1751], Loss: 3.1924\n",
      "Epoch [2/100], Step [40/1751], Loss: 3.1937\n",
      "Epoch [2/100], Step [50/1751], Loss: 3.1580\n",
      "Epoch [2/100], Step [60/1751], Loss: 3.1690\n",
      "Epoch [2/100], Step [70/1751], Loss: 3.1379\n",
      "Epoch [2/100], Step [80/1751], Loss: 3.1592\n",
      "Epoch [2/100], Step [90/1751], Loss: 3.1519\n",
      "Epoch [2/100], Step [100/1751], Loss: 3.1695\n",
      "Epoch [2/100], Step [110/1751], Loss: 3.0681\n",
      "Epoch [2/100], Step [120/1751], Loss: 3.1334\n",
      "Epoch [2/100], Step [130/1751], Loss: 3.2022\n",
      "Epoch [2/100], Step [140/1751], Loss: 3.1218\n",
      "Epoch [2/100], Step [150/1751], Loss: 3.1315\n",
      "Epoch [2/100], Step [160/1751], Loss: 3.1399\n",
      "Epoch [2/100], Step [170/1751], Loss: 3.1519\n",
      "Epoch [2/100], Step [180/1751], Loss: 3.1579\n",
      "Epoch [2/100], Step [190/1751], Loss: 3.1551\n",
      "Epoch [2/100], Step [200/1751], Loss: 3.0562\n",
      "Epoch [2/100], Step [210/1751], Loss: 3.1225\n",
      "Epoch [2/100], Step [220/1751], Loss: 3.0761\n",
      "Epoch [2/100], Step [230/1751], Loss: 3.0631\n",
      "Epoch [2/100], Step [240/1751], Loss: 2.9923\n",
      "Epoch [2/100], Step [250/1751], Loss: 3.0544\n",
      "Epoch [2/100], Step [260/1751], Loss: 3.0179\n",
      "Epoch [2/100], Step [270/1751], Loss: 3.1515\n",
      "Epoch [2/100], Step [280/1751], Loss: 3.1195\n",
      "Epoch [2/100], Step [290/1751], Loss: 3.1028\n",
      "Epoch [2/100], Step [300/1751], Loss: 3.0441\n",
      "Epoch [2/100], Step [310/1751], Loss: 3.0984\n",
      "Epoch [2/100], Step [320/1751], Loss: 2.9823\n",
      "Epoch [2/100], Step [330/1751], Loss: 3.0358\n",
      "Epoch [2/100], Step [340/1751], Loss: 3.0669\n",
      "Epoch [2/100], Step [350/1751], Loss: 2.9790\n",
      "Epoch [2/100], Step [360/1751], Loss: 3.0967\n",
      "Epoch [2/100], Step [370/1751], Loss: 2.9263\n",
      "Epoch [2/100], Step [380/1751], Loss: 3.0186\n",
      "Epoch [2/100], Step [390/1751], Loss: 3.0476\n",
      "Epoch [2/100], Step [400/1751], Loss: 3.0320\n",
      "Epoch [2/100], Step [410/1751], Loss: 3.0024\n",
      "Epoch [2/100], Step [420/1751], Loss: 2.9875\n",
      "Epoch [2/100], Step [430/1751], Loss: 3.0275\n",
      "Epoch [2/100], Step [440/1751], Loss: 2.9428\n",
      "Epoch [2/100], Step [450/1751], Loss: 3.0583\n",
      "Epoch [2/100], Step [460/1751], Loss: 2.9933\n",
      "Epoch [2/100], Step [470/1751], Loss: 2.9473\n",
      "Epoch [2/100], Step [480/1751], Loss: 3.1393\n",
      "Epoch [2/100], Step [490/1751], Loss: 2.9384\n",
      "Epoch [2/100], Step [500/1751], Loss: 3.0849\n",
      "Epoch [2/100], Step [510/1751], Loss: 2.9250\n",
      "Epoch [2/100], Step [520/1751], Loss: 2.9489\n",
      "Epoch [2/100], Step [530/1751], Loss: 2.9192\n",
      "Epoch [2/100], Step [540/1751], Loss: 2.9685\n",
      "Epoch [2/100], Step [550/1751], Loss: 2.9166\n",
      "Epoch [2/100], Step [560/1751], Loss: 2.8495\n",
      "Epoch [2/100], Step [570/1751], Loss: 3.0232\n",
      "Epoch [2/100], Step [580/1751], Loss: 2.9610\n",
      "Epoch [2/100], Step [590/1751], Loss: 2.9612\n",
      "Epoch [2/100], Step [600/1751], Loss: 2.9334\n",
      "Epoch [2/100], Step [610/1751], Loss: 2.8128\n",
      "Epoch [2/100], Step [620/1751], Loss: 2.9976\n",
      "Epoch [2/100], Step [630/1751], Loss: 2.9272\n",
      "Epoch [2/100], Step [640/1751], Loss: 2.9869\n",
      "Epoch [2/100], Step [650/1751], Loss: 2.9216\n",
      "Epoch [2/100], Step [660/1751], Loss: 2.8976\n",
      "Epoch [2/100], Step [670/1751], Loss: 2.9261\n",
      "Epoch [2/100], Step [680/1751], Loss: 2.9637\n",
      "Epoch [2/100], Step [690/1751], Loss: 2.8120\n",
      "Epoch [2/100], Step [700/1751], Loss: 2.8181\n",
      "Epoch [2/100], Step [710/1751], Loss: 2.8258\n",
      "Epoch [2/100], Step [720/1751], Loss: 2.9110\n",
      "Epoch [2/100], Step [730/1751], Loss: 2.7675\n",
      "Epoch [2/100], Step [740/1751], Loss: 2.8898\n",
      "Epoch [2/100], Step [750/1751], Loss: 2.8539\n",
      "Epoch [2/100], Step [760/1751], Loss: 2.8398\n",
      "Epoch [2/100], Step [770/1751], Loss: 2.7828\n",
      "Epoch [2/100], Step [780/1751], Loss: 2.9058\n",
      "Epoch [2/100], Step [790/1751], Loss: 2.7719\n",
      "Epoch [2/100], Step [800/1751], Loss: 2.8734\n",
      "Epoch [2/100], Step [810/1751], Loss: 2.7907\n",
      "Epoch [2/100], Step [820/1751], Loss: 2.8701\n",
      "Epoch [2/100], Step [830/1751], Loss: 2.7523\n",
      "Epoch [2/100], Step [840/1751], Loss: 2.8000\n",
      "Epoch [2/100], Step [850/1751], Loss: 2.8551\n",
      "Epoch [2/100], Step [860/1751], Loss: 2.8582\n",
      "Epoch [2/100], Step [870/1751], Loss: 2.7615\n",
      "Epoch [2/100], Step [880/1751], Loss: 2.7773\n",
      "Epoch [2/100], Step [890/1751], Loss: 2.6960\n",
      "Epoch [2/100], Step [900/1751], Loss: 2.8113\n",
      "Epoch [2/100], Step [910/1751], Loss: 2.7664\n",
      "Epoch [2/100], Step [920/1751], Loss: 2.8938\n",
      "Epoch [2/100], Step [930/1751], Loss: 2.7443\n",
      "Epoch [2/100], Step [940/1751], Loss: 2.6695\n",
      "Epoch [2/100], Step [950/1751], Loss: 2.7068\n",
      "Epoch [2/100], Step [960/1751], Loss: 2.7461\n",
      "Epoch [2/100], Step [970/1751], Loss: 2.7361\n",
      "Epoch [2/100], Step [980/1751], Loss: 2.8000\n",
      "Epoch [2/100], Step [990/1751], Loss: 2.8282\n",
      "Epoch [2/100], Step [1000/1751], Loss: 2.6889\n",
      "Epoch [2/100], Step [1010/1751], Loss: 2.6297\n",
      "Epoch [2/100], Step [1020/1751], Loss: 2.7925\n",
      "Epoch [2/100], Step [1030/1751], Loss: 2.6363\n",
      "Epoch [2/100], Step [1040/1751], Loss: 2.6374\n",
      "Epoch [2/100], Step [1050/1751], Loss: 2.7394\n",
      "Epoch [2/100], Step [1060/1751], Loss: 2.7265\n",
      "Epoch [2/100], Step [1070/1751], Loss: 2.7079\n",
      "Epoch [2/100], Step [1080/1751], Loss: 2.6280\n",
      "Epoch [2/100], Step [1090/1751], Loss: 2.8351\n",
      "Epoch [2/100], Step [1100/1751], Loss: 2.6062\n",
      "Epoch [2/100], Step [1110/1751], Loss: 2.6981\n",
      "Epoch [2/100], Step [1120/1751], Loss: 2.7578\n",
      "Epoch [2/100], Step [1130/1751], Loss: 2.5871\n",
      "Epoch [2/100], Step [1140/1751], Loss: 2.7002\n",
      "Epoch [2/100], Step [1150/1751], Loss: 2.6055\n",
      "Epoch [2/100], Step [1160/1751], Loss: 2.6782\n",
      "Epoch [2/100], Step [1170/1751], Loss: 2.6622\n",
      "Epoch [2/100], Step [1180/1751], Loss: 2.5777\n",
      "Epoch [2/100], Step [1190/1751], Loss: 2.5899\n",
      "Epoch [2/100], Step [1200/1751], Loss: 2.7856\n",
      "Epoch [2/100], Step [1210/1751], Loss: 2.8266\n",
      "Epoch [2/100], Step [1220/1751], Loss: 2.6331\n",
      "Epoch [2/100], Step [1230/1751], Loss: 2.6847\n",
      "Epoch [2/100], Step [1240/1751], Loss: 2.6587\n",
      "Epoch [2/100], Step [1250/1751], Loss: 2.5911\n",
      "Epoch [2/100], Step [1260/1751], Loss: 2.5481\n",
      "Epoch [2/100], Step [1270/1751], Loss: 2.6495\n",
      "Epoch [2/100], Step [1280/1751], Loss: 2.6663\n",
      "Epoch [2/100], Step [1290/1751], Loss: 2.6430\n",
      "Epoch [2/100], Step [1300/1751], Loss: 2.6359\n",
      "Epoch [2/100], Step [1310/1751], Loss: 2.7348\n",
      "Epoch [2/100], Step [1320/1751], Loss: 2.5244\n",
      "Epoch [2/100], Step [1330/1751], Loss: 2.7078\n",
      "Epoch [2/100], Step [1340/1751], Loss: 2.6720\n",
      "Epoch [2/100], Step [1350/1751], Loss: 2.6754\n",
      "Epoch [2/100], Step [1360/1751], Loss: 2.6281\n",
      "Epoch [2/100], Step [1370/1751], Loss: 2.5548\n",
      "Epoch [2/100], Step [1380/1751], Loss: 2.5990\n",
      "Epoch [2/100], Step [1390/1751], Loss: 2.6471\n",
      "Epoch [2/100], Step [1400/1751], Loss: 2.6466\n",
      "Epoch [2/100], Step [1410/1751], Loss: 2.6463\n",
      "Epoch [2/100], Step [1420/1751], Loss: 2.6332\n",
      "Epoch [2/100], Step [1430/1751], Loss: 2.7004\n",
      "Epoch [2/100], Step [1440/1751], Loss: 2.5788\n",
      "Epoch [2/100], Step [1450/1751], Loss: 2.6194\n",
      "Epoch [2/100], Step [1460/1751], Loss: 2.6528\n",
      "Epoch [2/100], Step [1470/1751], Loss: 2.5595\n",
      "Epoch [2/100], Step [1480/1751], Loss: 2.5664\n",
      "Epoch [2/100], Step [1490/1751], Loss: 2.5781\n",
      "Epoch [2/100], Step [1500/1751], Loss: 2.4974\n",
      "Epoch [2/100], Step [1510/1751], Loss: 2.5519\n",
      "Epoch [2/100], Step [1520/1751], Loss: 2.4758\n",
      "Epoch [2/100], Step [1530/1751], Loss: 2.4757\n",
      "Epoch [2/100], Step [1540/1751], Loss: 2.4859\n",
      "Epoch [2/100], Step [1550/1751], Loss: 2.5331\n",
      "Epoch [2/100], Step [1560/1751], Loss: 2.6070\n",
      "Epoch [2/100], Step [1570/1751], Loss: 2.6132\n",
      "Epoch [2/100], Step [1580/1751], Loss: 2.5094\n",
      "Epoch [2/100], Step [1590/1751], Loss: 2.5357\n",
      "Epoch [2/100], Step [1600/1751], Loss: 2.5642\n",
      "Epoch [2/100], Step [1610/1751], Loss: 2.6139\n",
      "Epoch [2/100], Step [1620/1751], Loss: 2.4611\n",
      "Epoch [2/100], Step [1630/1751], Loss: 2.5605\n",
      "Epoch [2/100], Step [1640/1751], Loss: 2.4854\n",
      "Epoch [2/100], Step [1650/1751], Loss: 2.4247\n",
      "Epoch [2/100], Step [1660/1751], Loss: 2.5250\n",
      "Epoch [2/100], Step [1670/1751], Loss: 2.2934\n",
      "Epoch [2/100], Step [1680/1751], Loss: 2.6002\n",
      "Epoch [2/100], Step [1690/1751], Loss: 2.5254\n",
      "Epoch [2/100], Step [1700/1751], Loss: 2.7242\n",
      "Epoch [2/100], Step [1710/1751], Loss: 2.6137\n",
      "Epoch [2/100], Step [1720/1751], Loss: 2.5162\n",
      "Epoch [2/100], Step [1730/1751], Loss: 2.4630\n",
      "Epoch [2/100], Step [1740/1751], Loss: 2.5635\n",
      "Epoch [2/100], Step [1750/1751], Loss: 2.3761\n",
      "Epoch [2/100], Average Loss: 2.8151, Time: 1634.6513s\n",
      "Epoch [3/100], Step [10/1751], Loss: 2.4690\n",
      "Epoch [3/100], Step [20/1751], Loss: 2.5706\n",
      "Epoch [3/100], Step [30/1751], Loss: 2.3579\n",
      "Epoch [3/100], Step [40/1751], Loss: 2.4444\n",
      "Epoch [3/100], Step [50/1751], Loss: 2.4969\n",
      "Epoch [3/100], Step [60/1751], Loss: 2.5244\n",
      "Epoch [3/100], Step [70/1751], Loss: 2.3153\n",
      "Epoch [3/100], Step [80/1751], Loss: 2.4711\n",
      "Epoch [3/100], Step [90/1751], Loss: 2.4643\n",
      "Epoch [3/100], Step [100/1751], Loss: 2.2349\n",
      "Epoch [3/100], Step [110/1751], Loss: 2.4813\n",
      "Epoch [3/100], Step [120/1751], Loss: 2.4694\n",
      "Epoch [3/100], Step [130/1751], Loss: 2.5119\n",
      "Epoch [3/100], Step [140/1751], Loss: 2.4393\n",
      "Epoch [3/100], Step [150/1751], Loss: 2.4720\n",
      "Epoch [3/100], Step [160/1751], Loss: 2.3417\n",
      "Epoch [3/100], Step [170/1751], Loss: 2.3540\n",
      "Epoch [3/100], Step [180/1751], Loss: 2.3870\n",
      "Epoch [3/100], Step [190/1751], Loss: 2.5183\n",
      "Epoch [3/100], Step [200/1751], Loss: 2.3413\n",
      "Epoch [3/100], Step [210/1751], Loss: 2.5182\n",
      "Epoch [3/100], Step [220/1751], Loss: 2.4239\n",
      "Epoch [3/100], Step [230/1751], Loss: 2.5060\n",
      "Epoch [3/100], Step [240/1751], Loss: 2.4762\n",
      "Epoch [3/100], Step [250/1751], Loss: 2.4813\n",
      "Epoch [3/100], Step [260/1751], Loss: 2.4588\n",
      "Epoch [3/100], Step [270/1751], Loss: 2.3798\n",
      "Epoch [3/100], Step [280/1751], Loss: 2.3960\n",
      "Epoch [3/100], Step [290/1751], Loss: 2.4432\n",
      "Epoch [3/100], Step [300/1751], Loss: 2.4425\n",
      "Epoch [3/100], Step [310/1751], Loss: 2.3479\n",
      "Epoch [3/100], Step [320/1751], Loss: 2.3343\n",
      "Epoch [3/100], Step [330/1751], Loss: 2.4053\n",
      "Epoch [3/100], Step [340/1751], Loss: 2.4559\n",
      "Epoch [3/100], Step [350/1751], Loss: 2.3571\n",
      "Epoch [3/100], Step [360/1751], Loss: 2.3023\n",
      "Epoch [3/100], Step [370/1751], Loss: 2.3321\n",
      "Epoch [3/100], Step [380/1751], Loss: 2.4324\n",
      "Epoch [3/100], Step [390/1751], Loss: 2.3699\n",
      "Epoch [3/100], Step [400/1751], Loss: 2.3156\n",
      "Epoch [3/100], Step [410/1751], Loss: 2.4223\n",
      "Epoch [3/100], Step [420/1751], Loss: 2.2220\n",
      "Epoch [3/100], Step [430/1751], Loss: 2.3383\n",
      "Epoch [3/100], Step [440/1751], Loss: 2.3214\n",
      "Epoch [3/100], Step [450/1751], Loss: 2.2788\n",
      "Epoch [3/100], Step [460/1751], Loss: 2.2764\n",
      "Epoch [3/100], Step [470/1751], Loss: 2.3913\n",
      "Epoch [3/100], Step [480/1751], Loss: 2.4475\n",
      "Epoch [3/100], Step [490/1751], Loss: 2.2883\n",
      "Epoch [3/100], Step [500/1751], Loss: 2.3618\n",
      "Epoch [3/100], Step [510/1751], Loss: 2.3174\n",
      "Epoch [3/100], Step [520/1751], Loss: 2.3279\n",
      "Epoch [3/100], Step [530/1751], Loss: 2.4185\n",
      "Epoch [3/100], Step [540/1751], Loss: 2.2343\n",
      "Epoch [3/100], Step [550/1751], Loss: 2.3263\n",
      "Epoch [3/100], Step [560/1751], Loss: 2.4188\n",
      "Epoch [3/100], Step [570/1751], Loss: 2.5110\n",
      "Epoch [3/100], Step [580/1751], Loss: 2.1750\n",
      "Epoch [3/100], Step [590/1751], Loss: 2.1798\n",
      "Epoch [3/100], Step [600/1751], Loss: 2.4210\n",
      "Epoch [3/100], Step [610/1751], Loss: 2.3239\n",
      "Epoch [3/100], Step [620/1751], Loss: 2.1818\n",
      "Epoch [3/100], Step [630/1751], Loss: 2.2814\n",
      "Epoch [3/100], Step [640/1751], Loss: 2.2818\n",
      "Epoch [3/100], Step [650/1751], Loss: 2.2703\n",
      "Epoch [3/100], Step [660/1751], Loss: 2.2533\n",
      "Epoch [3/100], Step [670/1751], Loss: 2.4072\n",
      "Epoch [3/100], Step [680/1751], Loss: 2.2114\n",
      "Epoch [3/100], Step [690/1751], Loss: 2.2968\n",
      "Epoch [3/100], Step [700/1751], Loss: 2.0938\n",
      "Epoch [3/100], Step [710/1751], Loss: 2.2735\n",
      "Epoch [3/100], Step [720/1751], Loss: 2.2998\n",
      "Epoch [3/100], Step [730/1751], Loss: 2.2866\n",
      "Epoch [3/100], Step [740/1751], Loss: 2.3344\n",
      "Epoch [3/100], Step [750/1751], Loss: 2.2554\n",
      "Epoch [3/100], Step [760/1751], Loss: 2.2417\n",
      "Epoch [3/100], Step [770/1751], Loss: 2.2780\n",
      "Epoch [3/100], Step [780/1751], Loss: 2.1868\n",
      "Epoch [3/100], Step [790/1751], Loss: 2.3343\n",
      "Epoch [3/100], Step [800/1751], Loss: 2.3445\n",
      "Epoch [3/100], Step [810/1751], Loss: 2.3505\n",
      "Epoch [3/100], Step [820/1751], Loss: 2.1206\n",
      "Epoch [3/100], Step [830/1751], Loss: 2.2526\n",
      "Epoch [3/100], Step [840/1751], Loss: 2.2516\n",
      "Epoch [3/100], Step [850/1751], Loss: 2.2054\n",
      "Epoch [3/100], Step [860/1751], Loss: 2.2362\n",
      "Epoch [3/100], Step [870/1751], Loss: 2.2531\n",
      "Epoch [3/100], Step [880/1751], Loss: 2.0560\n",
      "Epoch [3/100], Step [890/1751], Loss: 2.2391\n",
      "Epoch [3/100], Step [900/1751], Loss: 2.3884\n",
      "Epoch [3/100], Step [910/1751], Loss: 2.3639\n",
      "Epoch [3/100], Step [920/1751], Loss: 2.3281\n",
      "Epoch [3/100], Step [930/1751], Loss: 2.3099\n",
      "Epoch [3/100], Step [940/1751], Loss: 2.3184\n",
      "Epoch [3/100], Step [950/1751], Loss: 2.0635\n",
      "Epoch [3/100], Step [960/1751], Loss: 2.3556\n",
      "Epoch [3/100], Step [970/1751], Loss: 2.1336\n",
      "Epoch [3/100], Step [980/1751], Loss: 2.1198\n",
      "Epoch [3/100], Step [990/1751], Loss: 2.1140\n",
      "Epoch [3/100], Step [1000/1751], Loss: 2.2293\n",
      "Epoch [3/100], Step [1010/1751], Loss: 2.1652\n",
      "Epoch [3/100], Step [1020/1751], Loss: 2.2881\n",
      "Epoch [3/100], Step [1030/1751], Loss: 2.3231\n",
      "Epoch [3/100], Step [1040/1751], Loss: 2.2526\n",
      "Epoch [3/100], Step [1050/1751], Loss: 2.2425\n",
      "Epoch [3/100], Step [1060/1751], Loss: 2.0984\n",
      "Epoch [3/100], Step [1070/1751], Loss: 2.3096\n",
      "Epoch [3/100], Step [1080/1751], Loss: 2.0309\n",
      "Epoch [3/100], Step [1090/1751], Loss: 2.0921\n",
      "Epoch [3/100], Step [1100/1751], Loss: 2.2292\n",
      "Epoch [3/100], Step [1110/1751], Loss: 2.0822\n",
      "Epoch [3/100], Step [1120/1751], Loss: 2.1537\n",
      "Epoch [3/100], Step [1130/1751], Loss: 2.0971\n",
      "Epoch [3/100], Step [1140/1751], Loss: 2.0584\n",
      "Epoch [3/100], Step [1150/1751], Loss: 2.2436\n",
      "Epoch [3/100], Step [1160/1751], Loss: 2.2434\n",
      "Epoch [3/100], Step [1170/1751], Loss: 1.9813\n",
      "Epoch [3/100], Step [1180/1751], Loss: 2.2569\n",
      "Epoch [3/100], Step [1190/1751], Loss: 2.1380\n",
      "Epoch [3/100], Step [1200/1751], Loss: 2.1998\n",
      "Epoch [3/100], Step [1210/1751], Loss: 2.2151\n",
      "Epoch [3/100], Step [1220/1751], Loss: 2.0341\n",
      "Epoch [3/100], Step [1230/1751], Loss: 2.1432\n",
      "Epoch [3/100], Step [1240/1751], Loss: 2.3434\n",
      "Epoch [3/100], Step [1250/1751], Loss: 2.1284\n",
      "Epoch [3/100], Step [1260/1751], Loss: 2.0820\n",
      "Epoch [3/100], Step [1270/1751], Loss: 2.1873\n",
      "Epoch [3/100], Step [1280/1751], Loss: 2.1114\n",
      "Epoch [3/100], Step [1290/1751], Loss: 2.1435\n",
      "Epoch [3/100], Step [1300/1751], Loss: 2.2875\n",
      "Epoch [3/100], Step [1310/1751], Loss: 2.0596\n",
      "Epoch [3/100], Step [1320/1751], Loss: 2.2174\n",
      "Epoch [3/100], Step [1330/1751], Loss: 2.0845\n",
      "Epoch [3/100], Step [1340/1751], Loss: 2.0991\n",
      "Epoch [3/100], Step [1350/1751], Loss: 2.1375\n",
      "Epoch [3/100], Step [1360/1751], Loss: 2.2157\n",
      "Epoch [3/100], Step [1370/1751], Loss: 2.1347\n",
      "Epoch [3/100], Step [1380/1751], Loss: 2.2617\n",
      "Epoch [3/100], Step [1390/1751], Loss: 2.2621\n",
      "Epoch [3/100], Step [1400/1751], Loss: 2.0126\n",
      "Epoch [3/100], Step [1410/1751], Loss: 2.1747\n",
      "Epoch [3/100], Step [1420/1751], Loss: 2.0933\n",
      "Epoch [3/100], Step [1430/1751], Loss: 2.2959\n",
      "Epoch [3/100], Step [1440/1751], Loss: 2.1048\n",
      "Epoch [3/100], Step [1450/1751], Loss: 2.1014\n",
      "Epoch [3/100], Step [1460/1751], Loss: 2.2114\n",
      "Epoch [3/100], Step [1470/1751], Loss: 2.2059\n",
      "Epoch [3/100], Step [1480/1751], Loss: 2.1848\n",
      "Epoch [3/100], Step [1490/1751], Loss: 2.1274\n",
      "Epoch [3/100], Step [1500/1751], Loss: 2.1515\n",
      "Epoch [3/100], Step [1510/1751], Loss: 2.2210\n",
      "Epoch [3/100], Step [1520/1751], Loss: 2.0719\n",
      "Epoch [3/100], Step [1530/1751], Loss: 2.0235\n",
      "Epoch [3/100], Step [1540/1751], Loss: 2.1144\n",
      "Epoch [3/100], Step [1550/1751], Loss: 2.1993\n",
      "Epoch [3/100], Step [1560/1751], Loss: 2.1652\n",
      "Epoch [3/100], Step [1570/1751], Loss: 2.0337\n",
      "Epoch [3/100], Step [1580/1751], Loss: 2.2612\n",
      "Epoch [3/100], Step [1590/1751], Loss: 2.2580\n",
      "Epoch [3/100], Step [1600/1751], Loss: 2.0766\n",
      "Epoch [3/100], Step [1610/1751], Loss: 2.1043\n",
      "Epoch [3/100], Step [1620/1751], Loss: 1.9762\n",
      "Epoch [3/100], Step [1630/1751], Loss: 2.1623\n",
      "Epoch [3/100], Step [1640/1751], Loss: 2.0650\n",
      "Epoch [3/100], Step [1650/1751], Loss: 2.1634\n",
      "Epoch [3/100], Step [1660/1751], Loss: 2.0594\n",
      "Epoch [3/100], Step [1670/1751], Loss: 2.0720\n",
      "Epoch [3/100], Step [1680/1751], Loss: 1.8949\n",
      "Epoch [3/100], Step [1690/1751], Loss: 2.0057\n",
      "Epoch [3/100], Step [1700/1751], Loss: 2.0229\n",
      "Epoch [3/100], Step [1710/1751], Loss: 2.0298\n",
      "Epoch [3/100], Step [1720/1751], Loss: 1.9862\n",
      "Epoch [3/100], Step [1730/1751], Loss: 2.1589\n",
      "Epoch [3/100], Step [1740/1751], Loss: 2.2636\n",
      "Epoch [3/100], Step [1750/1751], Loss: 2.0437\n",
      "Epoch [3/100], Average Loss: 2.2607, Time: 1635.5460s\n",
      "Epoch [4/100], Step [10/1751], Loss: 1.9288\n",
      "Epoch [4/100], Step [20/1751], Loss: 1.9747\n",
      "Epoch [4/100], Step [30/1751], Loss: 1.9985\n",
      "Epoch [4/100], Step [40/1751], Loss: 2.1312\n",
      "Epoch [4/100], Step [50/1751], Loss: 1.9715\n",
      "Epoch [4/100], Step [60/1751], Loss: 2.1060\n",
      "Epoch [4/100], Step [70/1751], Loss: 1.9853\n",
      "Epoch [4/100], Step [80/1751], Loss: 2.2816\n",
      "Epoch [4/100], Step [90/1751], Loss: 2.0067\n",
      "Epoch [4/100], Step [100/1751], Loss: 2.1758\n",
      "Epoch [4/100], Step [110/1751], Loss: 1.9288\n",
      "Epoch [4/100], Step [120/1751], Loss: 1.9051\n",
      "Epoch [4/100], Step [130/1751], Loss: 2.0211\n",
      "Epoch [4/100], Step [140/1751], Loss: 2.1009\n",
      "Epoch [4/100], Step [150/1751], Loss: 2.1840\n",
      "Epoch [4/100], Step [160/1751], Loss: 2.0352\n",
      "Epoch [4/100], Step [170/1751], Loss: 1.9310\n",
      "Epoch [4/100], Step [180/1751], Loss: 2.1612\n",
      "Epoch [4/100], Step [190/1751], Loss: 2.0541\n",
      "Epoch [4/100], Step [200/1751], Loss: 2.0209\n",
      "Epoch [4/100], Step [210/1751], Loss: 1.8115\n",
      "Epoch [4/100], Step [220/1751], Loss: 2.0493\n",
      "Epoch [4/100], Step [230/1751], Loss: 1.8363\n",
      "Epoch [4/100], Step [240/1751], Loss: 2.0564\n",
      "Epoch [4/100], Step [250/1751], Loss: 2.0738\n",
      "Epoch [4/100], Step [260/1751], Loss: 1.8604\n",
      "Epoch [4/100], Step [270/1751], Loss: 2.1027\n",
      "Epoch [4/100], Step [280/1751], Loss: 1.9485\n",
      "Epoch [4/100], Step [290/1751], Loss: 2.0637\n",
      "Epoch [4/100], Step [300/1751], Loss: 2.0721\n",
      "Epoch [4/100], Step [310/1751], Loss: 2.2424\n",
      "Epoch [4/100], Step [320/1751], Loss: 1.8642\n",
      "Epoch [4/100], Step [330/1751], Loss: 1.8879\n",
      "Epoch [4/100], Step [340/1751], Loss: 1.9874\n",
      "Epoch [4/100], Step [350/1751], Loss: 1.9989\n",
      "Epoch [4/100], Step [360/1751], Loss: 2.0381\n",
      "Epoch [4/100], Step [370/1751], Loss: 1.9641\n",
      "Epoch [4/100], Step [380/1751], Loss: 2.0759\n",
      "Epoch [4/100], Step [390/1751], Loss: 1.9443\n",
      "Epoch [4/100], Step [400/1751], Loss: 1.9900\n",
      "Epoch [4/100], Step [410/1751], Loss: 2.1134\n",
      "Epoch [4/100], Step [420/1751], Loss: 1.9240\n",
      "Epoch [4/100], Step [430/1751], Loss: 2.0046\n",
      "Epoch [4/100], Step [440/1751], Loss: 1.9905\n",
      "Epoch [4/100], Step [450/1751], Loss: 1.9580\n",
      "Epoch [4/100], Step [460/1751], Loss: 1.9540\n",
      "Epoch [4/100], Step [470/1751], Loss: 1.9423\n",
      "Epoch [4/100], Step [480/1751], Loss: 2.0011\n",
      "Epoch [4/100], Step [490/1751], Loss: 1.8664\n",
      "Epoch [4/100], Step [500/1751], Loss: 1.9313\n",
      "Epoch [4/100], Step [510/1751], Loss: 2.0348\n",
      "Epoch [4/100], Step [520/1751], Loss: 1.8851\n",
      "Epoch [4/100], Step [530/1751], Loss: 2.0664\n",
      "Epoch [4/100], Step [540/1751], Loss: 1.9810\n",
      "Epoch [4/100], Step [550/1751], Loss: 2.0167\n",
      "Epoch [4/100], Step [560/1751], Loss: 1.8858\n",
      "Epoch [4/100], Step [570/1751], Loss: 2.0571\n",
      "Epoch [4/100], Step [580/1751], Loss: 2.1561\n",
      "Epoch [4/100], Step [590/1751], Loss: 1.9920\n",
      "Epoch [4/100], Step [600/1751], Loss: 1.9673\n",
      "Epoch [4/100], Step [610/1751], Loss: 2.0016\n",
      "Epoch [4/100], Step [620/1751], Loss: 1.9021\n",
      "Epoch [4/100], Step [630/1751], Loss: 2.0193\n",
      "Epoch [4/100], Step [640/1751], Loss: 1.8313\n",
      "Epoch [4/100], Step [650/1751], Loss: 2.0127\n",
      "Epoch [4/100], Step [660/1751], Loss: 1.9638\n",
      "Epoch [4/100], Step [670/1751], Loss: 2.0480\n",
      "Epoch [4/100], Step [680/1751], Loss: 2.0048\n",
      "Epoch [4/100], Step [690/1751], Loss: 1.9654\n",
      "Epoch [4/100], Step [700/1751], Loss: 1.9650\n",
      "Epoch [4/100], Step [710/1751], Loss: 1.8956\n",
      "Epoch [4/100], Step [720/1751], Loss: 1.9938\n",
      "Epoch [4/100], Step [730/1751], Loss: 1.8144\n",
      "Epoch [4/100], Step [740/1751], Loss: 1.9701\n",
      "Epoch [4/100], Step [750/1751], Loss: 1.9245\n",
      "Epoch [4/100], Step [760/1751], Loss: 1.8393\n",
      "Epoch [4/100], Step [770/1751], Loss: 1.8150\n",
      "Epoch [4/100], Step [780/1751], Loss: 1.8862\n",
      "Epoch [4/100], Step [790/1751], Loss: 1.8173\n",
      "Epoch [4/100], Step [800/1751], Loss: 2.0136\n",
      "Epoch [4/100], Step [810/1751], Loss: 2.1185\n",
      "Epoch [4/100], Step [820/1751], Loss: 2.0206\n",
      "Epoch [4/100], Step [830/1751], Loss: 1.9037\n",
      "Epoch [4/100], Step [840/1751], Loss: 1.8574\n",
      "Epoch [4/100], Step [850/1751], Loss: 1.9839\n",
      "Epoch [4/100], Step [860/1751], Loss: 1.8524\n",
      "Epoch [4/100], Step [870/1751], Loss: 2.0105\n",
      "Epoch [4/100], Step [880/1751], Loss: 1.8734\n",
      "Epoch [4/100], Step [890/1751], Loss: 1.8175\n",
      "Epoch [4/100], Step [900/1751], Loss: 2.0536\n",
      "Epoch [4/100], Step [910/1751], Loss: 1.9548\n",
      "Epoch [4/100], Step [920/1751], Loss: 1.9731\n",
      "Epoch [4/100], Step [930/1751], Loss: 1.8604\n",
      "Epoch [4/100], Step [940/1751], Loss: 1.9146\n",
      "Epoch [4/100], Step [950/1751], Loss: 1.8903\n",
      "Epoch [4/100], Step [960/1751], Loss: 1.9337\n",
      "Epoch [4/100], Step [970/1751], Loss: 2.0231\n",
      "Epoch [4/100], Step [980/1751], Loss: 1.9150\n",
      "Epoch [4/100], Step [990/1751], Loss: 2.1188\n",
      "Epoch [4/100], Step [1000/1751], Loss: 1.8963\n",
      "Epoch [4/100], Step [1010/1751], Loss: 1.7906\n",
      "Epoch [4/100], Step [1020/1751], Loss: 1.8329\n",
      "Epoch [4/100], Step [1030/1751], Loss: 1.9185\n",
      "Epoch [4/100], Step [1040/1751], Loss: 1.8133\n",
      "Epoch [4/100], Step [1050/1751], Loss: 1.9828\n",
      "Epoch [4/100], Step [1060/1751], Loss: 1.8708\n",
      "Epoch [4/100], Step [1070/1751], Loss: 1.7688\n",
      "Epoch [4/100], Step [1080/1751], Loss: 1.8794\n",
      "Epoch [4/100], Step [1090/1751], Loss: 1.9006\n",
      "Epoch [4/100], Step [1100/1751], Loss: 2.1672\n",
      "Epoch [4/100], Step [1110/1751], Loss: 1.9802\n",
      "Epoch [4/100], Step [1120/1751], Loss: 1.7759\n",
      "Epoch [4/100], Step [1130/1751], Loss: 1.9138\n",
      "Epoch [4/100], Step [1140/1751], Loss: 1.8689\n",
      "Epoch [4/100], Step [1150/1751], Loss: 1.8852\n",
      "Epoch [4/100], Step [1160/1751], Loss: 1.8985\n",
      "Epoch [4/100], Step [1170/1751], Loss: 1.9408\n",
      "Epoch [4/100], Step [1180/1751], Loss: 1.9547\n",
      "Epoch [4/100], Step [1190/1751], Loss: 1.9398\n",
      "Epoch [4/100], Step [1200/1751], Loss: 1.9449\n",
      "Epoch [4/100], Step [1210/1751], Loss: 1.8087\n",
      "Epoch [4/100], Step [1220/1751], Loss: 1.9107\n",
      "Epoch [4/100], Step [1230/1751], Loss: 1.8963\n",
      "Epoch [4/100], Step [1240/1751], Loss: 1.7294\n",
      "Epoch [4/100], Step [1250/1751], Loss: 1.8782\n",
      "Epoch [4/100], Step [1260/1751], Loss: 1.8942\n",
      "Epoch [4/100], Step [1270/1751], Loss: 1.7850\n",
      "Epoch [4/100], Step [1280/1751], Loss: 1.9058\n",
      "Epoch [4/100], Step [1290/1751], Loss: 1.8756\n",
      "Epoch [4/100], Step [1300/1751], Loss: 1.8684\n",
      "Epoch [4/100], Step [1310/1751], Loss: 1.8976\n",
      "Epoch [4/100], Step [1320/1751], Loss: 1.8865\n",
      "Epoch [4/100], Step [1330/1751], Loss: 2.0032\n",
      "Epoch [4/100], Step [1340/1751], Loss: 1.8905\n",
      "Epoch [4/100], Step [1350/1751], Loss: 1.8235\n",
      "Epoch [4/100], Step [1360/1751], Loss: 1.7673\n",
      "Epoch [4/100], Step [1370/1751], Loss: 1.7941\n",
      "Epoch [4/100], Step [1380/1751], Loss: 1.9868\n",
      "Epoch [4/100], Step [1390/1751], Loss: 1.7626\n",
      "Epoch [4/100], Step [1400/1751], Loss: 1.8564\n",
      "Epoch [4/100], Step [1410/1751], Loss: 1.7731\n",
      "Epoch [4/100], Step [1420/1751], Loss: 1.9387\n",
      "Epoch [4/100], Step [1430/1751], Loss: 1.8888\n",
      "Epoch [4/100], Step [1440/1751], Loss: 1.9456\n",
      "Epoch [4/100], Step [1450/1751], Loss: 1.7392\n",
      "Epoch [4/100], Step [1460/1751], Loss: 1.9041\n",
      "Epoch [4/100], Step [1470/1751], Loss: 1.9098\n",
      "Epoch [4/100], Step [1480/1751], Loss: 1.9506\n",
      "Epoch [4/100], Step [1490/1751], Loss: 1.7457\n",
      "Epoch [4/100], Step [1500/1751], Loss: 1.8850\n",
      "Epoch [4/100], Step [1510/1751], Loss: 1.7969\n",
      "Epoch [4/100], Step [1520/1751], Loss: 1.8583\n",
      "Epoch [4/100], Step [1530/1751], Loss: 2.0414\n",
      "Epoch [4/100], Step [1540/1751], Loss: 1.8487\n",
      "Epoch [4/100], Step [1550/1751], Loss: 1.9687\n",
      "Epoch [4/100], Step [1560/1751], Loss: 1.9787\n",
      "Epoch [4/100], Step [1570/1751], Loss: 1.8436\n",
      "Epoch [4/100], Step [1580/1751], Loss: 1.8083\n",
      "Epoch [4/100], Step [1590/1751], Loss: 2.0440\n",
      "Epoch [4/100], Step [1600/1751], Loss: 1.7922\n",
      "Epoch [4/100], Step [1610/1751], Loss: 1.7905\n",
      "Epoch [4/100], Step [1620/1751], Loss: 1.8383\n",
      "Epoch [4/100], Step [1630/1751], Loss: 1.7671\n",
      "Epoch [4/100], Step [1640/1751], Loss: 1.8294\n",
      "Epoch [4/100], Step [1650/1751], Loss: 1.8174\n",
      "Epoch [4/100], Step [1660/1751], Loss: 1.8067\n",
      "Epoch [4/100], Step [1670/1751], Loss: 1.8824\n",
      "Epoch [4/100], Step [1680/1751], Loss: 1.8013\n",
      "Epoch [4/100], Step [1690/1751], Loss: 1.7637\n",
      "Epoch [4/100], Step [1700/1751], Loss: 1.9480\n",
      "Epoch [4/100], Step [1710/1751], Loss: 1.6895\n",
      "Epoch [4/100], Step [1720/1751], Loss: 1.8104\n",
      "Epoch [4/100], Step [1730/1751], Loss: 1.8292\n",
      "Epoch [4/100], Step [1740/1751], Loss: 1.7427\n",
      "Epoch [4/100], Step [1750/1751], Loss: 1.8296\n",
      "Epoch [4/100], Average Loss: 1.9415, Time: 1636.4757s\n",
      "Epoch [5/100], Step [10/1751], Loss: 1.7874\n",
      "Epoch [5/100], Step [20/1751], Loss: 1.8983\n",
      "Epoch [5/100], Step [30/1751], Loss: 1.7100\n",
      "Epoch [5/100], Step [40/1751], Loss: 1.7758\n",
      "Epoch [5/100], Step [50/1751], Loss: 1.8779\n",
      "Epoch [5/100], Step [60/1751], Loss: 1.8380\n",
      "Epoch [5/100], Step [70/1751], Loss: 1.9438\n",
      "Epoch [5/100], Step [80/1751], Loss: 1.8772\n",
      "Epoch [5/100], Step [90/1751], Loss: 1.9192\n",
      "Epoch [5/100], Step [100/1751], Loss: 1.8266\n",
      "Epoch [5/100], Step [110/1751], Loss: 1.9182\n",
      "Epoch [5/100], Step [120/1751], Loss: 1.6733\n",
      "Epoch [5/100], Step [130/1751], Loss: 1.8440\n",
      "Epoch [5/100], Step [140/1751], Loss: 1.9909\n",
      "Epoch [5/100], Step [150/1751], Loss: 1.8132\n",
      "Epoch [5/100], Step [160/1751], Loss: 1.6297\n",
      "Epoch [5/100], Step [170/1751], Loss: 1.6998\n",
      "Epoch [5/100], Step [180/1751], Loss: 1.8783\n",
      "Epoch [5/100], Step [190/1751], Loss: 1.7504\n",
      "Epoch [5/100], Step [200/1751], Loss: 1.7590\n",
      "Epoch [5/100], Step [210/1751], Loss: 1.7651\n",
      "Epoch [5/100], Step [220/1751], Loss: 1.6948\n",
      "Epoch [5/100], Step [230/1751], Loss: 1.9944\n",
      "Epoch [5/100], Step [240/1751], Loss: 1.9980\n",
      "Epoch [5/100], Step [250/1751], Loss: 1.7720\n",
      "Epoch [5/100], Step [260/1751], Loss: 1.6245\n",
      "Epoch [5/100], Step [270/1751], Loss: 1.8704\n",
      "Epoch [5/100], Step [280/1751], Loss: 1.7554\n",
      "Epoch [5/100], Step [290/1751], Loss: 1.8510\n",
      "Epoch [5/100], Step [300/1751], Loss: 1.9431\n",
      "Epoch [5/100], Step [310/1751], Loss: 1.7722\n",
      "Epoch [5/100], Step [320/1751], Loss: 1.7006\n",
      "Epoch [5/100], Step [330/1751], Loss: 1.7767\n",
      "Epoch [5/100], Step [340/1751], Loss: 1.8912\n",
      "Epoch [5/100], Step [350/1751], Loss: 1.7026\n",
      "Epoch [5/100], Step [360/1751], Loss: 1.7316\n",
      "Epoch [5/100], Step [370/1751], Loss: 1.8184\n",
      "Epoch [5/100], Step [380/1751], Loss: 1.5669\n",
      "Epoch [5/100], Step [390/1751], Loss: 1.7468\n",
      "Epoch [5/100], Step [400/1751], Loss: 1.8696\n",
      "Epoch [5/100], Step [410/1751], Loss: 1.8694\n",
      "Epoch [5/100], Step [420/1751], Loss: 1.7757\n",
      "Epoch [5/100], Step [430/1751], Loss: 1.6247\n",
      "Epoch [5/100], Step [440/1751], Loss: 1.8528\n",
      "Epoch [5/100], Step [450/1751], Loss: 1.7765\n",
      "Epoch [5/100], Step [460/1751], Loss: 1.9489\n",
      "Epoch [5/100], Step [470/1751], Loss: 1.8810\n",
      "Epoch [5/100], Step [480/1751], Loss: 1.8727\n",
      "Epoch [5/100], Step [490/1751], Loss: 1.9156\n",
      "Epoch [5/100], Step [500/1751], Loss: 1.8855\n",
      "Epoch [5/100], Step [510/1751], Loss: 1.8659\n",
      "Epoch [5/100], Step [520/1751], Loss: 1.6432\n",
      "Epoch [5/100], Step [530/1751], Loss: 1.7564\n",
      "Epoch [5/100], Step [540/1751], Loss: 1.9138\n",
      "Epoch [5/100], Step [550/1751], Loss: 1.8375\n",
      "Epoch [5/100], Step [560/1751], Loss: 1.7208\n",
      "Epoch [5/100], Step [570/1751], Loss: 1.8036\n",
      "Epoch [5/100], Step [580/1751], Loss: 1.7983\n",
      "Epoch [5/100], Step [590/1751], Loss: 1.8091\n",
      "Epoch [5/100], Step [600/1751], Loss: 1.8215\n",
      "Epoch [5/100], Step [610/1751], Loss: 1.9008\n",
      "Epoch [5/100], Step [620/1751], Loss: 1.7153\n",
      "Epoch [5/100], Step [630/1751], Loss: 1.8163\n",
      "Epoch [5/100], Step [640/1751], Loss: 1.7609\n",
      "Epoch [5/100], Step [650/1751], Loss: 1.6964\n",
      "Epoch [5/100], Step [660/1751], Loss: 1.7715\n",
      "Epoch [5/100], Step [670/1751], Loss: 1.8575\n",
      "Epoch [5/100], Step [680/1751], Loss: 1.9042\n",
      "Epoch [5/100], Step [690/1751], Loss: 1.8103\n",
      "Epoch [5/100], Step [700/1751], Loss: 1.6747\n",
      "Epoch [5/100], Step [710/1751], Loss: 1.7682\n",
      "Epoch [5/100], Step [720/1751], Loss: 1.8257\n",
      "Epoch [5/100], Step [730/1751], Loss: 1.8797\n",
      "Epoch [5/100], Step [740/1751], Loss: 1.7112\n",
      "Epoch [5/100], Step [750/1751], Loss: 1.7966\n",
      "Epoch [5/100], Step [760/1751], Loss: 1.7511\n",
      "Epoch [5/100], Step [770/1751], Loss: 1.6870\n",
      "Epoch [5/100], Step [780/1751], Loss: 1.8065\n",
      "Epoch [5/100], Step [790/1751], Loss: 1.5621\n",
      "Epoch [5/100], Step [800/1751], Loss: 1.6772\n",
      "Epoch [5/100], Step [810/1751], Loss: 1.7304\n",
      "Epoch [5/100], Step [820/1751], Loss: 1.6955\n",
      "Epoch [5/100], Step [830/1751], Loss: 1.7457\n",
      "Epoch [5/100], Step [840/1751], Loss: 1.6901\n",
      "Epoch [5/100], Step [850/1751], Loss: 1.8685\n",
      "Epoch [5/100], Step [860/1751], Loss: 1.8962\n",
      "Epoch [5/100], Step [870/1751], Loss: 1.7458\n",
      "Epoch [5/100], Step [880/1751], Loss: 1.7290\n",
      "Epoch [5/100], Step [890/1751], Loss: 1.7179\n",
      "Epoch [5/100], Step [900/1751], Loss: 1.6849\n",
      "Epoch [5/100], Step [910/1751], Loss: 1.6904\n",
      "Epoch [5/100], Step [920/1751], Loss: 1.7733\n",
      "Epoch [5/100], Step [930/1751], Loss: 1.7738\n",
      "Epoch [5/100], Step [940/1751], Loss: 1.7943\n",
      "Epoch [5/100], Step [950/1751], Loss: 1.6471\n",
      "Epoch [5/100], Step [960/1751], Loss: 1.8434\n",
      "Epoch [5/100], Step [970/1751], Loss: 1.6812\n",
      "Epoch [5/100], Step [980/1751], Loss: 1.7588\n",
      "Epoch [5/100], Step [990/1751], Loss: 1.6151\n",
      "Epoch [5/100], Step [1000/1751], Loss: 1.8790\n",
      "Epoch [5/100], Step [1010/1751], Loss: 1.7357\n",
      "Epoch [5/100], Step [1020/1751], Loss: 1.6517\n",
      "Epoch [5/100], Step [1030/1751], Loss: 1.6784\n",
      "Epoch [5/100], Step [1040/1751], Loss: 1.6555\n",
      "Epoch [5/100], Step [1050/1751], Loss: 1.8258\n",
      "Epoch [5/100], Step [1060/1751], Loss: 1.7572\n",
      "Epoch [5/100], Step [1070/1751], Loss: 1.8036\n",
      "Epoch [5/100], Step [1080/1751], Loss: 1.6534\n",
      "Epoch [5/100], Step [1090/1751], Loss: 1.7242\n",
      "Epoch [5/100], Step [1100/1751], Loss: 1.6651\n",
      "Epoch [5/100], Step [1110/1751], Loss: 1.7518\n",
      "Epoch [5/100], Step [1120/1751], Loss: 1.7560\n",
      "Epoch [5/100], Step [1130/1751], Loss: 1.5685\n",
      "Epoch [5/100], Step [1140/1751], Loss: 1.7111\n",
      "Epoch [5/100], Step [1150/1751], Loss: 1.7676\n",
      "Epoch [5/100], Step [1160/1751], Loss: 1.7054\n",
      "Epoch [5/100], Step [1170/1751], Loss: 1.6892\n",
      "Epoch [5/100], Step [1180/1751], Loss: 1.6886\n",
      "Epoch [5/100], Step [1190/1751], Loss: 1.8158\n",
      "Epoch [5/100], Step [1200/1751], Loss: 1.7202\n",
      "Epoch [5/100], Step [1210/1751], Loss: 1.6841\n",
      "Epoch [5/100], Step [1220/1751], Loss: 1.9484\n",
      "Epoch [5/100], Step [1230/1751], Loss: 1.7503\n",
      "Epoch [5/100], Step [1240/1751], Loss: 1.5315\n",
      "Epoch [5/100], Step [1250/1751], Loss: 1.5749\n",
      "Epoch [5/100], Step [1260/1751], Loss: 1.7596\n",
      "Epoch [5/100], Step [1270/1751], Loss: 1.8157\n",
      "Epoch [5/100], Step [1280/1751], Loss: 1.7734\n",
      "Epoch [5/100], Step [1290/1751], Loss: 1.5504\n",
      "Epoch [5/100], Step [1300/1751], Loss: 1.7484\n",
      "Epoch [5/100], Step [1310/1751], Loss: 1.7309\n",
      "Epoch [5/100], Step [1320/1751], Loss: 1.8051\n",
      "Epoch [5/100], Step [1330/1751], Loss: 1.6435\n",
      "Epoch [5/100], Step [1340/1751], Loss: 1.7934\n",
      "Epoch [5/100], Step [1350/1751], Loss: 1.7042\n",
      "Epoch [5/100], Step [1360/1751], Loss: 1.8620\n",
      "Epoch [5/100], Step [1370/1751], Loss: 1.6684\n",
      "Epoch [5/100], Step [1380/1751], Loss: 1.5586\n",
      "Epoch [5/100], Step [1390/1751], Loss: 1.7813\n",
      "Epoch [5/100], Step [1400/1751], Loss: 1.6129\n",
      "Epoch [5/100], Step [1410/1751], Loss: 1.9286\n",
      "Epoch [5/100], Step [1420/1751], Loss: 1.6730\n",
      "Epoch [5/100], Step [1430/1751], Loss: 1.6998\n",
      "Epoch [5/100], Step [1440/1751], Loss: 1.6267\n",
      "Epoch [5/100], Step [1450/1751], Loss: 1.6603\n",
      "Epoch [5/100], Step [1460/1751], Loss: 1.5115\n",
      "Epoch [5/100], Step [1470/1751], Loss: 1.8308\n",
      "Epoch [5/100], Step [1480/1751], Loss: 1.7467\n",
      "Epoch [5/100], Step [1490/1751], Loss: 1.7190\n",
      "Epoch [5/100], Step [1500/1751], Loss: 1.7616\n",
      "Epoch [5/100], Step [1510/1751], Loss: 1.5113\n",
      "Epoch [5/100], Step [1520/1751], Loss: 1.7782\n",
      "Epoch [5/100], Step [1530/1751], Loss: 1.6500\n",
      "Epoch [5/100], Step [1540/1751], Loss: 1.6246\n",
      "Epoch [5/100], Step [1550/1751], Loss: 1.5864\n",
      "Epoch [5/100], Step [1560/1751], Loss: 1.7195\n",
      "Epoch [5/100], Step [1570/1751], Loss: 1.5791\n",
      "Epoch [5/100], Step [1580/1751], Loss: 1.6481\n",
      "Epoch [5/100], Step [1590/1751], Loss: 1.7251\n",
      "Epoch [5/100], Step [1600/1751], Loss: 1.7642\n",
      "Epoch [5/100], Step [1610/1751], Loss: 1.6692\n",
      "Epoch [5/100], Step [1620/1751], Loss: 1.7397\n",
      "Epoch [5/100], Step [1630/1751], Loss: 1.8126\n",
      "Epoch [5/100], Step [1640/1751], Loss: 1.7795\n",
      "Epoch [5/100], Step [1650/1751], Loss: 1.6251\n",
      "Epoch [5/100], Step [1660/1751], Loss: 1.7355\n",
      "Epoch [5/100], Step [1670/1751], Loss: 1.6713\n",
      "Epoch [5/100], Step [1680/1751], Loss: 1.5625\n",
      "Epoch [5/100], Step [1690/1751], Loss: 1.7240\n",
      "Epoch [5/100], Step [1700/1751], Loss: 1.8373\n",
      "Epoch [5/100], Step [1710/1751], Loss: 1.6936\n",
      "Epoch [5/100], Step [1720/1751], Loss: 1.7290\n",
      "Epoch [5/100], Step [1730/1751], Loss: 1.7751\n",
      "Epoch [5/100], Step [1740/1751], Loss: 1.6930\n",
      "Epoch [5/100], Step [1750/1751], Loss: 1.6420\n",
      "Epoch [5/100], Average Loss: 1.7504, Time: 1637.1923s\n",
      "Epoch [6/100], Step [10/1751], Loss: 1.6490\n",
      "Epoch [6/100], Step [20/1751], Loss: 1.6830\n",
      "Epoch [6/100], Step [30/1751], Loss: 1.4696\n",
      "Epoch [6/100], Step [40/1751], Loss: 1.6606\n",
      "Epoch [6/100], Step [50/1751], Loss: 1.7041\n",
      "Epoch [6/100], Step [60/1751], Loss: 1.7490\n",
      "Epoch [6/100], Step [70/1751], Loss: 1.6494\n",
      "Epoch [6/100], Step [80/1751], Loss: 1.7839\n",
      "Epoch [6/100], Step [90/1751], Loss: 1.6756\n",
      "Epoch [6/100], Step [100/1751], Loss: 1.7307\n",
      "Epoch [6/100], Step [110/1751], Loss: 1.7976\n",
      "Epoch [6/100], Step [120/1751], Loss: 1.5976\n",
      "Epoch [6/100], Step [130/1751], Loss: 1.5750\n",
      "Epoch [6/100], Step [140/1751], Loss: 1.6323\n",
      "Epoch [6/100], Step [150/1751], Loss: 1.7177\n",
      "Epoch [6/100], Step [160/1751], Loss: 1.5390\n",
      "Epoch [6/100], Step [170/1751], Loss: 1.6299\n",
      "Epoch [6/100], Step [180/1751], Loss: 1.6801\n",
      "Epoch [6/100], Step [190/1751], Loss: 1.6035\n",
      "Epoch [6/100], Step [200/1751], Loss: 1.7017\n",
      "Epoch [6/100], Step [210/1751], Loss: 1.8005\n",
      "Epoch [6/100], Step [220/1751], Loss: 1.6349\n",
      "Epoch [6/100], Step [230/1751], Loss: 1.5236\n",
      "Epoch [6/100], Step [240/1751], Loss: 1.5698\n",
      "Epoch [6/100], Step [250/1751], Loss: 1.6881\n",
      "Epoch [6/100], Step [260/1751], Loss: 1.8925\n",
      "Epoch [6/100], Step [270/1751], Loss: 1.5369\n",
      "Epoch [6/100], Step [280/1751], Loss: 1.6921\n",
      "Epoch [6/100], Step [290/1751], Loss: 1.6311\n",
      "Epoch [6/100], Step [300/1751], Loss: 1.7292\n",
      "Epoch [6/100], Step [310/1751], Loss: 1.6494\n",
      "Epoch [6/100], Step [320/1751], Loss: 1.6314\n",
      "Epoch [6/100], Step [330/1751], Loss: 1.5846\n",
      "Epoch [6/100], Step [340/1751], Loss: 1.3863\n",
      "Epoch [6/100], Step [350/1751], Loss: 1.6452\n",
      "Epoch [6/100], Step [360/1751], Loss: 1.6368\n",
      "Epoch [6/100], Step [370/1751], Loss: 1.6802\n",
      "Epoch [6/100], Step [380/1751], Loss: 1.6361\n",
      "Epoch [6/100], Step [390/1751], Loss: 1.6338\n",
      "Epoch [6/100], Step [400/1751], Loss: 1.7780\n",
      "Epoch [6/100], Step [410/1751], Loss: 1.7212\n",
      "Epoch [6/100], Step [420/1751], Loss: 1.6327\n",
      "Epoch [6/100], Step [430/1751], Loss: 1.6689\n",
      "Epoch [6/100], Step [440/1751], Loss: 1.5793\n",
      "Epoch [6/100], Step [450/1751], Loss: 1.5678\n",
      "Epoch [6/100], Step [460/1751], Loss: 1.7163\n",
      "Epoch [6/100], Step [470/1751], Loss: 1.5021\n",
      "Epoch [6/100], Step [480/1751], Loss: 1.8262\n",
      "Epoch [6/100], Step [490/1751], Loss: 1.7450\n",
      "Epoch [6/100], Step [500/1751], Loss: 1.5966\n",
      "Epoch [6/100], Step [510/1751], Loss: 1.8907\n",
      "Epoch [6/100], Step [520/1751], Loss: 1.7453\n",
      "Epoch [6/100], Step [530/1751], Loss: 1.6242\n",
      "Epoch [6/100], Step [540/1751], Loss: 1.5744\n",
      "Epoch [6/100], Step [550/1751], Loss: 1.6761\n",
      "Epoch [6/100], Step [560/1751], Loss: 1.7052\n",
      "Epoch [6/100], Step [570/1751], Loss: 1.5205\n",
      "Epoch [6/100], Step [580/1751], Loss: 1.6928\n",
      "Epoch [6/100], Step [590/1751], Loss: 1.8115\n",
      "Epoch [6/100], Step [600/1751], Loss: 1.6065\n",
      "Epoch [6/100], Step [610/1751], Loss: 1.7879\n",
      "Epoch [6/100], Step [620/1751], Loss: 1.7044\n",
      "Epoch [6/100], Step [630/1751], Loss: 1.6964\n",
      "Epoch [6/100], Step [640/1751], Loss: 1.6803\n",
      "Epoch [6/100], Step [650/1751], Loss: 1.4811\n",
      "Epoch [6/100], Step [660/1751], Loss: 1.6101\n",
      "Epoch [6/100], Step [670/1751], Loss: 1.7299\n",
      "Epoch [6/100], Step [680/1751], Loss: 1.7567\n",
      "Epoch [6/100], Step [690/1751], Loss: 1.7424\n",
      "Epoch [6/100], Step [700/1751], Loss: 1.6167\n",
      "Epoch [6/100], Step [710/1751], Loss: 1.6977\n",
      "Epoch [6/100], Step [720/1751], Loss: 1.5372\n",
      "Epoch [6/100], Step [730/1751], Loss: 1.6679\n",
      "Epoch [6/100], Step [740/1751], Loss: 1.6381\n",
      "Epoch [6/100], Step [750/1751], Loss: 1.5190\n",
      "Epoch [6/100], Step [760/1751], Loss: 1.7457\n",
      "Epoch [6/100], Step [770/1751], Loss: 1.6482\n",
      "Epoch [6/100], Step [780/1751], Loss: 1.7517\n",
      "Epoch [6/100], Step [790/1751], Loss: 1.6316\n",
      "Epoch [6/100], Step [800/1751], Loss: 1.6470\n",
      "Epoch [6/100], Step [810/1751], Loss: 1.6842\n",
      "Epoch [6/100], Step [820/1751], Loss: 1.5397\n",
      "Epoch [6/100], Step [830/1751], Loss: 1.4605\n",
      "Epoch [6/100], Step [840/1751], Loss: 1.5866\n",
      "Epoch [6/100], Step [850/1751], Loss: 1.4398\n",
      "Epoch [6/100], Step [860/1751], Loss: 1.6097\n",
      "Epoch [6/100], Step [870/1751], Loss: 1.6362\n",
      "Epoch [6/100], Step [880/1751], Loss: 1.6847\n",
      "Epoch [6/100], Step [890/1751], Loss: 1.5449\n",
      "Epoch [6/100], Step [900/1751], Loss: 1.5050\n",
      "Epoch [6/100], Step [910/1751], Loss: 1.7644\n",
      "Epoch [6/100], Step [920/1751], Loss: 1.6377\n",
      "Epoch [6/100], Step [930/1751], Loss: 1.6797\n",
      "Epoch [6/100], Step [940/1751], Loss: 1.7219\n",
      "Epoch [6/100], Step [950/1751], Loss: 1.6208\n",
      "Epoch [6/100], Step [960/1751], Loss: 1.6289\n",
      "Epoch [6/100], Step [970/1751], Loss: 1.7527\n",
      "Epoch [6/100], Step [980/1751], Loss: 1.5591\n",
      "Epoch [6/100], Step [990/1751], Loss: 1.6101\n",
      "Epoch [6/100], Step [1000/1751], Loss: 1.5988\n",
      "Epoch [6/100], Step [1010/1751], Loss: 1.7506\n",
      "Epoch [6/100], Step [1020/1751], Loss: 1.5191\n",
      "Epoch [6/100], Step [1030/1751], Loss: 1.6194\n",
      "Epoch [6/100], Step [1040/1751], Loss: 1.7860\n",
      "Epoch [6/100], Step [1050/1751], Loss: 1.6275\n",
      "Epoch [6/100], Step [1060/1751], Loss: 1.6839\n",
      "Epoch [6/100], Step [1070/1751], Loss: 1.7622\n",
      "Epoch [6/100], Step [1080/1751], Loss: 1.5815\n",
      "Epoch [6/100], Step [1090/1751], Loss: 1.5851\n",
      "Epoch [6/100], Step [1100/1751], Loss: 1.6457\n",
      "Epoch [6/100], Step [1110/1751], Loss: 1.6420\n",
      "Epoch [6/100], Step [1120/1751], Loss: 1.4889\n",
      "Epoch [6/100], Step [1130/1751], Loss: 1.7623\n",
      "Epoch [6/100], Step [1140/1751], Loss: 1.6884\n",
      "Epoch [6/100], Step [1150/1751], Loss: 1.5079\n",
      "Epoch [6/100], Step [1160/1751], Loss: 1.5141\n",
      "Epoch [6/100], Step [1170/1751], Loss: 1.6920\n",
      "Epoch [6/100], Step [1180/1751], Loss: 1.7456\n",
      "Epoch [6/100], Step [1190/1751], Loss: 1.4216\n",
      "Epoch [6/100], Step [1200/1751], Loss: 1.5466\n",
      "Epoch [6/100], Step [1210/1751], Loss: 1.6285\n",
      "Epoch [6/100], Step [1220/1751], Loss: 1.5103\n",
      "Epoch [6/100], Step [1230/1751], Loss: 1.5944\n",
      "Epoch [6/100], Step [1240/1751], Loss: 1.5142\n",
      "Epoch [6/100], Step [1250/1751], Loss: 1.5742\n",
      "Epoch [6/100], Step [1260/1751], Loss: 1.5583\n",
      "Epoch [6/100], Step [1270/1751], Loss: 1.5596\n",
      "Epoch [6/100], Step [1280/1751], Loss: 1.5980\n",
      "Epoch [6/100], Step [1290/1751], Loss: 1.5639\n",
      "Epoch [6/100], Step [1300/1751], Loss: 1.5815\n",
      "Epoch [6/100], Step [1310/1751], Loss: 1.6708\n",
      "Epoch [6/100], Step [1320/1751], Loss: 1.6356\n",
      "Epoch [6/100], Step [1330/1751], Loss: 1.7184\n",
      "Epoch [6/100], Step [1340/1751], Loss: 1.4268\n",
      "Epoch [6/100], Step [1350/1751], Loss: 1.5432\n",
      "Epoch [6/100], Step [1360/1751], Loss: 1.5642\n",
      "Epoch [6/100], Step [1370/1751], Loss: 1.7372\n",
      "Epoch [6/100], Step [1380/1751], Loss: 1.5593\n",
      "Epoch [6/100], Step [1390/1751], Loss: 1.6092\n",
      "Epoch [6/100], Step [1400/1751], Loss: 1.6597\n",
      "Epoch [6/100], Step [1410/1751], Loss: 1.4760\n",
      "Epoch [6/100], Step [1420/1751], Loss: 1.4379\n",
      "Epoch [6/100], Step [1430/1751], Loss: 1.5513\n",
      "Epoch [6/100], Step [1440/1751], Loss: 1.5821\n",
      "Epoch [6/100], Step [1450/1751], Loss: 1.6092\n",
      "Epoch [6/100], Step [1460/1751], Loss: 1.6631\n",
      "Epoch [6/100], Step [1470/1751], Loss: 1.6465\n",
      "Epoch [6/100], Step [1480/1751], Loss: 1.4764\n",
      "Epoch [6/100], Step [1490/1751], Loss: 1.6062\n",
      "Epoch [6/100], Step [1500/1751], Loss: 1.6458\n",
      "Epoch [6/100], Step [1510/1751], Loss: 1.6488\n",
      "Epoch [6/100], Step [1520/1751], Loss: 1.6085\n",
      "Epoch [6/100], Step [1530/1751], Loss: 1.4731\n",
      "Epoch [6/100], Step [1540/1751], Loss: 1.5780\n",
      "Epoch [6/100], Step [1550/1751], Loss: 1.6217\n",
      "Epoch [6/100], Step [1560/1751], Loss: 1.4967\n",
      "Epoch [6/100], Step [1570/1751], Loss: 1.6600\n",
      "Epoch [6/100], Step [1580/1751], Loss: 1.6567\n",
      "Epoch [6/100], Step [1590/1751], Loss: 1.7245\n",
      "Epoch [6/100], Step [1600/1751], Loss: 1.8048\n",
      "Epoch [6/100], Step [1610/1751], Loss: 1.7176\n",
      "Epoch [6/100], Step [1620/1751], Loss: 1.5123\n",
      "Epoch [6/100], Step [1630/1751], Loss: 1.3737\n",
      "Epoch [6/100], Step [1640/1751], Loss: 1.6005\n",
      "Epoch [6/100], Step [1650/1751], Loss: 1.5773\n",
      "Epoch [6/100], Step [1660/1751], Loss: 1.6918\n",
      "Epoch [6/100], Step [1670/1751], Loss: 1.5917\n",
      "Epoch [6/100], Step [1680/1751], Loss: 1.5709\n",
      "Epoch [6/100], Step [1690/1751], Loss: 1.6658\n",
      "Epoch [6/100], Step [1700/1751], Loss: 1.5579\n",
      "Epoch [6/100], Step [1710/1751], Loss: 1.5422\n",
      "Epoch [6/100], Step [1720/1751], Loss: 1.3707\n",
      "Epoch [6/100], Step [1730/1751], Loss: 1.6006\n",
      "Epoch [6/100], Step [1740/1751], Loss: 1.6155\n",
      "Epoch [6/100], Step [1750/1751], Loss: 1.6576\n",
      "Epoch [6/100], Average Loss: 1.6299, Time: 1637.6004s\n",
      "Epoch [7/100], Step [10/1751], Loss: 1.5619\n",
      "Epoch [7/100], Step [20/1751], Loss: 1.3254\n",
      "Epoch [7/100], Step [30/1751], Loss: 1.6943\n",
      "Epoch [7/100], Step [40/1751], Loss: 1.5242\n",
      "Epoch [7/100], Step [50/1751], Loss: 1.5685\n",
      "Epoch [7/100], Step [60/1751], Loss: 1.5943\n",
      "Epoch [7/100], Step [70/1751], Loss: 1.6714\n",
      "Epoch [7/100], Step [80/1751], Loss: 1.4507\n",
      "Epoch [7/100], Step [90/1751], Loss: 1.6907\n",
      "Epoch [7/100], Step [100/1751], Loss: 1.5235\n",
      "Epoch [7/100], Step [110/1751], Loss: 1.6212\n",
      "Epoch [7/100], Step [120/1751], Loss: 1.5573\n",
      "Epoch [7/100], Step [130/1751], Loss: 1.5976\n",
      "Epoch [7/100], Step [140/1751], Loss: 1.6015\n",
      "Epoch [7/100], Step [150/1751], Loss: 1.5818\n",
      "Epoch [7/100], Step [160/1751], Loss: 1.6405\n",
      "Epoch [7/100], Step [170/1751], Loss: 1.5383\n",
      "Epoch [7/100], Step [180/1751], Loss: 1.5571\n",
      "Epoch [7/100], Step [190/1751], Loss: 1.6002\n",
      "Epoch [7/100], Step [200/1751], Loss: 1.4367\n",
      "Epoch [7/100], Step [210/1751], Loss: 1.6279\n",
      "Epoch [7/100], Step [220/1751], Loss: 1.4785\n",
      "Epoch [7/100], Step [230/1751], Loss: 1.4941\n",
      "Epoch [7/100], Step [240/1751], Loss: 1.5090\n",
      "Epoch [7/100], Step [250/1751], Loss: 1.5700\n",
      "Epoch [7/100], Step [260/1751], Loss: 1.5380\n",
      "Epoch [7/100], Step [270/1751], Loss: 1.4860\n",
      "Epoch [7/100], Step [280/1751], Loss: 1.4047\n",
      "Epoch [7/100], Step [290/1751], Loss: 1.6417\n",
      "Epoch [7/100], Step [300/1751], Loss: 1.6012\n",
      "Epoch [7/100], Step [310/1751], Loss: 1.6296\n",
      "Epoch [7/100], Step [320/1751], Loss: 1.5783\n",
      "Epoch [7/100], Step [330/1751], Loss: 1.4550\n",
      "Epoch [7/100], Step [340/1751], Loss: 1.6528\n",
      "Epoch [7/100], Step [350/1751], Loss: 1.5326\n",
      "Epoch [7/100], Step [360/1751], Loss: 1.5291\n",
      "Epoch [7/100], Step [370/1751], Loss: 1.5440\n",
      "Epoch [7/100], Step [380/1751], Loss: 1.5930\n",
      "Epoch [7/100], Step [390/1751], Loss: 1.6957\n",
      "Epoch [7/100], Step [400/1751], Loss: 1.8073\n",
      "Epoch [7/100], Step [410/1751], Loss: 1.5252\n",
      "Epoch [7/100], Step [420/1751], Loss: 1.6066\n",
      "Epoch [7/100], Step [430/1751], Loss: 1.5868\n",
      "Epoch [7/100], Step [440/1751], Loss: 1.6020\n",
      "Epoch [7/100], Step [450/1751], Loss: 1.5270\n",
      "Epoch [7/100], Step [460/1751], Loss: 1.8297\n",
      "Epoch [7/100], Step [470/1751], Loss: 1.6062\n",
      "Epoch [7/100], Step [480/1751], Loss: 1.6160\n",
      "Epoch [7/100], Step [490/1751], Loss: 1.6650\n",
      "Epoch [7/100], Step [500/1751], Loss: 1.5541\n",
      "Epoch [7/100], Step [510/1751], Loss: 1.5790\n",
      "Epoch [7/100], Step [520/1751], Loss: 1.7277\n",
      "Epoch [7/100], Step [530/1751], Loss: 1.7041\n",
      "Epoch [7/100], Step [540/1751], Loss: 1.5464\n",
      "Epoch [7/100], Step [550/1751], Loss: 1.5526\n",
      "Epoch [7/100], Step [560/1751], Loss: 1.6302\n",
      "Epoch [7/100], Step [570/1751], Loss: 1.3824\n",
      "Epoch [7/100], Step [580/1751], Loss: 1.5667\n",
      "Epoch [7/100], Step [590/1751], Loss: 1.6840\n",
      "Epoch [7/100], Step [600/1751], Loss: 1.3514\n",
      "Epoch [7/100], Step [610/1751], Loss: 1.4660\n",
      "Epoch [7/100], Step [620/1751], Loss: 1.4827\n",
      "Epoch [7/100], Step [630/1751], Loss: 1.4764\n",
      "Epoch [7/100], Step [640/1751], Loss: 1.5810\n",
      "Epoch [7/100], Step [650/1751], Loss: 1.4022\n",
      "Epoch [7/100], Step [660/1751], Loss: 1.5331\n",
      "Epoch [7/100], Step [670/1751], Loss: 1.5503\n",
      "Epoch [7/100], Step [680/1751], Loss: 1.8251\n",
      "Epoch [7/100], Step [690/1751], Loss: 1.5809\n",
      "Epoch [7/100], Step [700/1751], Loss: 1.4301\n",
      "Epoch [7/100], Step [710/1751], Loss: 1.6641\n",
      "Epoch [7/100], Step [720/1751], Loss: 1.5565\n",
      "Epoch [7/100], Step [730/1751], Loss: 1.5990\n",
      "Epoch [7/100], Step [740/1751], Loss: 1.6221\n",
      "Epoch [7/100], Step [750/1751], Loss: 1.4249\n",
      "Epoch [7/100], Step [760/1751], Loss: 1.4330\n",
      "Epoch [7/100], Step [770/1751], Loss: 1.6460\n",
      "Epoch [7/100], Step [780/1751], Loss: 1.5505\n",
      "Epoch [7/100], Step [790/1751], Loss: 1.7418\n",
      "Epoch [7/100], Step [800/1751], Loss: 1.5797\n",
      "Epoch [7/100], Step [810/1751], Loss: 1.5980\n",
      "Epoch [7/100], Step [820/1751], Loss: 1.5983\n",
      "Epoch [7/100], Step [830/1751], Loss: 1.3380\n",
      "Epoch [7/100], Step [840/1751], Loss: 1.5075\n",
      "Epoch [7/100], Step [850/1751], Loss: 1.5510\n",
      "Epoch [7/100], Step [860/1751], Loss: 1.5753\n",
      "Epoch [7/100], Step [870/1751], Loss: 1.4896\n",
      "Epoch [7/100], Step [880/1751], Loss: 1.4829\n",
      "Epoch [7/100], Step [890/1751], Loss: 1.4904\n",
      "Epoch [7/100], Step [900/1751], Loss: 1.6258\n",
      "Epoch [7/100], Step [910/1751], Loss: 1.5028\n",
      "Epoch [7/100], Step [920/1751], Loss: 1.5568\n",
      "Epoch [7/100], Step [930/1751], Loss: 1.6306\n",
      "Epoch [7/100], Step [940/1751], Loss: 1.5792\n",
      "Epoch [7/100], Step [950/1751], Loss: 1.5460\n",
      "Epoch [7/100], Step [960/1751], Loss: 1.4418\n",
      "Epoch [7/100], Step [970/1751], Loss: 1.5087\n",
      "Epoch [7/100], Step [980/1751], Loss: 1.5182\n",
      "Epoch [7/100], Step [990/1751], Loss: 1.7491\n",
      "Epoch [7/100], Step [1000/1751], Loss: 1.5701\n",
      "Epoch [7/100], Step [1010/1751], Loss: 1.5891\n",
      "Epoch [7/100], Step [1020/1751], Loss: 1.4514\n",
      "Epoch [7/100], Step [1030/1751], Loss: 1.5210\n",
      "Epoch [7/100], Step [1040/1751], Loss: 1.6407\n",
      "Epoch [7/100], Step [1050/1751], Loss: 1.4610\n",
      "Epoch [7/100], Step [1060/1751], Loss: 1.5249\n",
      "Epoch [7/100], Step [1070/1751], Loss: 1.4212\n",
      "Epoch [7/100], Step [1080/1751], Loss: 1.4560\n",
      "Epoch [7/100], Step [1090/1751], Loss: 1.6026\n",
      "Epoch [7/100], Step [1100/1751], Loss: 1.4760\n",
      "Epoch [7/100], Step [1110/1751], Loss: 1.3910\n",
      "Epoch [7/100], Step [1120/1751], Loss: 1.4819\n",
      "Epoch [7/100], Step [1130/1751], Loss: 1.5276\n",
      "Epoch [7/100], Step [1140/1751], Loss: 1.5493\n",
      "Epoch [7/100], Step [1150/1751], Loss: 1.6247\n",
      "Epoch [7/100], Step [1160/1751], Loss: 1.6069\n",
      "Epoch [7/100], Step [1170/1751], Loss: 1.7995\n",
      "Epoch [7/100], Step [1180/1751], Loss: 1.5766\n",
      "Epoch [7/100], Step [1190/1751], Loss: 1.5630\n",
      "Epoch [7/100], Step [1200/1751], Loss: 1.5217\n",
      "Epoch [7/100], Step [1210/1751], Loss: 1.5204\n",
      "Epoch [7/100], Step [1220/1751], Loss: 1.6160\n",
      "Epoch [7/100], Step [1230/1751], Loss: 1.4070\n",
      "Epoch [7/100], Step [1240/1751], Loss: 1.4488\n",
      "Epoch [7/100], Step [1250/1751], Loss: 1.5360\n",
      "Epoch [7/100], Step [1260/1751], Loss: 1.5287\n",
      "Epoch [7/100], Step [1270/1751], Loss: 1.5646\n",
      "Epoch [7/100], Step [1280/1751], Loss: 1.5703\n",
      "Epoch [7/100], Step [1290/1751], Loss: 1.4574\n",
      "Epoch [7/100], Step [1300/1751], Loss: 1.6480\n",
      "Epoch [7/100], Step [1310/1751], Loss: 1.4739\n",
      "Epoch [7/100], Step [1320/1751], Loss: 1.5147\n",
      "Epoch [7/100], Step [1330/1751], Loss: 1.5029\n",
      "Epoch [7/100], Step [1340/1751], Loss: 1.5521\n",
      "Epoch [7/100], Step [1350/1751], Loss: 1.5006\n",
      "Epoch [7/100], Step [1360/1751], Loss: 1.5692\n",
      "Epoch [7/100], Step [1370/1751], Loss: 1.6674\n",
      "Epoch [7/100], Step [1380/1751], Loss: 1.5714\n",
      "Epoch [7/100], Step [1390/1751], Loss: 1.5181\n",
      "Epoch [7/100], Step [1400/1751], Loss: 1.4201\n",
      "Epoch [7/100], Step [1410/1751], Loss: 1.3750\n",
      "Epoch [7/100], Step [1420/1751], Loss: 1.5056\n",
      "Epoch [7/100], Step [1430/1751], Loss: 1.6463\n",
      "Epoch [7/100], Step [1440/1751], Loss: 1.6641\n",
      "Epoch [7/100], Step [1450/1751], Loss: 1.4255\n",
      "Epoch [7/100], Step [1460/1751], Loss: 1.3635\n",
      "Epoch [7/100], Step [1470/1751], Loss: 1.4486\n",
      "Epoch [7/100], Step [1480/1751], Loss: 1.4929\n",
      "Epoch [7/100], Step [1490/1751], Loss: 1.6614\n",
      "Epoch [7/100], Step [1500/1751], Loss: 1.4320\n",
      "Epoch [7/100], Step [1510/1751], Loss: 1.5287\n",
      "Epoch [7/100], Step [1520/1751], Loss: 1.4265\n",
      "Epoch [7/100], Step [1530/1751], Loss: 1.5548\n",
      "Epoch [7/100], Step [1540/1751], Loss: 1.4585\n",
      "Epoch [7/100], Step [1550/1751], Loss: 1.6185\n",
      "Epoch [7/100], Step [1560/1751], Loss: 1.5454\n",
      "Epoch [7/100], Step [1570/1751], Loss: 1.7909\n",
      "Epoch [7/100], Step [1580/1751], Loss: 1.5197\n",
      "Epoch [7/100], Step [1590/1751], Loss: 1.4942\n",
      "Epoch [7/100], Step [1600/1751], Loss: 1.5919\n",
      "Epoch [7/100], Step [1610/1751], Loss: 1.3959\n",
      "Epoch [7/100], Step [1620/1751], Loss: 1.5874\n",
      "Epoch [7/100], Step [1630/1751], Loss: 1.4506\n",
      "Epoch [7/100], Step [1640/1751], Loss: 1.4907\n",
      "Epoch [7/100], Step [1650/1751], Loss: 1.3650\n",
      "Epoch [7/100], Step [1660/1751], Loss: 1.6095\n",
      "Epoch [7/100], Step [1670/1751], Loss: 1.5179\n",
      "Epoch [7/100], Step [1680/1751], Loss: 1.5081\n",
      "Epoch [7/100], Step [1690/1751], Loss: 1.5872\n",
      "Epoch [7/100], Step [1700/1751], Loss: 1.4423\n",
      "Epoch [7/100], Step [1710/1751], Loss: 1.3715\n",
      "Epoch [7/100], Step [1720/1751], Loss: 1.6445\n",
      "Epoch [7/100], Step [1730/1751], Loss: 1.5127\n",
      "Epoch [7/100], Step [1740/1751], Loss: 1.3654\n",
      "Epoch [7/100], Step [1750/1751], Loss: 1.6115\n",
      "Epoch [7/100], Average Loss: 1.5475, Time: 1637.8162s\n",
      "Epoch [8/100], Step [10/1751], Loss: 1.4457\n",
      "Epoch [8/100], Step [20/1751], Loss: 1.5987\n",
      "Epoch [8/100], Step [30/1751], Loss: 1.5489\n",
      "Epoch [8/100], Step [40/1751], Loss: 1.5464\n",
      "Epoch [8/100], Step [50/1751], Loss: 1.5458\n",
      "Epoch [8/100], Step [60/1751], Loss: 1.4000\n",
      "Epoch [8/100], Step [70/1751], Loss: 1.5137\n",
      "Epoch [8/100], Step [80/1751], Loss: 1.4532\n",
      "Epoch [8/100], Step [90/1751], Loss: 1.4075\n",
      "Epoch [8/100], Step [100/1751], Loss: 1.5635\n",
      "Epoch [8/100], Step [110/1751], Loss: 1.4041\n",
      "Epoch [8/100], Step [120/1751], Loss: 1.6918\n",
      "Epoch [8/100], Step [130/1751], Loss: 1.4879\n",
      "Epoch [8/100], Step [140/1751], Loss: 1.4236\n",
      "Epoch [8/100], Step [150/1751], Loss: 1.3099\n",
      "Epoch [8/100], Step [160/1751], Loss: 1.5250\n",
      "Epoch [8/100], Step [170/1751], Loss: 1.5004\n",
      "Epoch [8/100], Step [180/1751], Loss: 1.4208\n",
      "Epoch [8/100], Step [190/1751], Loss: 1.4235\n",
      "Epoch [8/100], Step [200/1751], Loss: 1.3378\n",
      "Epoch [8/100], Step [210/1751], Loss: 1.4676\n",
      "Epoch [8/100], Step [220/1751], Loss: 1.4517\n",
      "Epoch [8/100], Step [230/1751], Loss: 1.4744\n",
      "Epoch [8/100], Step [240/1751], Loss: 1.6291\n",
      "Epoch [8/100], Step [250/1751], Loss: 1.5476\n",
      "Epoch [8/100], Step [260/1751], Loss: 1.3037\n",
      "Epoch [8/100], Step [270/1751], Loss: 1.4607\n",
      "Epoch [8/100], Step [280/1751], Loss: 1.4765\n",
      "Epoch [8/100], Step [290/1751], Loss: 1.4873\n",
      "Epoch [8/100], Step [300/1751], Loss: 1.5723\n",
      "Epoch [8/100], Step [310/1751], Loss: 1.4642\n",
      "Epoch [8/100], Step [320/1751], Loss: 1.5716\n",
      "Epoch [8/100], Step [330/1751], Loss: 1.5827\n",
      "Epoch [8/100], Step [340/1751], Loss: 1.5113\n",
      "Epoch [8/100], Step [350/1751], Loss: 1.4713\n",
      "Epoch [8/100], Step [360/1751], Loss: 1.5187\n",
      "Epoch [8/100], Step [370/1751], Loss: 1.4147\n",
      "Epoch [8/100], Step [380/1751], Loss: 1.5400\n",
      "Epoch [8/100], Step [390/1751], Loss: 1.6342\n",
      "Epoch [8/100], Step [400/1751], Loss: 1.7067\n",
      "Epoch [8/100], Step [410/1751], Loss: 1.3670\n",
      "Epoch [8/100], Step [420/1751], Loss: 1.5519\n",
      "Epoch [8/100], Step [430/1751], Loss: 1.6747\n",
      "Epoch [8/100], Step [440/1751], Loss: 1.5734\n",
      "Epoch [8/100], Step [450/1751], Loss: 1.3600\n",
      "Epoch [8/100], Step [460/1751], Loss: 1.4124\n",
      "Epoch [8/100], Step [470/1751], Loss: 1.6406\n",
      "Epoch [8/100], Step [480/1751], Loss: 1.5091\n",
      "Epoch [8/100], Step [490/1751], Loss: 1.4149\n",
      "Epoch [8/100], Step [500/1751], Loss: 1.5366\n",
      "Epoch [8/100], Step [510/1751], Loss: 1.3763\n",
      "Epoch [8/100], Step [520/1751], Loss: 1.6288\n",
      "Epoch [8/100], Step [530/1751], Loss: 1.4335\n",
      "Epoch [8/100], Step [540/1751], Loss: 1.5921\n",
      "Epoch [8/100], Step [550/1751], Loss: 1.5654\n",
      "Epoch [8/100], Step [560/1751], Loss: 1.4072\n",
      "Epoch [8/100], Step [570/1751], Loss: 1.4813\n",
      "Epoch [8/100], Step [580/1751], Loss: 1.3784\n",
      "Epoch [8/100], Step [590/1751], Loss: 1.6209\n",
      "Epoch [8/100], Step [600/1751], Loss: 1.4864\n",
      "Epoch [8/100], Step [610/1751], Loss: 1.5880\n",
      "Epoch [8/100], Step [620/1751], Loss: 1.4850\n",
      "Epoch [8/100], Step [630/1751], Loss: 1.4579\n",
      "Epoch [8/100], Step [640/1751], Loss: 1.5377\n",
      "Epoch [8/100], Step [650/1751], Loss: 1.4331\n",
      "Epoch [8/100], Step [660/1751], Loss: 1.6824\n",
      "Epoch [8/100], Step [670/1751], Loss: 1.5821\n",
      "Epoch [8/100], Step [680/1751], Loss: 1.4846\n",
      "Epoch [8/100], Step [690/1751], Loss: 1.4889\n",
      "Epoch [8/100], Step [700/1751], Loss: 1.5881\n",
      "Epoch [8/100], Step [710/1751], Loss: 1.5093\n",
      "Epoch [8/100], Step [720/1751], Loss: 1.5355\n",
      "Epoch [8/100], Step [730/1751], Loss: 1.3521\n",
      "Epoch [8/100], Step [740/1751], Loss: 1.3910\n",
      "Epoch [8/100], Step [750/1751], Loss: 1.5443\n",
      "Epoch [8/100], Step [760/1751], Loss: 1.5551\n",
      "Epoch [8/100], Step [770/1751], Loss: 1.5339\n",
      "Epoch [8/100], Step [780/1751], Loss: 1.3930\n",
      "Epoch [8/100], Step [790/1751], Loss: 1.5099\n",
      "Epoch [8/100], Step [800/1751], Loss: 1.5388\n",
      "Epoch [8/100], Step [810/1751], Loss: 1.4885\n",
      "Epoch [8/100], Step [820/1751], Loss: 1.5048\n",
      "Epoch [8/100], Step [830/1751], Loss: 1.4336\n",
      "Epoch [8/100], Step [840/1751], Loss: 1.4791\n",
      "Epoch [8/100], Step [850/1751], Loss: 1.3848\n",
      "Epoch [8/100], Step [860/1751], Loss: 1.4361\n",
      "Epoch [8/100], Step [870/1751], Loss: 1.5348\n",
      "Epoch [8/100], Step [880/1751], Loss: 1.5454\n",
      "Epoch [8/100], Step [890/1751], Loss: 1.4567\n",
      "Epoch [8/100], Step [900/1751], Loss: 1.5931\n",
      "Epoch [8/100], Step [910/1751], Loss: 1.4828\n",
      "Epoch [8/100], Step [920/1751], Loss: 1.5314\n",
      "Epoch [8/100], Step [930/1751], Loss: 1.4537\n",
      "Epoch [8/100], Step [940/1751], Loss: 1.4883\n",
      "Epoch [8/100], Step [950/1751], Loss: 1.4622\n",
      "Epoch [8/100], Step [960/1751], Loss: 1.4880\n",
      "Epoch [8/100], Step [970/1751], Loss: 1.4116\n",
      "Epoch [8/100], Step [980/1751], Loss: 1.4511\n",
      "Epoch [8/100], Step [990/1751], Loss: 1.4793\n",
      "Epoch [8/100], Step [1000/1751], Loss: 1.5747\n",
      "Epoch [8/100], Step [1010/1751], Loss: 1.3565\n",
      "Epoch [8/100], Step [1020/1751], Loss: 1.4548\n",
      "Epoch [8/100], Step [1030/1751], Loss: 1.3844\n",
      "Epoch [8/100], Step [1040/1751], Loss: 1.5475\n",
      "Epoch [8/100], Step [1050/1751], Loss: 1.5348\n",
      "Epoch [8/100], Step [1060/1751], Loss: 1.5839\n",
      "Epoch [8/100], Step [1070/1751], Loss: 1.5150\n",
      "Epoch [8/100], Step [1080/1751], Loss: 1.5387\n",
      "Epoch [8/100], Step [1090/1751], Loss: 1.5329\n",
      "Epoch [8/100], Step [1100/1751], Loss: 1.3431\n",
      "Epoch [8/100], Step [1110/1751], Loss: 1.5646\n",
      "Epoch [8/100], Step [1120/1751], Loss: 1.4710\n",
      "Epoch [8/100], Step [1130/1751], Loss: 1.5328\n",
      "Epoch [8/100], Step [1140/1751], Loss: 1.4696\n",
      "Epoch [8/100], Step [1150/1751], Loss: 1.5127\n",
      "Epoch [8/100], Step [1160/1751], Loss: 1.4854\n",
      "Epoch [8/100], Step [1170/1751], Loss: 1.3555\n",
      "Epoch [8/100], Step [1180/1751], Loss: 1.3875\n",
      "Epoch [8/100], Step [1190/1751], Loss: 1.6268\n",
      "Epoch [8/100], Step [1200/1751], Loss: 1.7138\n",
      "Epoch [8/100], Step [1210/1751], Loss: 1.4684\n",
      "Epoch [8/100], Step [1220/1751], Loss: 1.4620\n",
      "Epoch [8/100], Step [1230/1751], Loss: 1.5195\n",
      "Epoch [8/100], Step [1240/1751], Loss: 1.4517\n",
      "Epoch [8/100], Step [1250/1751], Loss: 1.5809\n",
      "Epoch [8/100], Step [1260/1751], Loss: 1.6506\n",
      "Epoch [8/100], Step [1270/1751], Loss: 1.4293\n",
      "Epoch [8/100], Step [1280/1751], Loss: 1.4664\n",
      "Epoch [8/100], Step [1290/1751], Loss: 1.5035\n",
      "Epoch [8/100], Step [1300/1751], Loss: 1.4154\n",
      "Epoch [8/100], Step [1310/1751], Loss: 1.3962\n",
      "Epoch [8/100], Step [1320/1751], Loss: 1.5027\n",
      "Epoch [8/100], Step [1330/1751], Loss: 1.4781\n",
      "Epoch [8/100], Step [1340/1751], Loss: 1.5777\n",
      "Epoch [8/100], Step [1350/1751], Loss: 1.5457\n",
      "Epoch [8/100], Step [1360/1751], Loss: 1.6273\n",
      "Epoch [8/100], Step [1370/1751], Loss: 1.4540\n",
      "Epoch [8/100], Step [1380/1751], Loss: 1.5056\n",
      "Epoch [8/100], Step [1390/1751], Loss: 1.4520\n",
      "Epoch [8/100], Step [1400/1751], Loss: 1.4491\n",
      "Epoch [8/100], Step [1410/1751], Loss: 1.5095\n",
      "Epoch [8/100], Step [1420/1751], Loss: 1.3970\n",
      "Epoch [8/100], Step [1430/1751], Loss: 1.5887\n",
      "Epoch [8/100], Step [1440/1751], Loss: 1.4088\n",
      "Epoch [8/100], Step [1450/1751], Loss: 1.4555\n",
      "Epoch [8/100], Step [1460/1751], Loss: 1.4273\n",
      "Epoch [8/100], Step [1470/1751], Loss: 1.3500\n",
      "Epoch [8/100], Step [1480/1751], Loss: 1.5052\n",
      "Epoch [8/100], Step [1490/1751], Loss: 1.4298\n",
      "Epoch [8/100], Step [1500/1751], Loss: 1.4377\n",
      "Epoch [8/100], Step [1510/1751], Loss: 1.3995\n",
      "Epoch [8/100], Step [1520/1751], Loss: 1.4442\n",
      "Epoch [8/100], Step [1530/1751], Loss: 1.4426\n",
      "Epoch [8/100], Step [1540/1751], Loss: 1.4416\n",
      "Epoch [8/100], Step [1550/1751], Loss: 1.3290\n",
      "Epoch [8/100], Step [1560/1751], Loss: 1.5334\n",
      "Epoch [8/100], Step [1570/1751], Loss: 1.5685\n",
      "Epoch [8/100], Step [1580/1751], Loss: 1.4028\n",
      "Epoch [8/100], Step [1590/1751], Loss: 1.4556\n",
      "Epoch [8/100], Step [1600/1751], Loss: 1.5000\n",
      "Epoch [8/100], Step [1610/1751], Loss: 1.3766\n",
      "Epoch [8/100], Step [1620/1751], Loss: 1.3472\n",
      "Epoch [8/100], Step [1630/1751], Loss: 1.5869\n",
      "Epoch [8/100], Step [1640/1751], Loss: 1.3765\n",
      "Epoch [8/100], Step [1650/1751], Loss: 1.4655\n",
      "Epoch [8/100], Step [1660/1751], Loss: 1.5417\n",
      "Epoch [8/100], Step [1670/1751], Loss: 1.3546\n",
      "Epoch [8/100], Step [1680/1751], Loss: 1.4694\n",
      "Epoch [8/100], Step [1690/1751], Loss: 1.5897\n",
      "Epoch [8/100], Step [1700/1751], Loss: 1.6882\n",
      "Epoch [8/100], Step [1710/1751], Loss: 1.4356\n",
      "Epoch [8/100], Step [1720/1751], Loss: 1.2937\n",
      "Epoch [8/100], Step [1730/1751], Loss: 1.5580\n",
      "Epoch [8/100], Step [1740/1751], Loss: 1.4630\n",
      "Epoch [8/100], Step [1750/1751], Loss: 1.6624\n",
      "Epoch [8/100], Average Loss: 1.4903, Time: 1637.2486s\n",
      "Epoch [9/100], Step [10/1751], Loss: 1.5805\n",
      "Epoch [9/100], Step [20/1751], Loss: 1.5173\n",
      "Epoch [9/100], Step [30/1751], Loss: 1.3592\n",
      "Epoch [9/100], Step [40/1751], Loss: 1.4741\n",
      "Epoch [9/100], Step [50/1751], Loss: 1.3697\n",
      "Epoch [9/100], Step [60/1751], Loss: 1.3722\n",
      "Epoch [9/100], Step [70/1751], Loss: 1.5374\n",
      "Epoch [9/100], Step [80/1751], Loss: 1.3078\n",
      "Epoch [9/100], Step [90/1751], Loss: 1.4185\n",
      "Epoch [9/100], Step [100/1751], Loss: 1.4698\n",
      "Epoch [9/100], Step [110/1751], Loss: 1.5608\n",
      "Epoch [9/100], Step [120/1751], Loss: 1.4632\n",
      "Epoch [9/100], Step [130/1751], Loss: 1.4472\n",
      "Epoch [9/100], Step [140/1751], Loss: 1.4401\n",
      "Epoch [9/100], Step [150/1751], Loss: 1.5108\n",
      "Epoch [9/100], Step [160/1751], Loss: 1.5034\n",
      "Epoch [9/100], Step [170/1751], Loss: 1.4535\n",
      "Epoch [9/100], Step [180/1751], Loss: 1.5094\n",
      "Epoch [9/100], Step [190/1751], Loss: 1.4145\n",
      "Epoch [9/100], Step [200/1751], Loss: 1.5003\n",
      "Epoch [9/100], Step [210/1751], Loss: 1.5258\n",
      "Epoch [9/100], Step [220/1751], Loss: 1.3829\n",
      "Epoch [9/100], Step [230/1751], Loss: 1.3806\n",
      "Epoch [9/100], Step [240/1751], Loss: 1.3230\n",
      "Epoch [9/100], Step [250/1751], Loss: 1.5574\n",
      "Epoch [9/100], Step [260/1751], Loss: 1.3839\n",
      "Epoch [9/100], Step [270/1751], Loss: 1.6129\n",
      "Epoch [9/100], Step [280/1751], Loss: 1.4650\n",
      "Epoch [9/100], Step [290/1751], Loss: 1.5325\n",
      "Epoch [9/100], Step [300/1751], Loss: 1.4142\n",
      "Epoch [9/100], Step [310/1751], Loss: 1.4014\n",
      "Epoch [9/100], Step [320/1751], Loss: 1.4092\n",
      "Epoch [9/100], Step [330/1751], Loss: 1.7361\n",
      "Epoch [9/100], Step [340/1751], Loss: 1.5073\n",
      "Epoch [9/100], Step [350/1751], Loss: 1.4308\n",
      "Epoch [9/100], Step [360/1751], Loss: 1.5568\n",
      "Epoch [9/100], Step [370/1751], Loss: 1.3813\n",
      "Epoch [9/100], Step [380/1751], Loss: 1.5138\n",
      "Epoch [9/100], Step [390/1751], Loss: 1.4119\n",
      "Epoch [9/100], Step [400/1751], Loss: 1.6682\n",
      "Epoch [9/100], Step [410/1751], Loss: 1.5157\n",
      "Epoch [9/100], Step [420/1751], Loss: 1.4194\n",
      "Epoch [9/100], Step [430/1751], Loss: 1.5003\n",
      "Epoch [9/100], Step [440/1751], Loss: 1.3814\n",
      "Epoch [9/100], Step [450/1751], Loss: 1.4360\n",
      "Epoch [9/100], Step [460/1751], Loss: 1.4492\n",
      "Epoch [9/100], Step [470/1751], Loss: 1.4200\n",
      "Epoch [9/100], Step [480/1751], Loss: 1.6253\n",
      "Epoch [9/100], Step [490/1751], Loss: 1.4258\n",
      "Epoch [9/100], Step [500/1751], Loss: 1.4195\n",
      "Epoch [9/100], Step [510/1751], Loss: 1.4604\n",
      "Epoch [9/100], Step [520/1751], Loss: 1.3016\n",
      "Epoch [9/100], Step [530/1751], Loss: 1.4123\n",
      "Epoch [9/100], Step [540/1751], Loss: 1.3251\n",
      "Epoch [9/100], Step [550/1751], Loss: 1.6487\n",
      "Epoch [9/100], Step [560/1751], Loss: 1.4895\n",
      "Epoch [9/100], Step [570/1751], Loss: 1.4022\n",
      "Epoch [9/100], Step [580/1751], Loss: 1.4942\n",
      "Epoch [9/100], Step [590/1751], Loss: 1.4866\n",
      "Epoch [9/100], Step [600/1751], Loss: 1.3986\n",
      "Epoch [9/100], Step [610/1751], Loss: 1.4382\n",
      "Epoch [9/100], Step [620/1751], Loss: 1.3141\n",
      "Epoch [9/100], Step [630/1751], Loss: 1.4207\n",
      "Epoch [9/100], Step [640/1751], Loss: 1.4215\n",
      "Epoch [9/100], Step [650/1751], Loss: 1.5287\n",
      "Epoch [9/100], Step [660/1751], Loss: 1.4196\n",
      "Epoch [9/100], Step [670/1751], Loss: 1.6618\n",
      "Epoch [9/100], Step [680/1751], Loss: 1.3920\n",
      "Epoch [9/100], Step [690/1751], Loss: 1.4734\n",
      "Epoch [9/100], Step [700/1751], Loss: 1.3956\n",
      "Epoch [9/100], Step [710/1751], Loss: 1.4961\n",
      "Epoch [9/100], Step [720/1751], Loss: 1.4405\n",
      "Epoch [9/100], Step [730/1751], Loss: 1.4038\n",
      "Epoch [9/100], Step [740/1751], Loss: 1.2852\n",
      "Epoch [9/100], Step [750/1751], Loss: 1.6257\n",
      "Epoch [9/100], Step [760/1751], Loss: 1.4539\n",
      "Epoch [9/100], Step [770/1751], Loss: 1.5424\n",
      "Epoch [9/100], Step [780/1751], Loss: 1.4414\n",
      "Epoch [9/100], Step [790/1751], Loss: 1.4843\n",
      "Epoch [9/100], Step [800/1751], Loss: 1.4365\n",
      "Epoch [9/100], Step [810/1751], Loss: 1.5688\n",
      "Epoch [9/100], Step [820/1751], Loss: 1.5669\n",
      "Epoch [9/100], Step [830/1751], Loss: 1.3193\n",
      "Epoch [9/100], Step [840/1751], Loss: 1.3729\n",
      "Epoch [9/100], Step [850/1751], Loss: 1.4143\n",
      "Epoch [9/100], Step [860/1751], Loss: 1.5092\n",
      "Epoch [9/100], Step [870/1751], Loss: 1.4096\n",
      "Epoch [9/100], Step [880/1751], Loss: 1.1888\n",
      "Epoch [9/100], Step [890/1751], Loss: 1.3691\n",
      "Epoch [9/100], Step [900/1751], Loss: 1.2609\n",
      "Epoch [9/100], Step [910/1751], Loss: 1.5680\n",
      "Epoch [9/100], Step [920/1751], Loss: 1.4149\n",
      "Epoch [9/100], Step [930/1751], Loss: 1.3515\n",
      "Epoch [9/100], Step [940/1751], Loss: 1.4984\n",
      "Epoch [9/100], Step [950/1751], Loss: 1.4108\n",
      "Epoch [9/100], Step [960/1751], Loss: 1.4440\n",
      "Epoch [9/100], Step [970/1751], Loss: 1.5358\n",
      "Epoch [9/100], Step [980/1751], Loss: 1.5447\n",
      "Epoch [9/100], Step [990/1751], Loss: 1.3805\n",
      "Epoch [9/100], Step [1000/1751], Loss: 1.4536\n",
      "Epoch [9/100], Step [1010/1751], Loss: 1.4749\n",
      "Epoch [9/100], Step [1020/1751], Loss: 1.4704\n",
      "Epoch [9/100], Step [1030/1751], Loss: 1.4700\n",
      "Epoch [9/100], Step [1040/1751], Loss: 1.4929\n",
      "Epoch [9/100], Step [1050/1751], Loss: 1.4612\n",
      "Epoch [9/100], Step [1060/1751], Loss: 1.3249\n",
      "Epoch [9/100], Step [1070/1751], Loss: 1.3809\n",
      "Epoch [9/100], Step [1080/1751], Loss: 1.5427\n",
      "Epoch [9/100], Step [1090/1751], Loss: 1.3576\n",
      "Epoch [9/100], Step [1100/1751], Loss: 1.4693\n",
      "Epoch [9/100], Step [1110/1751], Loss: 1.4977\n",
      "Epoch [9/100], Step [1120/1751], Loss: 1.5004\n",
      "Epoch [9/100], Step [1130/1751], Loss: 1.5303\n",
      "Epoch [9/100], Step [1140/1751], Loss: 1.3922\n",
      "Epoch [9/100], Step [1150/1751], Loss: 1.3228\n",
      "Epoch [9/100], Step [1160/1751], Loss: 1.5026\n",
      "Epoch [9/100], Step [1170/1751], Loss: 1.3945\n",
      "Epoch [9/100], Step [1180/1751], Loss: 1.4658\n",
      "Epoch [9/100], Step [1190/1751], Loss: 1.4113\n",
      "Epoch [9/100], Step [1200/1751], Loss: 1.5212\n",
      "Epoch [9/100], Step [1210/1751], Loss: 1.5150\n",
      "Epoch [9/100], Step [1220/1751], Loss: 1.3910\n",
      "Epoch [9/100], Step [1230/1751], Loss: 1.4551\n",
      "Epoch [9/100], Step [1240/1751], Loss: 1.4598\n",
      "Epoch [9/100], Step [1250/1751], Loss: 1.4433\n",
      "Epoch [9/100], Step [1260/1751], Loss: 1.3346\n",
      "Epoch [9/100], Step [1270/1751], Loss: 1.2986\n",
      "Epoch [9/100], Step [1280/1751], Loss: 1.4453\n",
      "Epoch [9/100], Step [1290/1751], Loss: 1.4813\n",
      "Epoch [9/100], Step [1300/1751], Loss: 1.3849\n",
      "Epoch [9/100], Step [1310/1751], Loss: 1.3871\n",
      "Epoch [9/100], Step [1320/1751], Loss: 1.3805\n",
      "Epoch [9/100], Step [1330/1751], Loss: 1.3724\n",
      "Epoch [9/100], Step [1340/1751], Loss: 1.4638\n",
      "Epoch [9/100], Step [1350/1751], Loss: 1.4776\n",
      "Epoch [9/100], Step [1360/1751], Loss: 1.4574\n",
      "Epoch [9/100], Step [1370/1751], Loss: 1.5512\n",
      "Epoch [9/100], Step [1380/1751], Loss: 1.4015\n",
      "Epoch [9/100], Step [1390/1751], Loss: 1.4235\n",
      "Epoch [9/100], Step [1400/1751], Loss: 1.3757\n",
      "Epoch [9/100], Step [1410/1751], Loss: 1.3848\n",
      "Epoch [9/100], Step [1420/1751], Loss: 1.3034\n",
      "Epoch [9/100], Step [1430/1751], Loss: 1.3263\n",
      "Epoch [9/100], Step [1440/1751], Loss: 1.4850\n",
      "Epoch [9/100], Step [1450/1751], Loss: 1.5112\n",
      "Epoch [9/100], Step [1460/1751], Loss: 1.4604\n",
      "Epoch [9/100], Step [1470/1751], Loss: 1.3120\n",
      "Epoch [9/100], Step [1480/1751], Loss: 1.3742\n",
      "Epoch [9/100], Step [1490/1751], Loss: 1.4305\n",
      "Epoch [9/100], Step [1500/1751], Loss: 1.4641\n",
      "Epoch [9/100], Step [1510/1751], Loss: 1.4692\n",
      "Epoch [9/100], Step [1520/1751], Loss: 1.3902\n",
      "Epoch [9/100], Step [1530/1751], Loss: 1.5070\n",
      "Epoch [9/100], Step [1540/1751], Loss: 1.3396\n",
      "Epoch [9/100], Step [1550/1751], Loss: 1.4493\n",
      "Epoch [9/100], Step [1560/1751], Loss: 1.4247\n",
      "Epoch [9/100], Step [1570/1751], Loss: 1.5438\n",
      "Epoch [9/100], Step [1580/1751], Loss: 1.1680\n",
      "Epoch [9/100], Step [1590/1751], Loss: 1.4388\n",
      "Epoch [9/100], Step [1600/1751], Loss: 1.3815\n",
      "Epoch [9/100], Step [1610/1751], Loss: 1.5879\n",
      "Epoch [9/100], Step [1620/1751], Loss: 1.2843\n",
      "Epoch [9/100], Step [1630/1751], Loss: 1.5033\n",
      "Epoch [9/100], Step [1640/1751], Loss: 1.4961\n",
      "Epoch [9/100], Step [1650/1751], Loss: 1.3748\n",
      "Epoch [9/100], Step [1660/1751], Loss: 1.4300\n",
      "Epoch [9/100], Step [1670/1751], Loss: 1.4451\n",
      "Epoch [9/100], Step [1680/1751], Loss: 1.3741\n",
      "Epoch [9/100], Step [1690/1751], Loss: 1.3667\n",
      "Epoch [9/100], Step [1700/1751], Loss: 1.2405\n",
      "Epoch [9/100], Step [1710/1751], Loss: 1.0990\n",
      "Epoch [9/100], Step [1720/1751], Loss: 1.3338\n",
      "Epoch [9/100], Step [1730/1751], Loss: 1.4476\n",
      "Epoch [9/100], Step [1740/1751], Loss: 1.3893\n",
      "Epoch [9/100], Step [1750/1751], Loss: 1.3588\n",
      "Epoch [9/100], Average Loss: 1.4472, Time: 1637.0982s\n",
      "Epoch [10/100], Step [10/1751], Loss: 1.3444\n",
      "Epoch [10/100], Step [20/1751], Loss: 1.4622\n",
      "Epoch [10/100], Step [30/1751], Loss: 1.3453\n",
      "Epoch [10/100], Step [40/1751], Loss: 1.3928\n",
      "Epoch [10/100], Step [50/1751], Loss: 1.5050\n",
      "Epoch [10/100], Step [60/1751], Loss: 1.4519\n",
      "Epoch [10/100], Step [70/1751], Loss: 1.5477\n",
      "Epoch [10/100], Step [80/1751], Loss: 1.3304\n",
      "Epoch [10/100], Step [90/1751], Loss: 1.4578\n",
      "Epoch [10/100], Step [100/1751], Loss: 1.4680\n",
      "Epoch [10/100], Step [110/1751], Loss: 1.3243\n",
      "Epoch [10/100], Step [120/1751], Loss: 1.4325\n",
      "Epoch [10/100], Step [130/1751], Loss: 1.4209\n",
      "Epoch [10/100], Step [140/1751], Loss: 1.4348\n",
      "Epoch [10/100], Step [150/1751], Loss: 1.6259\n",
      "Epoch [10/100], Step [160/1751], Loss: 1.5919\n",
      "Epoch [10/100], Step [170/1751], Loss: 1.4616\n",
      "Epoch [10/100], Step [180/1751], Loss: 1.3525\n",
      "Epoch [10/100], Step [190/1751], Loss: 1.3797\n",
      "Epoch [10/100], Step [200/1751], Loss: 1.4196\n",
      "Epoch [10/100], Step [210/1751], Loss: 1.6118\n",
      "Epoch [10/100], Step [220/1751], Loss: 1.4492\n",
      "Epoch [10/100], Step [230/1751], Loss: 1.2517\n",
      "Epoch [10/100], Step [240/1751], Loss: 1.4400\n",
      "Epoch [10/100], Step [250/1751], Loss: 1.5652\n",
      "Epoch [10/100], Step [260/1751], Loss: 1.3415\n",
      "Epoch [10/100], Step [270/1751], Loss: 1.4207\n",
      "Epoch [10/100], Step [280/1751], Loss: 1.5666\n",
      "Epoch [10/100], Step [290/1751], Loss: 1.5012\n",
      "Epoch [10/100], Step [300/1751], Loss: 1.4540\n",
      "Epoch [10/100], Step [310/1751], Loss: 1.2816\n",
      "Epoch [10/100], Step [320/1751], Loss: 1.4369\n",
      "Epoch [10/100], Step [330/1751], Loss: 1.3136\n",
      "Epoch [10/100], Step [340/1751], Loss: 1.4358\n",
      "Epoch [10/100], Step [350/1751], Loss: 1.3958\n",
      "Epoch [10/100], Step [360/1751], Loss: 1.4644\n",
      "Epoch [10/100], Step [370/1751], Loss: 1.3917\n",
      "Epoch [10/100], Step [380/1751], Loss: 1.3945\n",
      "Epoch [10/100], Step [390/1751], Loss: 1.4688\n",
      "Epoch [10/100], Step [400/1751], Loss: 1.5663\n",
      "Epoch [10/100], Step [410/1751], Loss: 1.4622\n",
      "Epoch [10/100], Step [420/1751], Loss: 1.3767\n",
      "Epoch [10/100], Step [430/1751], Loss: 1.4129\n",
      "Epoch [10/100], Step [440/1751], Loss: 1.4194\n",
      "Epoch [10/100], Step [450/1751], Loss: 1.2903\n",
      "Epoch [10/100], Step [460/1751], Loss: 1.4509\n",
      "Epoch [10/100], Step [470/1751], Loss: 1.5581\n",
      "Epoch [10/100], Step [480/1751], Loss: 1.2899\n",
      "Epoch [10/100], Step [490/1751], Loss: 1.3835\n",
      "Epoch [10/100], Step [500/1751], Loss: 1.3705\n",
      "Epoch [10/100], Step [510/1751], Loss: 1.4494\n",
      "Epoch [10/100], Step [520/1751], Loss: 1.3624\n",
      "Epoch [10/100], Step [530/1751], Loss: 1.3918\n",
      "Epoch [10/100], Step [540/1751], Loss: 1.3282\n",
      "Epoch [10/100], Step [550/1751], Loss: 1.4068\n",
      "Epoch [10/100], Step [560/1751], Loss: 1.4345\n",
      "Epoch [10/100], Step [570/1751], Loss: 1.3226\n",
      "Epoch [10/100], Step [580/1751], Loss: 1.5080\n",
      "Epoch [10/100], Step [590/1751], Loss: 1.5255\n",
      "Epoch [10/100], Step [600/1751], Loss: 1.4798\n",
      "Epoch [10/100], Step [610/1751], Loss: 1.3869\n",
      "Epoch [10/100], Step [620/1751], Loss: 1.4488\n",
      "Epoch [10/100], Step [630/1751], Loss: 1.4720\n",
      "Epoch [10/100], Step [640/1751], Loss: 1.5067\n",
      "Epoch [10/100], Step [650/1751], Loss: 1.3857\n",
      "Epoch [10/100], Step [660/1751], Loss: 1.4570\n",
      "Epoch [10/100], Step [670/1751], Loss: 1.3455\n",
      "Epoch [10/100], Step [680/1751], Loss: 1.3636\n",
      "Epoch [10/100], Step [690/1751], Loss: 1.4158\n",
      "Epoch [10/100], Step [700/1751], Loss: 1.4813\n",
      "Epoch [10/100], Step [710/1751], Loss: 1.4189\n",
      "Epoch [10/100], Step [720/1751], Loss: 1.2860\n",
      "Epoch [10/100], Step [730/1751], Loss: 1.3767\n",
      "Epoch [10/100], Step [740/1751], Loss: 1.5800\n",
      "Epoch [10/100], Step [750/1751], Loss: 1.4335\n",
      "Epoch [10/100], Step [760/1751], Loss: 1.5494\n",
      "Epoch [10/100], Step [770/1751], Loss: 1.4140\n",
      "Epoch [10/100], Step [780/1751], Loss: 1.3131\n",
      "Epoch [10/100], Step [790/1751], Loss: 1.4240\n",
      "Epoch [10/100], Step [800/1751], Loss: 1.3578\n",
      "Epoch [10/100], Step [810/1751], Loss: 1.4059\n",
      "Epoch [10/100], Step [820/1751], Loss: 1.5178\n",
      "Epoch [10/100], Step [830/1751], Loss: 1.4397\n",
      "Epoch [10/100], Step [840/1751], Loss: 1.4755\n",
      "Epoch [10/100], Step [850/1751], Loss: 1.5512\n",
      "Epoch [10/100], Step [860/1751], Loss: 1.2620\n",
      "Epoch [10/100], Step [870/1751], Loss: 1.4129\n",
      "Epoch [10/100], Step [880/1751], Loss: 1.3580\n",
      "Epoch [10/100], Step [890/1751], Loss: 1.5214\n",
      "Epoch [10/100], Step [900/1751], Loss: 1.3956\n",
      "Epoch [10/100], Step [910/1751], Loss: 1.3547\n",
      "Epoch [10/100], Step [920/1751], Loss: 1.4717\n",
      "Epoch [10/100], Step [930/1751], Loss: 1.4744\n",
      "Epoch [10/100], Step [940/1751], Loss: 1.3634\n",
      "Epoch [10/100], Step [950/1751], Loss: 1.3490\n",
      "Epoch [10/100], Step [960/1751], Loss: 1.4803\n",
      "Epoch [10/100], Step [970/1751], Loss: 1.5183\n",
      "Epoch [10/100], Step [980/1751], Loss: 1.4177\n",
      "Epoch [10/100], Step [990/1751], Loss: 1.3642\n",
      "Epoch [10/100], Step [1000/1751], Loss: 1.5320\n",
      "Epoch [10/100], Step [1010/1751], Loss: 1.4758\n",
      "Epoch [10/100], Step [1020/1751], Loss: 1.3346\n",
      "Epoch [10/100], Step [1030/1751], Loss: 1.4414\n",
      "Epoch [10/100], Step [1040/1751], Loss: 1.4686\n",
      "Epoch [10/100], Step [1050/1751], Loss: 1.4006\n",
      "Epoch [10/100], Step [1060/1751], Loss: 1.5283\n",
      "Epoch [10/100], Step [1070/1751], Loss: 1.4213\n",
      "Epoch [10/100], Step [1080/1751], Loss: 1.4011\n",
      "Epoch [10/100], Step [1090/1751], Loss: 1.3308\n",
      "Epoch [10/100], Step [1100/1751], Loss: 1.4888\n",
      "Epoch [10/100], Step [1110/1751], Loss: 1.3803\n",
      "Epoch [10/100], Step [1120/1751], Loss: 1.4611\n",
      "Epoch [10/100], Step [1130/1751], Loss: 1.3291\n",
      "Epoch [10/100], Step [1140/1751], Loss: 1.3475\n",
      "Epoch [10/100], Step [1150/1751], Loss: 1.3288\n",
      "Epoch [10/100], Step [1160/1751], Loss: 1.5028\n",
      "Epoch [10/100], Step [1170/1751], Loss: 1.5500\n",
      "Epoch [10/100], Step [1180/1751], Loss: 1.4665\n",
      "Epoch [10/100], Step [1190/1751], Loss: 1.4490\n",
      "Epoch [10/100], Step [1200/1751], Loss: 1.5170\n",
      "Epoch [10/100], Step [1210/1751], Loss: 1.3954\n",
      "Epoch [10/100], Step [1220/1751], Loss: 1.4088\n",
      "Epoch [10/100], Step [1230/1751], Loss: 1.2422\n",
      "Epoch [10/100], Step [1240/1751], Loss: 1.4771\n",
      "Epoch [10/100], Step [1250/1751], Loss: 1.2625\n",
      "Epoch [10/100], Step [1260/1751], Loss: 1.3969\n",
      "Epoch [10/100], Step [1270/1751], Loss: 1.5299\n",
      "Epoch [10/100], Step [1280/1751], Loss: 1.4324\n",
      "Epoch [10/100], Step [1290/1751], Loss: 1.4266\n",
      "Epoch [10/100], Step [1300/1751], Loss: 1.4194\n",
      "Epoch [10/100], Step [1310/1751], Loss: 1.3071\n",
      "Epoch [10/100], Step [1320/1751], Loss: 1.3229\n",
      "Epoch [10/100], Step [1330/1751], Loss: 1.3838\n",
      "Epoch [10/100], Step [1340/1751], Loss: 1.2769\n",
      "Epoch [10/100], Step [1350/1751], Loss: 1.4096\n",
      "Epoch [10/100], Step [1360/1751], Loss: 1.4496\n",
      "Epoch [10/100], Step [1370/1751], Loss: 1.3096\n",
      "Epoch [10/100], Step [1380/1751], Loss: 1.3440\n",
      "Epoch [10/100], Step [1390/1751], Loss: 1.4681\n",
      "Epoch [10/100], Step [1400/1751], Loss: 1.5578\n",
      "Epoch [10/100], Step [1410/1751], Loss: 1.4359\n",
      "Epoch [10/100], Step [1420/1751], Loss: 1.4731\n",
      "Epoch [10/100], Step [1430/1751], Loss: 1.6259\n",
      "Epoch [10/100], Step [1440/1751], Loss: 1.2442\n",
      "Epoch [10/100], Step [1450/1751], Loss: 1.4610\n",
      "Epoch [10/100], Step [1460/1751], Loss: 1.6172\n",
      "Epoch [10/100], Step [1470/1751], Loss: 1.4316\n",
      "Epoch [10/100], Step [1480/1751], Loss: 1.4541\n",
      "Epoch [10/100], Step [1490/1751], Loss: 1.5166\n",
      "Epoch [10/100], Step [1500/1751], Loss: 1.3944\n",
      "Epoch [10/100], Step [1510/1751], Loss: 1.3256\n",
      "Epoch [10/100], Step [1520/1751], Loss: 1.3394\n",
      "Epoch [10/100], Step [1530/1751], Loss: 1.3967\n",
      "Epoch [10/100], Step [1540/1751], Loss: 1.4903\n",
      "Epoch [10/100], Step [1550/1751], Loss: 1.3967\n",
      "Epoch [10/100], Step [1560/1751], Loss: 1.3517\n",
      "Epoch [10/100], Step [1570/1751], Loss: 1.3918\n",
      "Epoch [10/100], Step [1580/1751], Loss: 1.3411\n",
      "Epoch [10/100], Step [1590/1751], Loss: 1.4947\n",
      "Epoch [10/100], Step [1600/1751], Loss: 1.5144\n",
      "Epoch [10/100], Step [1610/1751], Loss: 1.3814\n",
      "Epoch [10/100], Step [1620/1751], Loss: 1.3837\n",
      "Epoch [10/100], Step [1630/1751], Loss: 1.4306\n",
      "Epoch [10/100], Step [1640/1751], Loss: 1.3114\n",
      "Epoch [10/100], Step [1650/1751], Loss: 1.4425\n",
      "Epoch [10/100], Step [1660/1751], Loss: 1.2830\n",
      "Epoch [10/100], Step [1670/1751], Loss: 1.4480\n",
      "Epoch [10/100], Step [1680/1751], Loss: 1.5044\n",
      "Epoch [10/100], Step [1690/1751], Loss: 1.3941\n",
      "Epoch [10/100], Step [1700/1751], Loss: 1.4635\n",
      "Epoch [10/100], Step [1710/1751], Loss: 1.4502\n",
      "Epoch [10/100], Step [1720/1751], Loss: 1.3731\n",
      "Epoch [10/100], Step [1730/1751], Loss: 1.2586\n",
      "Epoch [10/100], Step [1740/1751], Loss: 1.4712\n",
      "Epoch [10/100], Step [1750/1751], Loss: 1.2788\n",
      "Epoch [10/100], Average Loss: 1.4154, Time: 1637.3393s\n",
      "Epoch [11/100], Step [10/1751], Loss: 1.5122\n",
      "Epoch [11/100], Step [20/1751], Loss: 1.4012\n",
      "Epoch [11/100], Step [30/1751], Loss: 1.4337\n",
      "Epoch [11/100], Step [40/1751], Loss: 1.2615\n",
      "Epoch [11/100], Step [50/1751], Loss: 1.2994\n",
      "Epoch [11/100], Step [60/1751], Loss: 1.4763\n",
      "Epoch [11/100], Step [70/1751], Loss: 1.4042\n",
      "Epoch [11/100], Step [80/1751], Loss: 1.5063\n",
      "Epoch [11/100], Step [90/1751], Loss: 1.4098\n",
      "Epoch [11/100], Step [100/1751], Loss: 1.4907\n",
      "Epoch [11/100], Step [110/1751], Loss: 1.4515\n",
      "Epoch [11/100], Step [120/1751], Loss: 1.3889\n",
      "Epoch [11/100], Step [130/1751], Loss: 1.3654\n",
      "Epoch [11/100], Step [140/1751], Loss: 1.3075\n",
      "Epoch [11/100], Step [150/1751], Loss: 1.3166\n",
      "Epoch [11/100], Step [160/1751], Loss: 1.2540\n",
      "Epoch [11/100], Step [170/1751], Loss: 1.4211\n",
      "Epoch [11/100], Step [180/1751], Loss: 1.3681\n",
      "Epoch [11/100], Step [190/1751], Loss: 1.3558\n",
      "Epoch [11/100], Step [200/1751], Loss: 1.4607\n",
      "Epoch [11/100], Step [210/1751], Loss: 1.4226\n",
      "Epoch [11/100], Step [220/1751], Loss: 1.3748\n",
      "Epoch [11/100], Step [230/1751], Loss: 1.3014\n",
      "Epoch [11/100], Step [240/1751], Loss: 1.3494\n",
      "Epoch [11/100], Step [250/1751], Loss: 1.2461\n",
      "Epoch [11/100], Step [260/1751], Loss: 1.4403\n",
      "Epoch [11/100], Step [270/1751], Loss: 1.3413\n",
      "Epoch [11/100], Step [280/1751], Loss: 1.3986\n",
      "Epoch [11/100], Step [290/1751], Loss: 1.5108\n",
      "Epoch [11/100], Step [300/1751], Loss: 1.5574\n",
      "Epoch [11/100], Step [310/1751], Loss: 1.6069\n",
      "Epoch [11/100], Step [320/1751], Loss: 1.3337\n",
      "Epoch [11/100], Step [330/1751], Loss: 1.3115\n",
      "Epoch [11/100], Step [340/1751], Loss: 1.4460\n",
      "Epoch [11/100], Step [350/1751], Loss: 1.3789\n",
      "Epoch [11/100], Step [360/1751], Loss: 1.6039\n",
      "Epoch [11/100], Step [370/1751], Loss: 1.4639\n",
      "Epoch [11/100], Step [380/1751], Loss: 1.4604\n",
      "Epoch [11/100], Step [390/1751], Loss: 1.3677\n",
      "Epoch [11/100], Step [400/1751], Loss: 1.3907\n",
      "Epoch [11/100], Step [410/1751], Loss: 1.2379\n",
      "Epoch [11/100], Step [420/1751], Loss: 1.4612\n",
      "Epoch [11/100], Step [430/1751], Loss: 1.3559\n",
      "Epoch [11/100], Step [440/1751], Loss: 1.4845\n",
      "Epoch [11/100], Step [450/1751], Loss: 1.2699\n",
      "Epoch [11/100], Step [460/1751], Loss: 1.3296\n",
      "Epoch [11/100], Step [470/1751], Loss: 1.4375\n",
      "Epoch [11/100], Step [480/1751], Loss: 1.5066\n",
      "Epoch [11/100], Step [490/1751], Loss: 1.3065\n",
      "Epoch [11/100], Step [500/1751], Loss: 1.4872\n",
      "Epoch [11/100], Step [510/1751], Loss: 1.4318\n",
      "Epoch [11/100], Step [520/1751], Loss: 1.3868\n",
      "Epoch [11/100], Step [530/1751], Loss: 1.3701\n",
      "Epoch [11/100], Step [540/1751], Loss: 1.3886\n",
      "Epoch [11/100], Step [550/1751], Loss: 1.3899\n",
      "Epoch [11/100], Step [560/1751], Loss: 1.4047\n",
      "Epoch [11/100], Step [570/1751], Loss: 1.2867\n",
      "Epoch [11/100], Step [580/1751], Loss: 1.4005\n",
      "Epoch [11/100], Step [590/1751], Loss: 1.3670\n",
      "Epoch [11/100], Step [600/1751], Loss: 1.4443\n",
      "Epoch [11/100], Step [610/1751], Loss: 1.4865\n",
      "Epoch [11/100], Step [620/1751], Loss: 1.4937\n",
      "Epoch [11/100], Step [630/1751], Loss: 1.3611\n",
      "Epoch [11/100], Step [640/1751], Loss: 1.3486\n",
      "Epoch [11/100], Step [650/1751], Loss: 1.5017\n",
      "Epoch [11/100], Step [660/1751], Loss: 1.4672\n",
      "Epoch [11/100], Step [670/1751], Loss: 1.5170\n",
      "Epoch [11/100], Step [680/1751], Loss: 1.3101\n",
      "Epoch [11/100], Step [690/1751], Loss: 1.3700\n",
      "Epoch [11/100], Step [700/1751], Loss: 1.4048\n",
      "Epoch [11/100], Step [710/1751], Loss: 1.3458\n",
      "Epoch [11/100], Step [720/1751], Loss: 1.4831\n",
      "Epoch [11/100], Step [730/1751], Loss: 1.4395\n",
      "Epoch [11/100], Step [740/1751], Loss: 1.4494\n",
      "Epoch [11/100], Step [750/1751], Loss: 1.3823\n",
      "Epoch [11/100], Step [760/1751], Loss: 1.3650\n",
      "Epoch [11/100], Step [770/1751], Loss: 1.4291\n",
      "Epoch [11/100], Step [780/1751], Loss: 1.3922\n",
      "Epoch [11/100], Step [790/1751], Loss: 1.1939\n",
      "Epoch [11/100], Step [800/1751], Loss: 1.4601\n",
      "Epoch [11/100], Step [810/1751], Loss: 1.4830\n",
      "Epoch [11/100], Step [820/1751], Loss: 1.4311\n",
      "Epoch [11/100], Step [830/1751], Loss: 1.3050\n",
      "Epoch [11/100], Step [840/1751], Loss: 1.2326\n",
      "Epoch [11/100], Step [850/1751], Loss: 1.5623\n",
      "Epoch [11/100], Step [860/1751], Loss: 1.3475\n",
      "Epoch [11/100], Step [870/1751], Loss: 1.2972\n",
      "Epoch [11/100], Step [880/1751], Loss: 1.4264\n",
      "Epoch [11/100], Step [890/1751], Loss: 1.3838\n",
      "Epoch [11/100], Step [900/1751], Loss: 1.3717\n",
      "Epoch [11/100], Step [910/1751], Loss: 1.4276\n",
      "Epoch [11/100], Step [920/1751], Loss: 1.4102\n",
      "Epoch [11/100], Step [930/1751], Loss: 1.2270\n",
      "Epoch [11/100], Step [940/1751], Loss: 1.4346\n",
      "Epoch [11/100], Step [950/1751], Loss: 1.2646\n",
      "Epoch [11/100], Step [960/1751], Loss: 1.2769\n",
      "Epoch [11/100], Step [970/1751], Loss: 1.2462\n",
      "Epoch [11/100], Step [980/1751], Loss: 1.4751\n",
      "Epoch [11/100], Step [990/1751], Loss: 1.3449\n",
      "Epoch [11/100], Step [1000/1751], Loss: 1.4450\n",
      "Epoch [11/100], Step [1010/1751], Loss: 1.2224\n",
      "Epoch [11/100], Step [1020/1751], Loss: 1.4081\n",
      "Epoch [11/100], Step [1030/1751], Loss: 1.3396\n",
      "Epoch [11/100], Step [1040/1751], Loss: 1.4586\n",
      "Epoch [11/100], Step [1050/1751], Loss: 1.2764\n",
      "Epoch [11/100], Step [1060/1751], Loss: 1.5755\n",
      "Epoch [11/100], Step [1070/1751], Loss: 1.3267\n",
      "Epoch [11/100], Step [1080/1751], Loss: 1.3034\n",
      "Epoch [11/100], Step [1090/1751], Loss: 1.4896\n",
      "Epoch [11/100], Step [1100/1751], Loss: 1.4422\n",
      "Epoch [11/100], Step [1110/1751], Loss: 1.2104\n",
      "Epoch [11/100], Step [1120/1751], Loss: 1.3045\n",
      "Epoch [11/100], Step [1130/1751], Loss: 1.3573\n",
      "Epoch [11/100], Step [1140/1751], Loss: 1.2967\n",
      "Epoch [11/100], Step [1150/1751], Loss: 1.5536\n",
      "Epoch [11/100], Step [1160/1751], Loss: 1.5306\n",
      "Epoch [11/100], Step [1170/1751], Loss: 1.3993\n",
      "Epoch [11/100], Step [1180/1751], Loss: 1.4086\n",
      "Epoch [11/100], Step [1190/1751], Loss: 1.2253\n",
      "Epoch [11/100], Step [1200/1751], Loss: 1.3277\n",
      "Epoch [11/100], Step [1210/1751], Loss: 1.2675\n",
      "Epoch [11/100], Step [1220/1751], Loss: 1.4502\n",
      "Epoch [11/100], Step [1230/1751], Loss: 1.5054\n",
      "Epoch [11/100], Step [1240/1751], Loss: 1.2667\n",
      "Epoch [11/100], Step [1250/1751], Loss: 1.5064\n",
      "Epoch [11/100], Step [1260/1751], Loss: 1.3988\n",
      "Epoch [11/100], Step [1270/1751], Loss: 1.4883\n",
      "Epoch [11/100], Step [1280/1751], Loss: 1.4243\n",
      "Epoch [11/100], Step [1290/1751], Loss: 1.3408\n",
      "Epoch [11/100], Step [1300/1751], Loss: 1.2947\n",
      "Epoch [11/100], Step [1310/1751], Loss: 1.3400\n",
      "Epoch [11/100], Step [1320/1751], Loss: 1.4443\n",
      "Epoch [11/100], Step [1330/1751], Loss: 1.3735\n",
      "Epoch [11/100], Step [1340/1751], Loss: 1.4184\n",
      "Epoch [11/100], Step [1350/1751], Loss: 1.3625\n",
      "Epoch [11/100], Step [1360/1751], Loss: 1.4420\n",
      "Epoch [11/100], Step [1370/1751], Loss: 1.3267\n",
      "Epoch [11/100], Step [1380/1751], Loss: 1.2940\n",
      "Epoch [11/100], Step [1390/1751], Loss: 1.2970\n",
      "Epoch [11/100], Step [1400/1751], Loss: 1.2618\n",
      "Epoch [11/100], Step [1410/1751], Loss: 1.4485\n",
      "Epoch [11/100], Step [1420/1751], Loss: 1.4516\n",
      "Epoch [11/100], Step [1430/1751], Loss: 1.3839\n",
      "Epoch [11/100], Step [1440/1751], Loss: 1.5165\n",
      "Epoch [11/100], Step [1450/1751], Loss: 1.4691\n",
      "Epoch [11/100], Step [1460/1751], Loss: 1.3482\n",
      "Epoch [11/100], Step [1470/1751], Loss: 1.2968\n",
      "Epoch [11/100], Step [1480/1751], Loss: 1.4738\n",
      "Epoch [11/100], Step [1490/1751], Loss: 1.3743\n",
      "Epoch [11/100], Step [1500/1751], Loss: 1.3108\n",
      "Epoch [11/100], Step [1510/1751], Loss: 1.2096\n",
      "Epoch [11/100], Step [1520/1751], Loss: 1.5106\n",
      "Epoch [11/100], Step [1530/1751], Loss: 1.3215\n",
      "Epoch [11/100], Step [1540/1751], Loss: 1.2539\n",
      "Epoch [11/100], Step [1550/1751], Loss: 1.3429\n",
      "Epoch [11/100], Step [1560/1751], Loss: 1.4181\n",
      "Epoch [11/100], Step [1570/1751], Loss: 1.3503\n",
      "Epoch [11/100], Step [1580/1751], Loss: 1.3158\n",
      "Epoch [11/100], Step [1590/1751], Loss: 1.5331\n",
      "Epoch [11/100], Step [1600/1751], Loss: 1.3889\n",
      "Epoch [11/100], Step [1610/1751], Loss: 1.4476\n",
      "Epoch [11/100], Step [1620/1751], Loss: 1.2643\n",
      "Epoch [11/100], Step [1630/1751], Loss: 1.3300\n",
      "Epoch [11/100], Step [1640/1751], Loss: 1.4324\n",
      "Epoch [11/100], Step [1650/1751], Loss: 1.4060\n",
      "Epoch [11/100], Step [1660/1751], Loss: 1.3302\n",
      "Epoch [11/100], Step [1670/1751], Loss: 1.2437\n",
      "Epoch [11/100], Step [1680/1751], Loss: 1.2771\n",
      "Epoch [11/100], Step [1690/1751], Loss: 1.2347\n",
      "Epoch [11/100], Step [1700/1751], Loss: 1.2072\n",
      "Epoch [11/100], Step [1710/1751], Loss: 1.3523\n",
      "Epoch [11/100], Step [1720/1751], Loss: 1.3854\n",
      "Epoch [11/100], Step [1730/1751], Loss: 1.4366\n",
      "Epoch [11/100], Step [1740/1751], Loss: 1.3090\n",
      "Epoch [11/100], Step [1750/1751], Loss: 1.4588\n",
      "Epoch [11/100], Average Loss: 1.3886, Time: 1636.5108s\n",
      "Epoch [12/100], Step [10/1751], Loss: 1.4884\n",
      "Epoch [12/100], Step [20/1751], Loss: 1.4206\n",
      "Epoch [12/100], Step [30/1751], Loss: 1.4742\n",
      "Epoch [12/100], Step [40/1751], Loss: 1.5389\n",
      "Epoch [12/100], Step [50/1751], Loss: 1.4447\n",
      "Epoch [12/100], Step [60/1751], Loss: 1.2690\n",
      "Epoch [12/100], Step [70/1751], Loss: 1.2906\n",
      "Epoch [12/100], Step [80/1751], Loss: 1.4483\n",
      "Epoch [12/100], Step [90/1751], Loss: 1.4925\n",
      "Epoch [12/100], Step [100/1751], Loss: 1.5305\n",
      "Epoch [12/100], Step [110/1751], Loss: 1.2693\n",
      "Epoch [12/100], Step [120/1751], Loss: 1.5664\n",
      "Epoch [12/100], Step [130/1751], Loss: 1.1521\n",
      "Epoch [12/100], Step [140/1751], Loss: 1.3274\n",
      "Epoch [12/100], Step [150/1751], Loss: 1.3404\n",
      "Epoch [12/100], Step [160/1751], Loss: 1.3135\n",
      "Epoch [12/100], Step [170/1751], Loss: 1.3839\n",
      "Epoch [12/100], Step [180/1751], Loss: 1.3935\n",
      "Epoch [12/100], Step [190/1751], Loss: 1.4318\n",
      "Epoch [12/100], Step [200/1751], Loss: 1.3338\n",
      "Epoch [12/100], Step [210/1751], Loss: 1.4109\n",
      "Epoch [12/100], Step [220/1751], Loss: 1.2319\n",
      "Epoch [12/100], Step [230/1751], Loss: 1.3445\n",
      "Epoch [12/100], Step [240/1751], Loss: 1.3427\n",
      "Epoch [12/100], Step [250/1751], Loss: 1.4804\n",
      "Epoch [12/100], Step [260/1751], Loss: 1.4384\n",
      "Epoch [12/100], Step [270/1751], Loss: 1.3156\n",
      "Epoch [12/100], Step [280/1751], Loss: 1.4875\n",
      "Epoch [12/100], Step [290/1751], Loss: 1.3684\n",
      "Epoch [12/100], Step [300/1751], Loss: 1.2795\n",
      "Epoch [12/100], Step [310/1751], Loss: 1.2088\n",
      "Epoch [12/100], Step [320/1751], Loss: 1.4305\n",
      "Epoch [12/100], Step [330/1751], Loss: 1.4861\n",
      "Epoch [12/100], Step [340/1751], Loss: 1.4052\n",
      "Epoch [12/100], Step [350/1751], Loss: 1.5204\n",
      "Epoch [12/100], Step [360/1751], Loss: 1.4318\n",
      "Epoch [12/100], Step [370/1751], Loss: 1.1785\n",
      "Epoch [12/100], Step [380/1751], Loss: 1.3488\n",
      "Epoch [12/100], Step [390/1751], Loss: 1.3460\n",
      "Epoch [12/100], Step [400/1751], Loss: 1.1832\n",
      "Epoch [12/100], Step [410/1751], Loss: 1.3621\n",
      "Epoch [12/100], Step [420/1751], Loss: 1.2839\n",
      "Epoch [12/100], Step [430/1751], Loss: 1.3048\n",
      "Epoch [12/100], Step [440/1751], Loss: 1.3796\n",
      "Epoch [12/100], Step [450/1751], Loss: 1.2735\n",
      "Epoch [12/100], Step [460/1751], Loss: 1.2885\n",
      "Epoch [12/100], Step [470/1751], Loss: 1.2719\n",
      "Epoch [12/100], Step [480/1751], Loss: 1.3326\n",
      "Epoch [12/100], Step [490/1751], Loss: 1.2924\n",
      "Epoch [12/100], Step [500/1751], Loss: 1.2897\n",
      "Epoch [12/100], Step [510/1751], Loss: 1.3675\n",
      "Epoch [12/100], Step [520/1751], Loss: 1.2915\n",
      "Epoch [12/100], Step [530/1751], Loss: 1.3765\n",
      "Epoch [12/100], Step [540/1751], Loss: 1.2889\n",
      "Epoch [12/100], Step [550/1751], Loss: 1.4367\n",
      "Epoch [12/100], Step [560/1751], Loss: 1.5540\n",
      "Epoch [12/100], Step [570/1751], Loss: 1.3411\n",
      "Epoch [12/100], Step [580/1751], Loss: 1.5523\n",
      "Epoch [12/100], Step [590/1751], Loss: 1.3166\n",
      "Epoch [12/100], Step [600/1751], Loss: 1.4428\n",
      "Epoch [12/100], Step [610/1751], Loss: 1.2582\n",
      "Epoch [12/100], Step [620/1751], Loss: 1.5194\n",
      "Epoch [12/100], Step [630/1751], Loss: 1.4131\n",
      "Epoch [12/100], Step [640/1751], Loss: 1.3450\n",
      "Epoch [12/100], Step [650/1751], Loss: 1.3407\n",
      "Epoch [12/100], Step [660/1751], Loss: 1.4968\n",
      "Epoch [12/100], Step [670/1751], Loss: 1.2730\n",
      "Epoch [12/100], Step [680/1751], Loss: 1.4730\n",
      "Epoch [12/100], Step [690/1751], Loss: 1.3012\n",
      "Epoch [12/100], Step [700/1751], Loss: 1.2354\n",
      "Epoch [12/100], Step [710/1751], Loss: 1.4013\n",
      "Epoch [12/100], Step [720/1751], Loss: 1.2937\n",
      "Epoch [12/100], Step [730/1751], Loss: 1.3867\n",
      "Epoch [12/100], Step [740/1751], Loss: 1.3035\n",
      "Epoch [12/100], Step [750/1751], Loss: 1.2661\n",
      "Epoch [12/100], Step [760/1751], Loss: 1.4254\n",
      "Epoch [12/100], Step [770/1751], Loss: 1.4391\n",
      "Epoch [12/100], Step [780/1751], Loss: 1.3581\n",
      "Epoch [12/100], Step [790/1751], Loss: 1.3363\n",
      "Epoch [12/100], Step [800/1751], Loss: 1.4016\n",
      "Epoch [12/100], Step [810/1751], Loss: 1.4187\n",
      "Epoch [12/100], Step [820/1751], Loss: 1.5716\n",
      "Epoch [12/100], Step [830/1751], Loss: 1.3461\n",
      "Epoch [12/100], Step [840/1751], Loss: 1.2453\n",
      "Epoch [12/100], Step [850/1751], Loss: 1.4062\n",
      "Epoch [12/100], Step [860/1751], Loss: 1.3394\n",
      "Epoch [12/100], Step [870/1751], Loss: 1.3160\n",
      "Epoch [12/100], Step [880/1751], Loss: 1.3865\n",
      "Epoch [12/100], Step [890/1751], Loss: 1.3512\n",
      "Epoch [12/100], Step [900/1751], Loss: 1.2561\n",
      "Epoch [12/100], Step [910/1751], Loss: 1.5201\n",
      "Epoch [12/100], Step [920/1751], Loss: 1.2341\n",
      "Epoch [12/100], Step [930/1751], Loss: 1.3313\n",
      "Epoch [12/100], Step [940/1751], Loss: 1.3049\n",
      "Epoch [12/100], Step [950/1751], Loss: 1.5027\n",
      "Epoch [12/100], Step [960/1751], Loss: 1.4731\n",
      "Epoch [12/100], Step [970/1751], Loss: 1.3804\n",
      "Epoch [12/100], Step [980/1751], Loss: 1.3335\n",
      "Epoch [12/100], Step [990/1751], Loss: 1.2817\n",
      "Epoch [12/100], Step [1000/1751], Loss: 1.3697\n",
      "Epoch [12/100], Step [1010/1751], Loss: 1.4726\n",
      "Epoch [12/100], Step [1020/1751], Loss: 1.3886\n",
      "Epoch [12/100], Step [1030/1751], Loss: 1.3109\n",
      "Epoch [12/100], Step [1040/1751], Loss: 1.3229\n",
      "Epoch [12/100], Step [1050/1751], Loss: 1.3521\n",
      "Epoch [12/100], Step [1060/1751], Loss: 1.4058\n",
      "Epoch [12/100], Step [1070/1751], Loss: 1.3643\n",
      "Epoch [12/100], Step [1080/1751], Loss: 1.2871\n",
      "Epoch [12/100], Step [1090/1751], Loss: 1.4415\n",
      "Epoch [12/100], Step [1100/1751], Loss: 1.4601\n",
      "Epoch [12/100], Step [1110/1751], Loss: 1.2898\n",
      "Epoch [12/100], Step [1120/1751], Loss: 1.2620\n",
      "Epoch [12/100], Step [1130/1751], Loss: 1.5141\n",
      "Epoch [12/100], Step [1140/1751], Loss: 1.3692\n",
      "Epoch [12/100], Step [1150/1751], Loss: 1.3339\n",
      "Epoch [12/100], Step [1160/1751], Loss: 1.4167\n",
      "Epoch [12/100], Step [1170/1751], Loss: 1.3570\n",
      "Epoch [12/100], Step [1180/1751], Loss: 1.4176\n",
      "Epoch [12/100], Step [1190/1751], Loss: 1.4016\n",
      "Epoch [12/100], Step [1200/1751], Loss: 1.3486\n",
      "Epoch [12/100], Step [1210/1751], Loss: 1.3349\n",
      "Epoch [12/100], Step [1220/1751], Loss: 1.3742\n",
      "Epoch [12/100], Step [1230/1751], Loss: 1.3026\n",
      "Epoch [12/100], Step [1240/1751], Loss: 1.2923\n",
      "Epoch [12/100], Step [1250/1751], Loss: 1.2666\n",
      "Epoch [12/100], Step [1260/1751], Loss: 1.3503\n",
      "Epoch [12/100], Step [1270/1751], Loss: 1.3521\n",
      "Epoch [12/100], Step [1280/1751], Loss: 1.2425\n",
      "Epoch [12/100], Step [1290/1751], Loss: 1.2751\n",
      "Epoch [12/100], Step [1300/1751], Loss: 1.4433\n",
      "Epoch [12/100], Step [1310/1751], Loss: 1.3608\n",
      "Epoch [12/100], Step [1320/1751], Loss: 1.2445\n",
      "Epoch [12/100], Step [1330/1751], Loss: 1.4099\n",
      "Epoch [12/100], Step [1340/1751], Loss: 1.2056\n",
      "Epoch [12/100], Step [1350/1751], Loss: 1.5065\n",
      "Epoch [12/100], Step [1360/1751], Loss: 1.2736\n",
      "Epoch [12/100], Step [1370/1751], Loss: 1.4844\n",
      "Epoch [12/100], Step [1380/1751], Loss: 1.2776\n",
      "Epoch [12/100], Step [1390/1751], Loss: 1.2760\n",
      "Epoch [12/100], Step [1400/1751], Loss: 1.4020\n",
      "Epoch [12/100], Step [1410/1751], Loss: 1.2805\n",
      "Epoch [12/100], Step [1420/1751], Loss: 1.2340\n",
      "Epoch [12/100], Step [1430/1751], Loss: 1.3076\n",
      "Epoch [12/100], Step [1440/1751], Loss: 1.1981\n",
      "Epoch [12/100], Step [1450/1751], Loss: 1.3813\n",
      "Epoch [12/100], Step [1460/1751], Loss: 1.4104\n",
      "Epoch [12/100], Step [1470/1751], Loss: 1.1875\n",
      "Epoch [12/100], Step [1480/1751], Loss: 1.4083\n",
      "Epoch [12/100], Step [1490/1751], Loss: 1.3387\n",
      "Epoch [12/100], Step [1500/1751], Loss: 1.4114\n",
      "Epoch [12/100], Step [1510/1751], Loss: 1.5331\n",
      "Epoch [12/100], Step [1520/1751], Loss: 1.6095\n",
      "Epoch [12/100], Step [1530/1751], Loss: 1.5129\n",
      "Epoch [12/100], Step [1540/1751], Loss: 1.3635\n",
      "Epoch [12/100], Step [1550/1751], Loss: 1.4092\n",
      "Epoch [12/100], Step [1560/1751], Loss: 1.4453\n",
      "Epoch [12/100], Step [1570/1751], Loss: 1.2667\n",
      "Epoch [12/100], Step [1580/1751], Loss: 1.4439\n",
      "Epoch [12/100], Step [1590/1751], Loss: 1.3653\n",
      "Epoch [12/100], Step [1600/1751], Loss: 1.3575\n",
      "Epoch [12/100], Step [1610/1751], Loss: 1.3776\n",
      "Epoch [12/100], Step [1620/1751], Loss: 1.2656\n",
      "Epoch [12/100], Step [1630/1751], Loss: 1.3181\n",
      "Epoch [12/100], Step [1640/1751], Loss: 1.4756\n",
      "Epoch [12/100], Step [1650/1751], Loss: 1.3060\n",
      "Epoch [12/100], Step [1660/1751], Loss: 1.4354\n",
      "Epoch [12/100], Step [1670/1751], Loss: 1.2995\n",
      "Epoch [12/100], Step [1680/1751], Loss: 1.4712\n",
      "Epoch [12/100], Step [1690/1751], Loss: 1.5170\n",
      "Epoch [12/100], Step [1700/1751], Loss: 1.4066\n",
      "Epoch [12/100], Step [1710/1751], Loss: 1.4399\n",
      "Epoch [12/100], Step [1720/1751], Loss: 1.4531\n",
      "Epoch [12/100], Step [1730/1751], Loss: 1.3432\n",
      "Epoch [12/100], Step [1740/1751], Loss: 1.3291\n",
      "Epoch [12/100], Step [1750/1751], Loss: 1.3638\n",
      "Epoch [12/100], Average Loss: 1.3689, Time: 1636.7113s\n",
      "Epoch [13/100], Step [10/1751], Loss: 1.4418\n",
      "Epoch [13/100], Step [20/1751], Loss: 1.2329\n",
      "Epoch [13/100], Step [30/1751], Loss: 1.3863\n",
      "Epoch [13/100], Step [40/1751], Loss: 1.3974\n",
      "Epoch [13/100], Step [50/1751], Loss: 1.3306\n",
      "Epoch [13/100], Step [60/1751], Loss: 1.4154\n",
      "Epoch [13/100], Step [70/1751], Loss: 1.3821\n",
      "Epoch [13/100], Step [80/1751], Loss: 1.4435\n",
      "Epoch [13/100], Step [90/1751], Loss: 1.2905\n",
      "Epoch [13/100], Step [100/1751], Loss: 1.3774\n",
      "Epoch [13/100], Step [110/1751], Loss: 1.1590\n",
      "Epoch [13/100], Step [120/1751], Loss: 1.3085\n",
      "Epoch [13/100], Step [130/1751], Loss: 1.2702\n",
      "Epoch [13/100], Step [140/1751], Loss: 1.3579\n",
      "Epoch [13/100], Step [150/1751], Loss: 1.4725\n",
      "Epoch [13/100], Step [160/1751], Loss: 1.1709\n",
      "Epoch [13/100], Step [170/1751], Loss: 1.2015\n",
      "Epoch [13/100], Step [180/1751], Loss: 1.3914\n",
      "Epoch [13/100], Step [190/1751], Loss: 1.4995\n",
      "Epoch [13/100], Step [200/1751], Loss: 1.4336\n",
      "Epoch [13/100], Step [210/1751], Loss: 1.3999\n",
      "Epoch [13/100], Step [220/1751], Loss: 1.4015\n",
      "Epoch [13/100], Step [230/1751], Loss: 1.4124\n",
      "Epoch [13/100], Step [240/1751], Loss: 1.3222\n",
      "Epoch [13/100], Step [250/1751], Loss: 1.3080\n",
      "Epoch [13/100], Step [260/1751], Loss: 1.4295\n",
      "Epoch [13/100], Step [270/1751], Loss: 1.4488\n",
      "Epoch [13/100], Step [280/1751], Loss: 1.5504\n",
      "Epoch [13/100], Step [290/1751], Loss: 1.2877\n",
      "Epoch [13/100], Step [300/1751], Loss: 1.5021\n",
      "Epoch [13/100], Step [310/1751], Loss: 1.2965\n",
      "Epoch [13/100], Step [320/1751], Loss: 1.2796\n",
      "Epoch [13/100], Step [330/1751], Loss: 1.3658\n",
      "Epoch [13/100], Step [340/1751], Loss: 1.4507\n",
      "Epoch [13/100], Step [350/1751], Loss: 1.3284\n",
      "Epoch [13/100], Step [360/1751], Loss: 1.3776\n",
      "Epoch [13/100], Step [370/1751], Loss: 1.4942\n",
      "Epoch [13/100], Step [380/1751], Loss: 1.3525\n",
      "Epoch [13/100], Step [390/1751], Loss: 1.1752\n",
      "Epoch [13/100], Step [400/1751], Loss: 1.2482\n",
      "Epoch [13/100], Step [410/1751], Loss: 1.2059\n",
      "Epoch [13/100], Step [420/1751], Loss: 1.3824\n",
      "Epoch [13/100], Step [430/1751], Loss: 1.4063\n",
      "Epoch [13/100], Step [440/1751], Loss: 1.4319\n",
      "Epoch [13/100], Step [450/1751], Loss: 1.3505\n",
      "Epoch [13/100], Step [460/1751], Loss: 1.4538\n",
      "Epoch [13/100], Step [470/1751], Loss: 1.3097\n",
      "Epoch [13/100], Step [480/1751], Loss: 1.2132\n",
      "Epoch [13/100], Step [490/1751], Loss: 1.3581\n",
      "Epoch [13/100], Step [500/1751], Loss: 1.2724\n",
      "Epoch [13/100], Step [510/1751], Loss: 1.3666\n",
      "Epoch [13/100], Step [520/1751], Loss: 1.3879\n",
      "Epoch [13/100], Step [530/1751], Loss: 1.1895\n",
      "Epoch [13/100], Step [540/1751], Loss: 1.3698\n",
      "Epoch [13/100], Step [550/1751], Loss: 1.3482\n",
      "Epoch [13/100], Step [560/1751], Loss: 1.3485\n",
      "Epoch [13/100], Step [570/1751], Loss: 1.2923\n",
      "Epoch [13/100], Step [580/1751], Loss: 1.5523\n",
      "Epoch [13/100], Step [590/1751], Loss: 1.4036\n",
      "Epoch [13/100], Step [600/1751], Loss: 1.3913\n",
      "Epoch [13/100], Step [610/1751], Loss: 1.4584\n",
      "Epoch [13/100], Step [620/1751], Loss: 1.2618\n",
      "Epoch [13/100], Step [630/1751], Loss: 1.1314\n",
      "Epoch [13/100], Step [640/1751], Loss: 1.4002\n",
      "Epoch [13/100], Step [650/1751], Loss: 1.2660\n",
      "Epoch [13/100], Step [660/1751], Loss: 1.2627\n",
      "Epoch [13/100], Step [670/1751], Loss: 1.2719\n",
      "Epoch [13/100], Step [680/1751], Loss: 1.3388\n",
      "Epoch [13/100], Step [690/1751], Loss: 1.3598\n",
      "Epoch [13/100], Step [700/1751], Loss: 1.3050\n",
      "Epoch [13/100], Step [710/1751], Loss: 1.4141\n",
      "Epoch [13/100], Step [720/1751], Loss: 1.4495\n",
      "Epoch [13/100], Step [730/1751], Loss: 1.2982\n",
      "Epoch [13/100], Step [740/1751], Loss: 1.4089\n",
      "Epoch [13/100], Step [750/1751], Loss: 1.4164\n",
      "Epoch [13/100], Step [760/1751], Loss: 1.3259\n",
      "Epoch [13/100], Step [770/1751], Loss: 1.3221\n",
      "Epoch [13/100], Step [780/1751], Loss: 1.5385\n",
      "Epoch [13/100], Step [790/1751], Loss: 1.3091\n",
      "Epoch [13/100], Step [800/1751], Loss: 1.2535\n",
      "Epoch [13/100], Step [810/1751], Loss: 1.4335\n",
      "Epoch [13/100], Step [820/1751], Loss: 1.4726\n",
      "Epoch [13/100], Step [830/1751], Loss: 1.2592\n",
      "Epoch [13/100], Step [840/1751], Loss: 1.5310\n",
      "Epoch [13/100], Step [850/1751], Loss: 1.3224\n",
      "Epoch [13/100], Step [860/1751], Loss: 1.3781\n",
      "Epoch [13/100], Step [870/1751], Loss: 1.1365\n",
      "Epoch [13/100], Step [880/1751], Loss: 1.4116\n",
      "Epoch [13/100], Step [890/1751], Loss: 1.2902\n",
      "Epoch [13/100], Step [900/1751], Loss: 1.4416\n",
      "Epoch [13/100], Step [910/1751], Loss: 1.4258\n",
      "Epoch [13/100], Step [920/1751], Loss: 1.4990\n",
      "Epoch [13/100], Step [930/1751], Loss: 1.4353\n",
      "Epoch [13/100], Step [940/1751], Loss: 1.3463\n",
      "Epoch [13/100], Step [950/1751], Loss: 1.3549\n",
      "Epoch [13/100], Step [960/1751], Loss: 1.4422\n",
      "Epoch [13/100], Step [970/1751], Loss: 1.4124\n",
      "Epoch [13/100], Step [980/1751], Loss: 1.3457\n",
      "Epoch [13/100], Step [990/1751], Loss: 1.4069\n",
      "Epoch [13/100], Step [1000/1751], Loss: 1.3913\n",
      "Epoch [13/100], Step [1010/1751], Loss: 1.3799\n",
      "Epoch [13/100], Step [1020/1751], Loss: 1.3400\n",
      "Epoch [13/100], Step [1030/1751], Loss: 1.4610\n",
      "Epoch [13/100], Step [1040/1751], Loss: 1.3974\n",
      "Epoch [13/100], Step [1050/1751], Loss: 1.3866\n",
      "Epoch [13/100], Step [1060/1751], Loss: 1.2492\n",
      "Epoch [13/100], Step [1070/1751], Loss: 1.4463\n",
      "Epoch [13/100], Step [1080/1751], Loss: 1.3328\n",
      "Epoch [13/100], Step [1090/1751], Loss: 1.1616\n",
      "Epoch [13/100], Step [1100/1751], Loss: 1.3347\n",
      "Epoch [13/100], Step [1110/1751], Loss: 1.3066\n",
      "Epoch [13/100], Step [1120/1751], Loss: 1.1805\n",
      "Epoch [13/100], Step [1130/1751], Loss: 1.5743\n",
      "Epoch [13/100], Step [1140/1751], Loss: 1.3542\n",
      "Epoch [13/100], Step [1150/1751], Loss: 1.3909\n",
      "Epoch [13/100], Step [1160/1751], Loss: 1.2346\n",
      "Epoch [13/100], Step [1170/1751], Loss: 1.3534\n",
      "Epoch [13/100], Step [1180/1751], Loss: 1.2757\n",
      "Epoch [13/100], Step [1190/1751], Loss: 1.2820\n",
      "Epoch [13/100], Step [1200/1751], Loss: 1.2193\n",
      "Epoch [13/100], Step [1210/1751], Loss: 1.3175\n",
      "Epoch [13/100], Step [1220/1751], Loss: 1.2804\n",
      "Epoch [13/100], Step [1230/1751], Loss: 1.2786\n",
      "Epoch [13/100], Step [1240/1751], Loss: 1.5244\n",
      "Epoch [13/100], Step [1250/1751], Loss: 1.3019\n",
      "Epoch [13/100], Step [1260/1751], Loss: 1.4439\n",
      "Epoch [13/100], Step [1270/1751], Loss: 1.3442\n",
      "Epoch [13/100], Step [1280/1751], Loss: 1.3878\n",
      "Epoch [13/100], Step [1290/1751], Loss: 1.3316\n",
      "Epoch [13/100], Step [1300/1751], Loss: 1.2348\n",
      "Epoch [13/100], Step [1310/1751], Loss: 1.2937\n",
      "Epoch [13/100], Step [1320/1751], Loss: 1.2685\n",
      "Epoch [13/100], Step [1330/1751], Loss: 1.4950\n",
      "Epoch [13/100], Step [1340/1751], Loss: 1.2927\n",
      "Epoch [13/100], Step [1350/1751], Loss: 1.3938\n",
      "Epoch [13/100], Step [1360/1751], Loss: 1.3489\n",
      "Epoch [13/100], Step [1370/1751], Loss: 1.2706\n",
      "Epoch [13/100], Step [1380/1751], Loss: 1.3913\n",
      "Epoch [13/100], Step [1390/1751], Loss: 1.3851\n",
      "Epoch [13/100], Step [1400/1751], Loss: 1.2858\n",
      "Epoch [13/100], Step [1410/1751], Loss: 1.3312\n",
      "Epoch [13/100], Step [1420/1751], Loss: 1.2711\n",
      "Epoch [13/100], Step [1430/1751], Loss: 1.3628\n",
      "Epoch [13/100], Step [1440/1751], Loss: 1.5830\n",
      "Epoch [13/100], Step [1450/1751], Loss: 1.1364\n",
      "Epoch [13/100], Step [1460/1751], Loss: 1.3603\n",
      "Epoch [13/100], Step [1470/1751], Loss: 1.3922\n",
      "Epoch [13/100], Step [1480/1751], Loss: 1.3052\n",
      "Epoch [13/100], Step [1490/1751], Loss: 1.3080\n",
      "Epoch [13/100], Step [1500/1751], Loss: 1.3920\n",
      "Epoch [13/100], Step [1510/1751], Loss: 1.3817\n",
      "Epoch [13/100], Step [1520/1751], Loss: 1.3476\n",
      "Epoch [13/100], Step [1530/1751], Loss: 1.3857\n",
      "Epoch [13/100], Step [1540/1751], Loss: 1.2473\n",
      "Epoch [13/100], Step [1550/1751], Loss: 1.3116\n",
      "Epoch [13/100], Step [1560/1751], Loss: 1.3736\n",
      "Epoch [13/100], Step [1570/1751], Loss: 1.3938\n",
      "Epoch [13/100], Step [1580/1751], Loss: 1.2589\n",
      "Epoch [13/100], Step [1590/1751], Loss: 1.2307\n",
      "Epoch [13/100], Step [1600/1751], Loss: 1.2453\n",
      "Epoch [13/100], Step [1610/1751], Loss: 1.2590\n",
      "Epoch [13/100], Step [1620/1751], Loss: 1.4249\n",
      "Epoch [13/100], Step [1630/1751], Loss: 1.2937\n",
      "Epoch [13/100], Step [1640/1751], Loss: 1.2468\n",
      "Epoch [13/100], Step [1650/1751], Loss: 1.5213\n",
      "Epoch [13/100], Step [1660/1751], Loss: 1.4012\n",
      "Epoch [13/100], Step [1670/1751], Loss: 1.2670\n",
      "Epoch [13/100], Step [1680/1751], Loss: 1.4193\n",
      "Epoch [13/100], Step [1690/1751], Loss: 1.2512\n",
      "Epoch [13/100], Step [1700/1751], Loss: 1.3053\n",
      "Epoch [13/100], Step [1710/1751], Loss: 1.2993\n",
      "Epoch [13/100], Step [1720/1751], Loss: 1.3275\n",
      "Epoch [13/100], Step [1730/1751], Loss: 1.2402\n",
      "Epoch [13/100], Step [1740/1751], Loss: 1.4150\n",
      "Epoch [13/100], Step [1750/1751], Loss: 1.4387\n",
      "Epoch [13/100], Average Loss: 1.3519, Time: 1638.1356s\n",
      "Epoch [14/100], Step [10/1751], Loss: 1.4150\n",
      "Epoch [14/100], Step [20/1751], Loss: 1.3513\n",
      "Epoch [14/100], Step [30/1751], Loss: 1.3161\n",
      "Epoch [14/100], Step [40/1751], Loss: 1.4495\n",
      "Epoch [14/100], Step [50/1751], Loss: 1.2725\n",
      "Epoch [14/100], Step [60/1751], Loss: 1.4313\n",
      "Epoch [14/100], Step [70/1751], Loss: 1.3059\n",
      "Epoch [14/100], Step [80/1751], Loss: 1.3965\n",
      "Epoch [14/100], Step [90/1751], Loss: 1.4755\n",
      "Epoch [14/100], Step [100/1751], Loss: 1.2959\n",
      "Epoch [14/100], Step [110/1751], Loss: 1.2544\n",
      "Epoch [14/100], Step [120/1751], Loss: 1.2947\n",
      "Epoch [14/100], Step [130/1751], Loss: 1.3126\n",
      "Epoch [14/100], Step [140/1751], Loss: 1.3705\n",
      "Epoch [14/100], Step [150/1751], Loss: 1.1650\n",
      "Epoch [14/100], Step [160/1751], Loss: 1.3629\n",
      "Epoch [14/100], Step [170/1751], Loss: 1.1743\n",
      "Epoch [14/100], Step [180/1751], Loss: 1.1942\n",
      "Epoch [14/100], Step [190/1751], Loss: 1.1727\n",
      "Epoch [14/100], Step [200/1751], Loss: 1.4458\n",
      "Epoch [14/100], Step [210/1751], Loss: 1.2982\n",
      "Epoch [14/100], Step [220/1751], Loss: 1.3763\n",
      "Epoch [14/100], Step [230/1751], Loss: 1.2216\n",
      "Epoch [14/100], Step [240/1751], Loss: 1.4423\n",
      "Epoch [14/100], Step [250/1751], Loss: 1.4562\n",
      "Epoch [14/100], Step [260/1751], Loss: 1.2997\n",
      "Epoch [14/100], Step [270/1751], Loss: 1.4488\n",
      "Epoch [14/100], Step [280/1751], Loss: 1.3632\n",
      "Epoch [14/100], Step [290/1751], Loss: 1.3660\n",
      "Epoch [14/100], Step [300/1751], Loss: 1.3372\n",
      "Epoch [14/100], Step [310/1751], Loss: 1.3158\n",
      "Epoch [14/100], Step [320/1751], Loss: 1.2289\n",
      "Epoch [14/100], Step [330/1751], Loss: 1.1943\n",
      "Epoch [14/100], Step [340/1751], Loss: 1.3472\n",
      "Epoch [14/100], Step [350/1751], Loss: 1.3171\n",
      "Epoch [14/100], Step [360/1751], Loss: 1.2959\n",
      "Epoch [14/100], Step [370/1751], Loss: 1.4145\n",
      "Epoch [14/100], Step [380/1751], Loss: 1.4918\n",
      "Epoch [14/100], Step [390/1751], Loss: 1.2367\n",
      "Epoch [14/100], Step [400/1751], Loss: 1.3760\n",
      "Epoch [14/100], Step [410/1751], Loss: 1.3761\n",
      "Epoch [14/100], Step [420/1751], Loss: 1.3959\n",
      "Epoch [14/100], Step [430/1751], Loss: 1.4452\n",
      "Epoch [14/100], Step [440/1751], Loss: 1.3117\n",
      "Epoch [14/100], Step [450/1751], Loss: 1.3747\n",
      "Epoch [14/100], Step [460/1751], Loss: 1.4039\n",
      "Epoch [14/100], Step [470/1751], Loss: 1.4480\n",
      "Epoch [14/100], Step [480/1751], Loss: 1.3476\n",
      "Epoch [14/100], Step [490/1751], Loss: 1.2444\n",
      "Epoch [14/100], Step [500/1751], Loss: 1.2319\n",
      "Epoch [14/100], Step [510/1751], Loss: 1.4449\n",
      "Epoch [14/100], Step [520/1751], Loss: 1.3513\n",
      "Epoch [14/100], Step [530/1751], Loss: 1.2993\n",
      "Epoch [14/100], Step [540/1751], Loss: 1.1727\n",
      "Epoch [14/100], Step [550/1751], Loss: 1.4676\n",
      "Epoch [14/100], Step [560/1751], Loss: 1.4735\n",
      "Epoch [14/100], Step [570/1751], Loss: 1.2614\n",
      "Epoch [14/100], Step [580/1751], Loss: 1.2066\n",
      "Epoch [14/100], Step [590/1751], Loss: 1.5412\n",
      "Epoch [14/100], Step [600/1751], Loss: 1.2537\n",
      "Epoch [14/100], Step [610/1751], Loss: 1.5365\n",
      "Epoch [14/100], Step [620/1751], Loss: 1.2704\n",
      "Epoch [14/100], Step [630/1751], Loss: 1.3868\n",
      "Epoch [14/100], Step [640/1751], Loss: 1.3072\n",
      "Epoch [14/100], Step [650/1751], Loss: 1.3680\n",
      "Epoch [14/100], Step [660/1751], Loss: 1.3002\n",
      "Epoch [14/100], Step [670/1751], Loss: 1.4240\n",
      "Epoch [14/100], Step [680/1751], Loss: 1.3862\n",
      "Epoch [14/100], Step [690/1751], Loss: 1.2547\n",
      "Epoch [14/100], Step [700/1751], Loss: 1.3105\n",
      "Epoch [14/100], Step [710/1751], Loss: 1.2585\n",
      "Epoch [14/100], Step [720/1751], Loss: 1.2492\n",
      "Epoch [14/100], Step [730/1751], Loss: 1.3703\n",
      "Epoch [14/100], Step [740/1751], Loss: 1.3148\n",
      "Epoch [14/100], Step [750/1751], Loss: 1.3572\n",
      "Epoch [14/100], Step [760/1751], Loss: 1.2660\n",
      "Epoch [14/100], Step [770/1751], Loss: 1.4763\n",
      "Epoch [14/100], Step [780/1751], Loss: 1.4465\n",
      "Epoch [14/100], Step [790/1751], Loss: 1.3687\n",
      "Epoch [14/100], Step [800/1751], Loss: 1.5464\n",
      "Epoch [14/100], Step [810/1751], Loss: 1.2906\n",
      "Epoch [14/100], Step [820/1751], Loss: 1.4854\n",
      "Epoch [14/100], Step [830/1751], Loss: 1.2396\n",
      "Epoch [14/100], Step [840/1751], Loss: 1.3503\n",
      "Epoch [14/100], Step [850/1751], Loss: 1.3647\n",
      "Epoch [14/100], Step [860/1751], Loss: 1.2705\n",
      "Epoch [14/100], Step [870/1751], Loss: 1.4148\n",
      "Epoch [14/100], Step [880/1751], Loss: 1.3475\n",
      "Epoch [14/100], Step [890/1751], Loss: 1.3551\n",
      "Epoch [14/100], Step [900/1751], Loss: 1.3693\n",
      "Epoch [14/100], Step [910/1751], Loss: 1.1828\n",
      "Epoch [14/100], Step [920/1751], Loss: 1.3020\n",
      "Epoch [14/100], Step [930/1751], Loss: 1.3328\n",
      "Epoch [14/100], Step [940/1751], Loss: 1.4452\n",
      "Epoch [14/100], Step [950/1751], Loss: 1.2960\n",
      "Epoch [14/100], Step [960/1751], Loss: 1.3617\n",
      "Epoch [14/100], Step [970/1751], Loss: 1.2403\n",
      "Epoch [14/100], Step [980/1751], Loss: 1.2449\n",
      "Epoch [14/100], Step [990/1751], Loss: 1.2377\n",
      "Epoch [14/100], Step [1000/1751], Loss: 1.2760\n",
      "Epoch [14/100], Step [1010/1751], Loss: 1.4292\n",
      "Epoch [14/100], Step [1020/1751], Loss: 1.3057\n",
      "Epoch [14/100], Step [1030/1751], Loss: 1.2940\n",
      "Epoch [14/100], Step [1040/1751], Loss: 1.2797\n",
      "Epoch [14/100], Step [1050/1751], Loss: 1.3479\n",
      "Epoch [14/100], Step [1060/1751], Loss: 1.3473\n",
      "Epoch [14/100], Step [1070/1751], Loss: 1.5026\n",
      "Epoch [14/100], Step [1080/1751], Loss: 1.3785\n",
      "Epoch [14/100], Step [1090/1751], Loss: 1.2994\n",
      "Epoch [14/100], Step [1100/1751], Loss: 1.3429\n",
      "Epoch [14/100], Step [1110/1751], Loss: 1.2619\n",
      "Epoch [14/100], Step [1120/1751], Loss: 1.3831\n",
      "Epoch [14/100], Step [1130/1751], Loss: 1.3495\n",
      "Epoch [14/100], Step [1140/1751], Loss: 1.3572\n",
      "Epoch [14/100], Step [1150/1751], Loss: 1.3984\n",
      "Epoch [14/100], Step [1160/1751], Loss: 1.3496\n",
      "Epoch [14/100], Step [1170/1751], Loss: 1.4631\n",
      "Epoch [14/100], Step [1180/1751], Loss: 1.4227\n",
      "Epoch [14/100], Step [1190/1751], Loss: 1.4162\n",
      "Epoch [14/100], Step [1200/1751], Loss: 1.4099\n",
      "Epoch [14/100], Step [1210/1751], Loss: 1.2491\n",
      "Epoch [14/100], Step [1220/1751], Loss: 1.4092\n",
      "Epoch [14/100], Step [1230/1751], Loss: 1.1226\n",
      "Epoch [14/100], Step [1240/1751], Loss: 1.1971\n",
      "Epoch [14/100], Step [1250/1751], Loss: 1.3461\n",
      "Epoch [14/100], Step [1260/1751], Loss: 1.3489\n",
      "Epoch [14/100], Step [1270/1751], Loss: 1.2327\n",
      "Epoch [14/100], Step [1280/1751], Loss: 1.2272\n",
      "Epoch [14/100], Step [1290/1751], Loss: 1.2308\n",
      "Epoch [14/100], Step [1300/1751], Loss: 1.3042\n",
      "Epoch [14/100], Step [1310/1751], Loss: 1.3479\n",
      "Epoch [14/100], Step [1320/1751], Loss: 1.1445\n",
      "Epoch [14/100], Step [1330/1751], Loss: 1.3682\n",
      "Epoch [14/100], Step [1340/1751], Loss: 1.5463\n",
      "Epoch [14/100], Step [1350/1751], Loss: 1.2344\n",
      "Epoch [14/100], Step [1360/1751], Loss: 1.2926\n",
      "Epoch [14/100], Step [1370/1751], Loss: 1.1909\n",
      "Epoch [14/100], Step [1380/1751], Loss: 1.3037\n",
      "Epoch [14/100], Step [1390/1751], Loss: 1.4089\n",
      "Epoch [14/100], Step [1400/1751], Loss: 1.1927\n",
      "Epoch [14/100], Step [1410/1751], Loss: 1.4916\n",
      "Epoch [14/100], Step [1420/1751], Loss: 1.3677\n",
      "Epoch [14/100], Step [1430/1751], Loss: 1.2937\n",
      "Epoch [14/100], Step [1440/1751], Loss: 1.4299\n",
      "Epoch [14/100], Step [1450/1751], Loss: 1.2600\n",
      "Epoch [14/100], Step [1460/1751], Loss: 1.1445\n",
      "Epoch [14/100], Step [1470/1751], Loss: 1.2757\n",
      "Epoch [14/100], Step [1480/1751], Loss: 1.3960\n",
      "Epoch [14/100], Step [1490/1751], Loss: 1.2064\n",
      "Epoch [14/100], Step [1500/1751], Loss: 1.3055\n",
      "Epoch [14/100], Step [1510/1751], Loss: 1.4439\n",
      "Epoch [14/100], Step [1520/1751], Loss: 1.2288\n",
      "Epoch [14/100], Step [1530/1751], Loss: 1.2989\n",
      "Epoch [14/100], Step [1540/1751], Loss: 1.2612\n",
      "Epoch [14/100], Step [1550/1751], Loss: 1.2581\n",
      "Epoch [14/100], Step [1560/1751], Loss: 1.1917\n",
      "Epoch [14/100], Step [1570/1751], Loss: 1.2122\n",
      "Epoch [14/100], Step [1580/1751], Loss: 1.1975\n",
      "Epoch [14/100], Step [1590/1751], Loss: 1.3548\n",
      "Epoch [14/100], Step [1600/1751], Loss: 1.3001\n",
      "Epoch [14/100], Step [1610/1751], Loss: 1.1791\n",
      "Epoch [14/100], Step [1620/1751], Loss: 1.3634\n",
      "Epoch [14/100], Step [1630/1751], Loss: 1.2581\n",
      "Epoch [14/100], Step [1640/1751], Loss: 1.4903\n",
      "Epoch [14/100], Step [1650/1751], Loss: 1.3414\n",
      "Epoch [14/100], Step [1660/1751], Loss: 1.2809\n",
      "Epoch [14/100], Step [1670/1751], Loss: 1.2597\n",
      "Epoch [14/100], Step [1680/1751], Loss: 1.3715\n",
      "Epoch [14/100], Step [1690/1751], Loss: 1.4786\n",
      "Epoch [14/100], Step [1700/1751], Loss: 1.2737\n",
      "Epoch [14/100], Step [1710/1751], Loss: 1.2900\n",
      "Epoch [14/100], Step [1720/1751], Loss: 1.3350\n",
      "Epoch [14/100], Step [1730/1751], Loss: 1.3391\n",
      "Epoch [14/100], Step [1740/1751], Loss: 1.4855\n",
      "Epoch [14/100], Step [1750/1751], Loss: 1.2775\n",
      "Epoch [14/100], Average Loss: 1.3370, Time: 1637.9244s\n",
      "Epoch [15/100], Step [10/1751], Loss: 1.4533\n",
      "Epoch [15/100], Step [20/1751], Loss: 1.3834\n",
      "Epoch [15/100], Step [30/1751], Loss: 1.3335\n",
      "Epoch [15/100], Step [40/1751], Loss: 1.3095\n",
      "Epoch [15/100], Step [50/1751], Loss: 1.3814\n",
      "Epoch [15/100], Step [60/1751], Loss: 1.2395\n",
      "Epoch [15/100], Step [70/1751], Loss: 1.3878\n",
      "Epoch [15/100], Step [80/1751], Loss: 1.3460\n",
      "Epoch [15/100], Step [90/1751], Loss: 1.3343\n",
      "Epoch [15/100], Step [100/1751], Loss: 1.2074\n",
      "Epoch [15/100], Step [110/1751], Loss: 1.3061\n",
      "Epoch [15/100], Step [120/1751], Loss: 1.3034\n",
      "Epoch [15/100], Step [130/1751], Loss: 1.2314\n",
      "Epoch [15/100], Step [140/1751], Loss: 1.3978\n",
      "Epoch [15/100], Step [150/1751], Loss: 1.3033\n",
      "Epoch [15/100], Step [160/1751], Loss: 1.3908\n",
      "Epoch [15/100], Step [170/1751], Loss: 1.2633\n",
      "Epoch [15/100], Step [180/1751], Loss: 1.2590\n",
      "Epoch [15/100], Step [190/1751], Loss: 1.4389\n",
      "Epoch [15/100], Step [200/1751], Loss: 1.3345\n",
      "Epoch [15/100], Step [210/1751], Loss: 1.4814\n",
      "Epoch [15/100], Step [220/1751], Loss: 1.4376\n",
      "Epoch [15/100], Step [230/1751], Loss: 1.2809\n",
      "Epoch [15/100], Step [240/1751], Loss: 1.2433\n",
      "Epoch [15/100], Step [250/1751], Loss: 1.2512\n",
      "Epoch [15/100], Step [260/1751], Loss: 1.4118\n",
      "Epoch [15/100], Step [270/1751], Loss: 1.4111\n",
      "Epoch [15/100], Step [280/1751], Loss: 1.2554\n",
      "Epoch [15/100], Step [290/1751], Loss: 1.4451\n",
      "Epoch [15/100], Step [300/1751], Loss: 1.3098\n",
      "Epoch [15/100], Step [310/1751], Loss: 1.3772\n",
      "Epoch [15/100], Step [320/1751], Loss: 1.2935\n",
      "Epoch [15/100], Step [330/1751], Loss: 1.3647\n",
      "Epoch [15/100], Step [340/1751], Loss: 1.4247\n",
      "Epoch [15/100], Step [350/1751], Loss: 1.4587\n",
      "Epoch [15/100], Step [360/1751], Loss: 1.2848\n",
      "Epoch [15/100], Step [370/1751], Loss: 1.2086\n",
      "Epoch [15/100], Step [380/1751], Loss: 1.3143\n",
      "Epoch [15/100], Step [390/1751], Loss: 1.2037\n",
      "Epoch [15/100], Step [400/1751], Loss: 1.4524\n",
      "Epoch [15/100], Step [410/1751], Loss: 1.2095\n",
      "Epoch [15/100], Step [420/1751], Loss: 1.3478\n",
      "Epoch [15/100], Step [430/1751], Loss: 1.4884\n",
      "Epoch [15/100], Step [440/1751], Loss: 1.3059\n",
      "Epoch [15/100], Step [450/1751], Loss: 1.3725\n",
      "Epoch [15/100], Step [460/1751], Loss: 1.1906\n",
      "Epoch [15/100], Step [470/1751], Loss: 1.2874\n",
      "Epoch [15/100], Step [480/1751], Loss: 1.2851\n",
      "Epoch [15/100], Step [490/1751], Loss: 1.3417\n",
      "Epoch [15/100], Step [500/1751], Loss: 1.1174\n",
      "Epoch [15/100], Step [510/1751], Loss: 1.2742\n",
      "Epoch [15/100], Step [520/1751], Loss: 1.2381\n",
      "Epoch [15/100], Step [530/1751], Loss: 1.3126\n",
      "Epoch [15/100], Step [540/1751], Loss: 1.4040\n",
      "Epoch [15/100], Step [550/1751], Loss: 1.3637\n",
      "Epoch [15/100], Step [560/1751], Loss: 1.3054\n",
      "Epoch [15/100], Step [570/1751], Loss: 1.4654\n",
      "Epoch [15/100], Step [580/1751], Loss: 1.3994\n",
      "Epoch [15/100], Step [590/1751], Loss: 1.2403\n",
      "Epoch [15/100], Step [600/1751], Loss: 1.4008\n",
      "Epoch [15/100], Step [610/1751], Loss: 1.1386\n",
      "Epoch [15/100], Step [620/1751], Loss: 1.2905\n",
      "Epoch [15/100], Step [630/1751], Loss: 1.3147\n",
      "Epoch [15/100], Step [640/1751], Loss: 1.4550\n",
      "Epoch [15/100], Step [650/1751], Loss: 1.4034\n",
      "Epoch [15/100], Step [660/1751], Loss: 1.3536\n",
      "Epoch [15/100], Step [670/1751], Loss: 1.3945\n",
      "Epoch [15/100], Step [680/1751], Loss: 1.3605\n",
      "Epoch [15/100], Step [690/1751], Loss: 1.2507\n",
      "Epoch [15/100], Step [700/1751], Loss: 1.4286\n",
      "Epoch [15/100], Step [710/1751], Loss: 1.3853\n",
      "Epoch [15/100], Step [720/1751], Loss: 1.1492\n",
      "Epoch [15/100], Step [730/1751], Loss: 1.1964\n",
      "Epoch [15/100], Step [740/1751], Loss: 1.3108\n",
      "Epoch [15/100], Step [750/1751], Loss: 1.2367\n",
      "Epoch [15/100], Step [760/1751], Loss: 1.3275\n",
      "Epoch [15/100], Step [770/1751], Loss: 1.2474\n",
      "Epoch [15/100], Step [780/1751], Loss: 1.4799\n",
      "Epoch [15/100], Step [790/1751], Loss: 1.3727\n",
      "Epoch [15/100], Step [800/1751], Loss: 1.3177\n",
      "Epoch [15/100], Step [810/1751], Loss: 1.4169\n",
      "Epoch [15/100], Step [820/1751], Loss: 1.4390\n",
      "Epoch [15/100], Step [830/1751], Loss: 1.3339\n",
      "Epoch [15/100], Step [840/1751], Loss: 1.3646\n",
      "Epoch [15/100], Step [850/1751], Loss: 1.3355\n",
      "Epoch [15/100], Step [860/1751], Loss: 1.3169\n",
      "Epoch [15/100], Step [870/1751], Loss: 1.3276\n",
      "Epoch [15/100], Step [880/1751], Loss: 1.2259\n",
      "Epoch [15/100], Step [890/1751], Loss: 1.2502\n",
      "Epoch [15/100], Step [900/1751], Loss: 1.2179\n",
      "Epoch [15/100], Step [910/1751], Loss: 1.2771\n",
      "Epoch [15/100], Step [920/1751], Loss: 1.3162\n",
      "Epoch [15/100], Step [930/1751], Loss: 1.3765\n",
      "Epoch [15/100], Step [940/1751], Loss: 1.3099\n",
      "Epoch [15/100], Step [950/1751], Loss: 1.2841\n",
      "Epoch [15/100], Step [960/1751], Loss: 1.3617\n",
      "Epoch [15/100], Step [970/1751], Loss: 1.3640\n",
      "Epoch [15/100], Step [980/1751], Loss: 1.2686\n",
      "Epoch [15/100], Step [990/1751], Loss: 1.4424\n",
      "Epoch [15/100], Step [1000/1751], Loss: 1.4470\n",
      "Epoch [15/100], Step [1010/1751], Loss: 1.2731\n",
      "Epoch [15/100], Step [1020/1751], Loss: 1.3561\n",
      "Epoch [15/100], Step [1030/1751], Loss: 1.4308\n",
      "Epoch [15/100], Step [1040/1751], Loss: 1.2328\n",
      "Epoch [15/100], Step [1050/1751], Loss: 1.3170\n",
      "Epoch [15/100], Step [1060/1751], Loss: 1.3373\n",
      "Epoch [15/100], Step [1070/1751], Loss: 1.2523\n",
      "Epoch [15/100], Step [1080/1751], Loss: 1.4272\n",
      "Epoch [15/100], Step [1090/1751], Loss: 1.3236\n",
      "Epoch [15/100], Step [1100/1751], Loss: 1.3396\n",
      "Epoch [15/100], Step [1110/1751], Loss: 1.2705\n",
      "Epoch [15/100], Step [1120/1751], Loss: 1.3191\n",
      "Epoch [15/100], Step [1130/1751], Loss: 1.2428\n",
      "Epoch [15/100], Step [1140/1751], Loss: 1.2574\n",
      "Epoch [15/100], Step [1150/1751], Loss: 1.5136\n",
      "Epoch [15/100], Step [1160/1751], Loss: 1.5167\n",
      "Epoch [15/100], Step [1170/1751], Loss: 1.2693\n",
      "Epoch [15/100], Step [1180/1751], Loss: 1.4552\n",
      "Epoch [15/100], Step [1190/1751], Loss: 1.3434\n",
      "Epoch [15/100], Step [1200/1751], Loss: 1.3542\n",
      "Epoch [15/100], Step [1210/1751], Loss: 1.1258\n",
      "Epoch [15/100], Step [1220/1751], Loss: 1.2539\n",
      "Epoch [15/100], Step [1230/1751], Loss: 1.4665\n",
      "Epoch [15/100], Step [1240/1751], Loss: 1.2898\n",
      "Epoch [15/100], Step [1250/1751], Loss: 1.3971\n",
      "Epoch [15/100], Step [1260/1751], Loss: 1.2067\n",
      "Epoch [15/100], Step [1270/1751], Loss: 1.3568\n",
      "Epoch [15/100], Step [1280/1751], Loss: 1.3014\n",
      "Epoch [15/100], Step [1290/1751], Loss: 1.3173\n",
      "Epoch [15/100], Step [1300/1751], Loss: 1.3081\n",
      "Epoch [15/100], Step [1310/1751], Loss: 1.3143\n",
      "Epoch [15/100], Step [1320/1751], Loss: 1.2890\n",
      "Epoch [15/100], Step [1330/1751], Loss: 1.4815\n",
      "Epoch [15/100], Step [1340/1751], Loss: 1.2119\n",
      "Epoch [15/100], Step [1350/1751], Loss: 1.1338\n",
      "Epoch [15/100], Step [1360/1751], Loss: 1.5349\n",
      "Epoch [15/100], Step [1370/1751], Loss: 1.2404\n",
      "Epoch [15/100], Step [1380/1751], Loss: 1.4440\n",
      "Epoch [15/100], Step [1390/1751], Loss: 1.2895\n",
      "Epoch [15/100], Step [1400/1751], Loss: 1.2993\n",
      "Epoch [15/100], Step [1410/1751], Loss: 1.3189\n",
      "Epoch [15/100], Step [1420/1751], Loss: 1.3224\n",
      "Epoch [15/100], Step [1430/1751], Loss: 1.3436\n",
      "Epoch [15/100], Step [1440/1751], Loss: 1.3649\n",
      "Epoch [15/100], Step [1450/1751], Loss: 1.4315\n",
      "Epoch [15/100], Step [1460/1751], Loss: 1.2470\n",
      "Epoch [15/100], Step [1470/1751], Loss: 1.3130\n",
      "Epoch [15/100], Step [1480/1751], Loss: 1.3436\n",
      "Epoch [15/100], Step [1490/1751], Loss: 1.3708\n",
      "Epoch [15/100], Step [1500/1751], Loss: 1.2373\n",
      "Epoch [15/100], Step [1510/1751], Loss: 1.3904\n",
      "Epoch [15/100], Step [1520/1751], Loss: 1.2605\n",
      "Epoch [15/100], Step [1530/1751], Loss: 1.4190\n",
      "Epoch [15/100], Step [1540/1751], Loss: 1.3356\n",
      "Epoch [15/100], Step [1550/1751], Loss: 1.2705\n",
      "Epoch [15/100], Step [1560/1751], Loss: 1.3499\n",
      "Epoch [15/100], Step [1570/1751], Loss: 1.2872\n",
      "Epoch [15/100], Step [1580/1751], Loss: 1.1639\n",
      "Epoch [15/100], Step [1590/1751], Loss: 1.2217\n",
      "Epoch [15/100], Step [1600/1751], Loss: 1.3025\n",
      "Epoch [15/100], Step [1610/1751], Loss: 1.3131\n",
      "Epoch [15/100], Step [1620/1751], Loss: 1.4293\n",
      "Epoch [15/100], Step [1630/1751], Loss: 1.2298\n",
      "Epoch [15/100], Step [1640/1751], Loss: 1.2520\n",
      "Epoch [15/100], Step [1650/1751], Loss: 1.2668\n",
      "Epoch [15/100], Step [1660/1751], Loss: 1.1593\n",
      "Epoch [15/100], Step [1670/1751], Loss: 1.3607\n",
      "Epoch [15/100], Step [1680/1751], Loss: 1.2842\n",
      "Epoch [15/100], Step [1690/1751], Loss: 1.4040\n",
      "Epoch [15/100], Step [1700/1751], Loss: 1.1493\n",
      "Epoch [15/100], Step [1710/1751], Loss: 1.3095\n",
      "Epoch [15/100], Step [1720/1751], Loss: 1.2349\n",
      "Epoch [15/100], Step [1730/1751], Loss: 1.1620\n",
      "Epoch [15/100], Step [1740/1751], Loss: 1.2399\n",
      "Epoch [15/100], Step [1750/1751], Loss: 1.3237\n",
      "Epoch [15/100], Average Loss: 1.3250, Time: 1637.9240s\n",
      "Epoch [16/100], Step [10/1751], Loss: 1.2327\n",
      "Epoch [16/100], Step [20/1751], Loss: 1.3469\n",
      "Epoch [16/100], Step [30/1751], Loss: 1.3373\n",
      "Epoch [16/100], Step [40/1751], Loss: 1.3813\n",
      "Epoch [16/100], Step [50/1751], Loss: 1.3523\n",
      "Epoch [16/100], Step [60/1751], Loss: 1.3266\n",
      "Epoch [16/100], Step [70/1751], Loss: 1.3831\n",
      "Epoch [16/100], Step [80/1751], Loss: 1.4193\n",
      "Epoch [16/100], Step [90/1751], Loss: 1.2212\n",
      "Epoch [16/100], Step [100/1751], Loss: 1.3095\n",
      "Epoch [16/100], Step [110/1751], Loss: 1.2018\n",
      "Epoch [16/100], Step [120/1751], Loss: 1.4869\n",
      "Epoch [16/100], Step [130/1751], Loss: 1.3151\n",
      "Epoch [16/100], Step [140/1751], Loss: 1.4015\n",
      "Epoch [16/100], Step [150/1751], Loss: 1.2888\n",
      "Epoch [16/100], Step [160/1751], Loss: 1.3904\n",
      "Epoch [16/100], Step [170/1751], Loss: 1.2999\n",
      "Epoch [16/100], Step [180/1751], Loss: 1.2160\n",
      "Epoch [16/100], Step [190/1751], Loss: 1.2721\n",
      "Epoch [16/100], Step [200/1751], Loss: 1.3358\n",
      "Epoch [16/100], Step [210/1751], Loss: 1.2923\n",
      "Epoch [16/100], Step [220/1751], Loss: 1.2564\n",
      "Epoch [16/100], Step [230/1751], Loss: 1.3330\n",
      "Epoch [16/100], Step [240/1751], Loss: 1.0960\n",
      "Epoch [16/100], Step [250/1751], Loss: 1.4844\n",
      "Epoch [16/100], Step [260/1751], Loss: 1.2378\n",
      "Epoch [16/100], Step [270/1751], Loss: 1.2811\n",
      "Epoch [16/100], Step [280/1751], Loss: 1.3877\n",
      "Epoch [16/100], Step [290/1751], Loss: 1.1944\n",
      "Epoch [16/100], Step [300/1751], Loss: 1.1528\n",
      "Epoch [16/100], Step [310/1751], Loss: 1.2223\n",
      "Epoch [16/100], Step [320/1751], Loss: 1.4231\n",
      "Epoch [16/100], Step [330/1751], Loss: 1.2555\n",
      "Epoch [16/100], Step [340/1751], Loss: 1.2948\n",
      "Epoch [16/100], Step [350/1751], Loss: 1.4516\n",
      "Epoch [16/100], Step [360/1751], Loss: 1.2519\n",
      "Epoch [16/100], Step [370/1751], Loss: 1.3271\n",
      "Epoch [16/100], Step [380/1751], Loss: 1.2270\n",
      "Epoch [16/100], Step [390/1751], Loss: 1.2743\n",
      "Epoch [16/100], Step [400/1751], Loss: 1.3203\n",
      "Epoch [16/100], Step [410/1751], Loss: 1.3668\n",
      "Epoch [16/100], Step [420/1751], Loss: 1.2581\n",
      "Epoch [16/100], Step [430/1751], Loss: 1.2944\n",
      "Epoch [16/100], Step [440/1751], Loss: 1.2250\n",
      "Epoch [16/100], Step [450/1751], Loss: 1.2932\n",
      "Epoch [16/100], Step [460/1751], Loss: 1.1931\n",
      "Epoch [16/100], Step [470/1751], Loss: 1.4140\n",
      "Epoch [16/100], Step [480/1751], Loss: 1.2251\n",
      "Epoch [16/100], Step [490/1751], Loss: 1.2088\n",
      "Epoch [16/100], Step [500/1751], Loss: 1.2673\n",
      "Epoch [16/100], Step [510/1751], Loss: 1.2268\n",
      "Epoch [16/100], Step [520/1751], Loss: 1.3537\n",
      "Epoch [16/100], Step [530/1751], Loss: 1.1942\n",
      "Epoch [16/100], Step [540/1751], Loss: 1.2729\n",
      "Epoch [16/100], Step [550/1751], Loss: 1.2789\n",
      "Epoch [16/100], Step [560/1751], Loss: 1.2127\n",
      "Epoch [16/100], Step [570/1751], Loss: 1.2506\n",
      "Epoch [16/100], Step [580/1751], Loss: 1.5165\n",
      "Epoch [16/100], Step [590/1751], Loss: 1.1699\n",
      "Epoch [16/100], Step [600/1751], Loss: 1.2077\n",
      "Epoch [16/100], Step [610/1751], Loss: 1.2635\n",
      "Epoch [16/100], Step [620/1751], Loss: 1.4234\n",
      "Epoch [16/100], Step [630/1751], Loss: 1.3404\n",
      "Epoch [16/100], Step [640/1751], Loss: 1.3481\n",
      "Epoch [16/100], Step [650/1751], Loss: 1.2878\n",
      "Epoch [16/100], Step [660/1751], Loss: 1.2247\n",
      "Epoch [16/100], Step [670/1751], Loss: 1.4315\n",
      "Epoch [16/100], Step [680/1751], Loss: 1.3828\n",
      "Epoch [16/100], Step [690/1751], Loss: 1.2937\n",
      "Epoch [16/100], Step [700/1751], Loss: 1.3063\n",
      "Epoch [16/100], Step [710/1751], Loss: 1.3055\n",
      "Epoch [16/100], Step [720/1751], Loss: 1.2938\n",
      "Epoch [16/100], Step [730/1751], Loss: 1.3082\n",
      "Epoch [16/100], Step [740/1751], Loss: 1.3099\n",
      "Epoch [16/100], Step [750/1751], Loss: 1.2920\n",
      "Epoch [16/100], Step [760/1751], Loss: 1.4187\n",
      "Epoch [16/100], Step [770/1751], Loss: 1.3218\n",
      "Epoch [16/100], Step [780/1751], Loss: 1.3021\n",
      "Epoch [16/100], Step [790/1751], Loss: 1.2763\n",
      "Epoch [16/100], Step [800/1751], Loss: 1.3238\n",
      "Epoch [16/100], Step [810/1751], Loss: 1.2562\n",
      "Epoch [16/100], Step [820/1751], Loss: 1.2597\n",
      "Epoch [16/100], Step [830/1751], Loss: 1.3266\n",
      "Epoch [16/100], Step [840/1751], Loss: 1.3294\n",
      "Epoch [16/100], Step [850/1751], Loss: 1.4314\n",
      "Epoch [16/100], Step [860/1751], Loss: 1.1612\n",
      "Epoch [16/100], Step [870/1751], Loss: 1.2815\n",
      "Epoch [16/100], Step [880/1751], Loss: 1.2038\n",
      "Epoch [16/100], Step [890/1751], Loss: 1.2754\n",
      "Epoch [16/100], Step [900/1751], Loss: 1.3744\n",
      "Epoch [16/100], Step [910/1751], Loss: 1.3249\n",
      "Epoch [16/100], Step [920/1751], Loss: 1.4312\n",
      "Epoch [16/100], Step [930/1751], Loss: 1.3323\n",
      "Epoch [16/100], Step [940/1751], Loss: 1.2333\n",
      "Epoch [16/100], Step [950/1751], Loss: 1.3017\n",
      "Epoch [16/100], Step [960/1751], Loss: 1.4053\n",
      "Epoch [16/100], Step [970/1751], Loss: 1.2612\n",
      "Epoch [16/100], Step [980/1751], Loss: 1.4036\n",
      "Epoch [16/100], Step [990/1751], Loss: 1.5762\n",
      "Epoch [16/100], Step [1000/1751], Loss: 1.2243\n",
      "Epoch [16/100], Step [1010/1751], Loss: 1.3691\n",
      "Epoch [16/100], Step [1020/1751], Loss: 1.2542\n",
      "Epoch [16/100], Step [1030/1751], Loss: 1.3079\n",
      "Epoch [16/100], Step [1040/1751], Loss: 1.3134\n",
      "Epoch [16/100], Step [1050/1751], Loss: 1.4211\n",
      "Epoch [16/100], Step [1060/1751], Loss: 1.3936\n",
      "Epoch [16/100], Step [1070/1751], Loss: 1.2018\n",
      "Epoch [16/100], Step [1080/1751], Loss: 1.2993\n",
      "Epoch [16/100], Step [1090/1751], Loss: 1.2237\n",
      "Epoch [16/100], Step [1100/1751], Loss: 1.3611\n",
      "Epoch [16/100], Step [1110/1751], Loss: 1.3641\n",
      "Epoch [16/100], Step [1120/1751], Loss: 1.3315\n",
      "Epoch [16/100], Step [1130/1751], Loss: 1.2027\n",
      "Epoch [16/100], Step [1140/1751], Loss: 1.2609\n",
      "Epoch [16/100], Step [1150/1751], Loss: 1.1674\n",
      "Epoch [16/100], Step [1160/1751], Loss: 1.4360\n",
      "Epoch [16/100], Step [1170/1751], Loss: 1.2049\n",
      "Epoch [16/100], Step [1180/1751], Loss: 1.3753\n",
      "Epoch [16/100], Step [1190/1751], Loss: 1.2220\n",
      "Epoch [16/100], Step [1200/1751], Loss: 1.2340\n",
      "Epoch [16/100], Step [1210/1751], Loss: 1.1955\n",
      "Epoch [16/100], Step [1220/1751], Loss: 1.3462\n",
      "Epoch [16/100], Step [1230/1751], Loss: 1.3524\n",
      "Epoch [16/100], Step [1240/1751], Loss: 1.4408\n",
      "Epoch [16/100], Step [1250/1751], Loss: 1.3161\n",
      "Epoch [16/100], Step [1260/1751], Loss: 1.1811\n",
      "Epoch [16/100], Step [1270/1751], Loss: 1.5680\n",
      "Epoch [16/100], Step [1280/1751], Loss: 1.4527\n",
      "Epoch [16/100], Step [1290/1751], Loss: 1.2965\n",
      "Epoch [16/100], Step [1300/1751], Loss: 1.2721\n",
      "Epoch [16/100], Step [1310/1751], Loss: 1.2862\n",
      "Epoch [16/100], Step [1320/1751], Loss: 1.2971\n",
      "Epoch [16/100], Step [1330/1751], Loss: 1.2814\n",
      "Epoch [16/100], Step [1340/1751], Loss: 1.4147\n",
      "Epoch [16/100], Step [1350/1751], Loss: 1.2753\n",
      "Epoch [16/100], Step [1360/1751], Loss: 1.3149\n",
      "Epoch [16/100], Step [1370/1751], Loss: 1.3540\n",
      "Epoch [16/100], Step [1380/1751], Loss: 1.4675\n",
      "Epoch [16/100], Step [1390/1751], Loss: 1.2852\n",
      "Epoch [16/100], Step [1400/1751], Loss: 1.3375\n",
      "Epoch [16/100], Step [1410/1751], Loss: 1.3246\n",
      "Epoch [16/100], Step [1420/1751], Loss: 1.3861\n",
      "Epoch [16/100], Step [1430/1751], Loss: 1.3931\n",
      "Epoch [16/100], Step [1440/1751], Loss: 1.2833\n",
      "Epoch [16/100], Step [1450/1751], Loss: 1.2403\n",
      "Epoch [16/100], Step [1460/1751], Loss: 1.2427\n",
      "Epoch [16/100], Step [1470/1751], Loss: 1.2948\n",
      "Epoch [16/100], Step [1480/1751], Loss: 1.2853\n",
      "Epoch [16/100], Step [1490/1751], Loss: 1.3242\n",
      "Epoch [16/100], Step [1500/1751], Loss: 1.4495\n",
      "Epoch [16/100], Step [1510/1751], Loss: 1.2707\n",
      "Epoch [16/100], Step [1520/1751], Loss: 1.3642\n",
      "Epoch [16/100], Step [1530/1751], Loss: 1.1964\n",
      "Epoch [16/100], Step [1540/1751], Loss: 1.2982\n",
      "Epoch [16/100], Step [1550/1751], Loss: 1.3052\n",
      "Epoch [16/100], Step [1560/1751], Loss: 1.3125\n",
      "Epoch [16/100], Step [1570/1751], Loss: 1.3073\n",
      "Epoch [16/100], Step [1580/1751], Loss: 1.2986\n",
      "Epoch [16/100], Step [1590/1751], Loss: 1.2558\n",
      "Epoch [16/100], Step [1600/1751], Loss: 1.1764\n",
      "Epoch [16/100], Step [1610/1751], Loss: 1.3334\n",
      "Epoch [16/100], Step [1620/1751], Loss: 1.3228\n",
      "Epoch [16/100], Step [1630/1751], Loss: 1.3258\n",
      "Epoch [16/100], Step [1640/1751], Loss: 1.1606\n",
      "Epoch [16/100], Step [1650/1751], Loss: 1.3893\n",
      "Epoch [16/100], Step [1660/1751], Loss: 1.4002\n",
      "Epoch [16/100], Step [1670/1751], Loss: 1.2123\n",
      "Epoch [16/100], Step [1680/1751], Loss: 1.1995\n",
      "Epoch [16/100], Step [1690/1751], Loss: 1.3852\n",
      "Epoch [16/100], Step [1700/1751], Loss: 1.3714\n",
      "Epoch [16/100], Step [1710/1751], Loss: 1.2759\n",
      "Epoch [16/100], Step [1720/1751], Loss: 1.3816\n",
      "Epoch [16/100], Step [1730/1751], Loss: 1.2242\n",
      "Epoch [16/100], Step [1740/1751], Loss: 1.2735\n",
      "Epoch [16/100], Step [1750/1751], Loss: 1.3237\n",
      "Epoch [16/100], Average Loss: 1.3140, Time: 1637.4860s\n",
      "Epoch [17/100], Step [10/1751], Loss: 1.2929\n",
      "Epoch [17/100], Step [20/1751], Loss: 1.3336\n",
      "Epoch [17/100], Step [30/1751], Loss: 1.2589\n",
      "Epoch [17/100], Step [40/1751], Loss: 1.4008\n",
      "Epoch [17/100], Step [50/1751], Loss: 1.3719\n",
      "Epoch [17/100], Step [60/1751], Loss: 1.1110\n",
      "Epoch [17/100], Step [70/1751], Loss: 1.2099\n",
      "Epoch [17/100], Step [80/1751], Loss: 1.1124\n",
      "Epoch [17/100], Step [90/1751], Loss: 1.3633\n",
      "Epoch [17/100], Step [100/1751], Loss: 1.3459\n",
      "Epoch [17/100], Step [110/1751], Loss: 1.3967\n",
      "Epoch [17/100], Step [120/1751], Loss: 1.3215\n",
      "Epoch [17/100], Step [130/1751], Loss: 1.4148\n",
      "Epoch [17/100], Step [140/1751], Loss: 1.4435\n",
      "Epoch [17/100], Step [150/1751], Loss: 1.2977\n",
      "Epoch [17/100], Step [160/1751], Loss: 1.3850\n",
      "Epoch [17/100], Step [170/1751], Loss: 1.3191\n",
      "Epoch [17/100], Step [180/1751], Loss: 1.3058\n",
      "Epoch [17/100], Step [190/1751], Loss: 1.1503\n",
      "Epoch [17/100], Step [200/1751], Loss: 1.3363\n",
      "Epoch [17/100], Step [210/1751], Loss: 1.1610\n",
      "Epoch [17/100], Step [220/1751], Loss: 1.3584\n",
      "Epoch [17/100], Step [230/1751], Loss: 1.2642\n",
      "Epoch [17/100], Step [240/1751], Loss: 1.2201\n",
      "Epoch [17/100], Step [250/1751], Loss: 1.1993\n",
      "Epoch [17/100], Step [260/1751], Loss: 1.2991\n",
      "Epoch [17/100], Step [270/1751], Loss: 1.2673\n",
      "Epoch [17/100], Step [280/1751], Loss: 1.1545\n",
      "Epoch [17/100], Step [290/1751], Loss: 1.3665\n",
      "Epoch [17/100], Step [300/1751], Loss: 1.3405\n",
      "Epoch [17/100], Step [310/1751], Loss: 1.1539\n",
      "Epoch [17/100], Step [320/1751], Loss: 1.2642\n",
      "Epoch [17/100], Step [330/1751], Loss: 1.4700\n",
      "Epoch [17/100], Step [340/1751], Loss: 1.3933\n",
      "Epoch [17/100], Step [350/1751], Loss: 1.1560\n",
      "Epoch [17/100], Step [360/1751], Loss: 1.3342\n",
      "Epoch [17/100], Step [370/1751], Loss: 1.2171\n",
      "Epoch [17/100], Step [380/1751], Loss: 1.2752\n",
      "Epoch [17/100], Step [390/1751], Loss: 1.4380\n",
      "Epoch [17/100], Step [400/1751], Loss: 1.2776\n",
      "Epoch [17/100], Step [410/1751], Loss: 1.2558\n",
      "Epoch [17/100], Step [420/1751], Loss: 1.1733\n",
      "Epoch [17/100], Step [430/1751], Loss: 1.3617\n",
      "Epoch [17/100], Step [440/1751], Loss: 1.1851\n",
      "Epoch [17/100], Step [450/1751], Loss: 1.2690\n",
      "Epoch [17/100], Step [460/1751], Loss: 1.3039\n",
      "Epoch [17/100], Step [470/1751], Loss: 1.2683\n",
      "Epoch [17/100], Step [480/1751], Loss: 1.2757\n",
      "Epoch [17/100], Step [490/1751], Loss: 1.3250\n",
      "Epoch [17/100], Step [500/1751], Loss: 1.3444\n",
      "Epoch [17/100], Step [510/1751], Loss: 1.2406\n",
      "Epoch [17/100], Step [520/1751], Loss: 1.3164\n",
      "Epoch [17/100], Step [530/1751], Loss: 1.3439\n",
      "Epoch [17/100], Step [540/1751], Loss: 1.4169\n",
      "Epoch [17/100], Step [550/1751], Loss: 1.3547\n",
      "Epoch [17/100], Step [560/1751], Loss: 1.3566\n",
      "Epoch [17/100], Step [570/1751], Loss: 1.4277\n",
      "Epoch [17/100], Step [580/1751], Loss: 1.2567\n",
      "Epoch [17/100], Step [590/1751], Loss: 1.3324\n",
      "Epoch [17/100], Step [600/1751], Loss: 1.4160\n",
      "Epoch [17/100], Step [610/1751], Loss: 1.3413\n",
      "Epoch [17/100], Step [620/1751], Loss: 1.3210\n",
      "Epoch [17/100], Step [630/1751], Loss: 1.3323\n",
      "Epoch [17/100], Step [640/1751], Loss: 1.3829\n",
      "Epoch [17/100], Step [650/1751], Loss: 1.2811\n",
      "Epoch [17/100], Step [660/1751], Loss: 1.4065\n",
      "Epoch [17/100], Step [670/1751], Loss: 1.2607\n",
      "Epoch [17/100], Step [680/1751], Loss: 1.3660\n",
      "Epoch [17/100], Step [690/1751], Loss: 1.2740\n",
      "Epoch [17/100], Step [700/1751], Loss: 1.2679\n",
      "Epoch [17/100], Step [710/1751], Loss: 1.1029\n",
      "Epoch [17/100], Step [720/1751], Loss: 1.2062\n",
      "Epoch [17/100], Step [730/1751], Loss: 1.4099\n",
      "Epoch [17/100], Step [740/1751], Loss: 1.3419\n",
      "Epoch [17/100], Step [750/1751], Loss: 1.3663\n",
      "Epoch [17/100], Step [760/1751], Loss: 1.3695\n",
      "Epoch [17/100], Step [770/1751], Loss: 1.1857\n",
      "Epoch [17/100], Step [780/1751], Loss: 1.2878\n",
      "Epoch [17/100], Step [790/1751], Loss: 1.4581\n",
      "Epoch [17/100], Step [800/1751], Loss: 1.3440\n",
      "Epoch [17/100], Step [810/1751], Loss: 1.2667\n",
      "Epoch [17/100], Step [820/1751], Loss: 1.2922\n",
      "Epoch [17/100], Step [830/1751], Loss: 1.3944\n",
      "Epoch [17/100], Step [840/1751], Loss: 1.2092\n",
      "Epoch [17/100], Step [850/1751], Loss: 1.3282\n",
      "Epoch [17/100], Step [860/1751], Loss: 1.3604\n",
      "Epoch [17/100], Step [870/1751], Loss: 1.3916\n",
      "Epoch [17/100], Step [880/1751], Loss: 1.4148\n",
      "Epoch [17/100], Step [890/1751], Loss: 1.2978\n",
      "Epoch [17/100], Step [900/1751], Loss: 1.3202\n",
      "Epoch [17/100], Step [910/1751], Loss: 1.2628\n",
      "Epoch [17/100], Step [920/1751], Loss: 1.3809\n",
      "Epoch [17/100], Step [930/1751], Loss: 1.3186\n",
      "Epoch [17/100], Step [940/1751], Loss: 1.2494\n",
      "Epoch [17/100], Step [950/1751], Loss: 1.3653\n",
      "Epoch [17/100], Step [960/1751], Loss: 1.2966\n",
      "Epoch [17/100], Step [970/1751], Loss: 1.3807\n",
      "Epoch [17/100], Step [980/1751], Loss: 1.2680\n",
      "Epoch [17/100], Step [990/1751], Loss: 1.3799\n",
      "Epoch [17/100], Step [1000/1751], Loss: 1.1483\n",
      "Epoch [17/100], Step [1010/1751], Loss: 1.3656\n",
      "Epoch [17/100], Step [1020/1751], Loss: 1.3020\n",
      "Epoch [17/100], Step [1030/1751], Loss: 1.3211\n",
      "Epoch [17/100], Step [1040/1751], Loss: 1.3005\n",
      "Epoch [17/100], Step [1050/1751], Loss: 1.2545\n",
      "Epoch [17/100], Step [1060/1751], Loss: 1.2385\n",
      "Epoch [17/100], Step [1070/1751], Loss: 1.1267\n",
      "Epoch [17/100], Step [1080/1751], Loss: 1.2548\n",
      "Epoch [17/100], Step [1090/1751], Loss: 1.3464\n",
      "Epoch [17/100], Step [1100/1751], Loss: 1.3688\n",
      "Epoch [17/100], Step [1110/1751], Loss: 1.3561\n",
      "Epoch [17/100], Step [1120/1751], Loss: 1.2508\n",
      "Epoch [17/100], Step [1130/1751], Loss: 1.1516\n",
      "Epoch [17/100], Step [1140/1751], Loss: 1.4170\n",
      "Epoch [17/100], Step [1150/1751], Loss: 1.3406\n",
      "Epoch [17/100], Step [1160/1751], Loss: 1.3237\n",
      "Epoch [17/100], Step [1170/1751], Loss: 1.1402\n",
      "Epoch [17/100], Step [1180/1751], Loss: 1.4076\n",
      "Epoch [17/100], Step [1190/1751], Loss: 1.2977\n",
      "Epoch [17/100], Step [1200/1751], Loss: 1.3220\n",
      "Epoch [17/100], Step [1210/1751], Loss: 1.3544\n",
      "Epoch [17/100], Step [1220/1751], Loss: 1.2413\n",
      "Epoch [17/100], Step [1230/1751], Loss: 1.2560\n",
      "Epoch [17/100], Step [1240/1751], Loss: 1.2431\n",
      "Epoch [17/100], Step [1250/1751], Loss: 1.1981\n",
      "Epoch [17/100], Step [1260/1751], Loss: 1.3278\n",
      "Epoch [17/100], Step [1270/1751], Loss: 1.2422\n",
      "Epoch [17/100], Step [1280/1751], Loss: 1.1317\n",
      "Epoch [17/100], Step [1290/1751], Loss: 1.2757\n",
      "Epoch [17/100], Step [1300/1751], Loss: 1.2372\n",
      "Epoch [17/100], Step [1310/1751], Loss: 1.2056\n",
      "Epoch [17/100], Step [1320/1751], Loss: 1.4264\n",
      "Epoch [17/100], Step [1330/1751], Loss: 1.3264\n",
      "Epoch [17/100], Step [1340/1751], Loss: 1.3379\n",
      "Epoch [17/100], Step [1350/1751], Loss: 1.4379\n",
      "Epoch [17/100], Step [1360/1751], Loss: 1.2054\n",
      "Epoch [17/100], Step [1370/1751], Loss: 1.3443\n",
      "Epoch [17/100], Step [1380/1751], Loss: 1.3671\n",
      "Epoch [17/100], Step [1390/1751], Loss: 1.3076\n",
      "Epoch [17/100], Step [1400/1751], Loss: 1.2822\n",
      "Epoch [17/100], Step [1410/1751], Loss: 1.4116\n",
      "Epoch [17/100], Step [1420/1751], Loss: 1.4286\n",
      "Epoch [17/100], Step [1430/1751], Loss: 1.3384\n",
      "Epoch [17/100], Step [1440/1751], Loss: 1.3245\n",
      "Epoch [17/100], Step [1450/1751], Loss: 1.2970\n",
      "Epoch [17/100], Step [1460/1751], Loss: 1.2504\n",
      "Epoch [17/100], Step [1470/1751], Loss: 1.2674\n",
      "Epoch [17/100], Step [1480/1751], Loss: 1.2268\n",
      "Epoch [17/100], Step [1490/1751], Loss: 1.4387\n",
      "Epoch [17/100], Step [1500/1751], Loss: 1.3230\n",
      "Epoch [17/100], Step [1510/1751], Loss: 1.2204\n",
      "Epoch [17/100], Step [1520/1751], Loss: 1.2142\n",
      "Epoch [17/100], Step [1530/1751], Loss: 1.3470\n",
      "Epoch [17/100], Step [1540/1751], Loss: 1.3128\n",
      "Epoch [17/100], Step [1550/1751], Loss: 1.1620\n",
      "Epoch [17/100], Step [1560/1751], Loss: 1.2447\n",
      "Epoch [17/100], Step [1570/1751], Loss: 1.3128\n",
      "Epoch [17/100], Step [1580/1751], Loss: 1.3179\n",
      "Epoch [17/100], Step [1590/1751], Loss: 1.1859\n",
      "Epoch [17/100], Step [1600/1751], Loss: 1.2323\n",
      "Epoch [17/100], Step [1610/1751], Loss: 1.2836\n",
      "Epoch [17/100], Step [1620/1751], Loss: 1.3476\n",
      "Epoch [17/100], Step [1630/1751], Loss: 1.2768\n",
      "Epoch [17/100], Step [1640/1751], Loss: 1.3932\n",
      "Epoch [17/100], Step [1650/1751], Loss: 1.2343\n",
      "Epoch [17/100], Step [1660/1751], Loss: 1.3964\n",
      "Epoch [17/100], Step [1670/1751], Loss: 1.1926\n",
      "Epoch [17/100], Step [1680/1751], Loss: 1.1603\n",
      "Epoch [17/100], Step [1690/1751], Loss: 1.2566\n",
      "Epoch [17/100], Step [1700/1751], Loss: 1.3546\n",
      "Epoch [17/100], Step [1710/1751], Loss: 1.3496\n",
      "Epoch [17/100], Step [1720/1751], Loss: 1.2683\n",
      "Epoch [17/100], Step [1730/1751], Loss: 1.3001\n",
      "Epoch [17/100], Step [1740/1751], Loss: 1.3289\n",
      "Epoch [17/100], Step [1750/1751], Loss: 1.2930\n",
      "Epoch [17/100], Average Loss: 1.3043, Time: 1638.1556s\n",
      "Epoch [18/100], Step [10/1751], Loss: 1.3295\n",
      "Epoch [18/100], Step [20/1751], Loss: 1.2948\n",
      "Epoch [18/100], Step [30/1751], Loss: 1.3151\n",
      "Epoch [18/100], Step [40/1751], Loss: 1.2350\n",
      "Epoch [18/100], Step [50/1751], Loss: 1.2649\n",
      "Epoch [18/100], Step [60/1751], Loss: 1.3133\n",
      "Epoch [18/100], Step [70/1751], Loss: 1.2604\n",
      "Epoch [18/100], Step [80/1751], Loss: 1.3034\n",
      "Epoch [18/100], Step [90/1751], Loss: 1.3581\n",
      "Epoch [18/100], Step [100/1751], Loss: 1.3769\n",
      "Epoch [18/100], Step [110/1751], Loss: 1.2502\n",
      "Epoch [18/100], Step [120/1751], Loss: 1.3201\n",
      "Epoch [18/100], Step [130/1751], Loss: 1.2441\n",
      "Epoch [18/100], Step [140/1751], Loss: 1.2769\n",
      "Epoch [18/100], Step [150/1751], Loss: 1.2890\n",
      "Epoch [18/100], Step [160/1751], Loss: 1.3063\n",
      "Epoch [18/100], Step [170/1751], Loss: 1.1335\n",
      "Epoch [18/100], Step [180/1751], Loss: 1.5279\n",
      "Epoch [18/100], Step [190/1751], Loss: 1.3076\n",
      "Epoch [18/100], Step [200/1751], Loss: 1.2333\n",
      "Epoch [18/100], Step [210/1751], Loss: 1.2479\n",
      "Epoch [18/100], Step [220/1751], Loss: 1.3917\n",
      "Epoch [18/100], Step [230/1751], Loss: 1.2808\n",
      "Epoch [18/100], Step [240/1751], Loss: 1.3371\n",
      "Epoch [18/100], Step [250/1751], Loss: 1.3951\n",
      "Epoch [18/100], Step [260/1751], Loss: 1.4217\n",
      "Epoch [18/100], Step [270/1751], Loss: 1.4336\n",
      "Epoch [18/100], Step [280/1751], Loss: 1.4086\n",
      "Epoch [18/100], Step [290/1751], Loss: 1.1952\n",
      "Epoch [18/100], Step [300/1751], Loss: 1.1052\n",
      "Epoch [18/100], Step [310/1751], Loss: 1.1710\n",
      "Epoch [18/100], Step [320/1751], Loss: 1.2147\n",
      "Epoch [18/100], Step [330/1751], Loss: 1.2889\n",
      "Epoch [18/100], Step [340/1751], Loss: 1.3562\n",
      "Epoch [18/100], Step [350/1751], Loss: 1.1366\n",
      "Epoch [18/100], Step [360/1751], Loss: 1.2983\n",
      "Epoch [18/100], Step [370/1751], Loss: 1.1747\n",
      "Epoch [18/100], Step [380/1751], Loss: 1.1919\n",
      "Epoch [18/100], Step [390/1751], Loss: 1.2336\n",
      "Epoch [18/100], Step [400/1751], Loss: 1.1074\n",
      "Epoch [18/100], Step [410/1751], Loss: 1.2015\n",
      "Epoch [18/100], Step [420/1751], Loss: 1.3174\n",
      "Epoch [18/100], Step [430/1751], Loss: 1.1920\n",
      "Epoch [18/100], Step [440/1751], Loss: 1.3145\n",
      "Epoch [18/100], Step [450/1751], Loss: 1.4277\n",
      "Epoch [18/100], Step [460/1751], Loss: 1.3467\n",
      "Epoch [18/100], Step [470/1751], Loss: 1.3165\n",
      "Epoch [18/100], Step [480/1751], Loss: 1.4456\n",
      "Epoch [18/100], Step [490/1751], Loss: 1.2741\n",
      "Epoch [18/100], Step [500/1751], Loss: 1.4068\n",
      "Epoch [18/100], Step [510/1751], Loss: 1.2932\n",
      "Epoch [18/100], Step [520/1751], Loss: 1.2372\n",
      "Epoch [18/100], Step [530/1751], Loss: 1.3009\n",
      "Epoch [18/100], Step [540/1751], Loss: 1.1221\n",
      "Epoch [18/100], Step [550/1751], Loss: 1.4524\n",
      "Epoch [18/100], Step [560/1751], Loss: 1.2433\n",
      "Epoch [18/100], Step [570/1751], Loss: 1.4466\n",
      "Epoch [18/100], Step [580/1751], Loss: 1.3793\n",
      "Epoch [18/100], Step [590/1751], Loss: 1.2880\n",
      "Epoch [18/100], Step [600/1751], Loss: 1.2973\n",
      "Epoch [18/100], Step [610/1751], Loss: 1.4093\n",
      "Epoch [18/100], Step [620/1751], Loss: 1.2679\n",
      "Epoch [18/100], Step [630/1751], Loss: 1.2612\n",
      "Epoch [18/100], Step [640/1751], Loss: 1.3365\n",
      "Epoch [18/100], Step [650/1751], Loss: 1.2514\n",
      "Epoch [18/100], Step [660/1751], Loss: 1.1950\n",
      "Epoch [18/100], Step [670/1751], Loss: 1.4049\n",
      "Epoch [18/100], Step [680/1751], Loss: 1.2128\n",
      "Epoch [18/100], Step [690/1751], Loss: 1.3620\n",
      "Epoch [18/100], Step [700/1751], Loss: 1.1603\n",
      "Epoch [18/100], Step [710/1751], Loss: 1.2241\n",
      "Epoch [18/100], Step [720/1751], Loss: 1.3446\n",
      "Epoch [18/100], Step [730/1751], Loss: 1.3318\n",
      "Epoch [18/100], Step [740/1751], Loss: 1.3118\n",
      "Epoch [18/100], Step [750/1751], Loss: 1.3365\n",
      "Epoch [18/100], Step [760/1751], Loss: 1.2571\n",
      "Epoch [18/100], Step [770/1751], Loss: 1.3009\n",
      "Epoch [18/100], Step [780/1751], Loss: 1.3361\n",
      "Epoch [18/100], Step [790/1751], Loss: 1.3139\n",
      "Epoch [18/100], Step [800/1751], Loss: 1.3763\n",
      "Epoch [18/100], Step [810/1751], Loss: 1.2034\n",
      "Epoch [18/100], Step [820/1751], Loss: 1.5059\n",
      "Epoch [18/100], Step [830/1751], Loss: 1.3098\n",
      "Epoch [18/100], Step [840/1751], Loss: 1.5059\n",
      "Epoch [18/100], Step [850/1751], Loss: 1.2182\n",
      "Epoch [18/100], Step [860/1751], Loss: 1.2443\n",
      "Epoch [18/100], Step [870/1751], Loss: 1.2746\n",
      "Epoch [18/100], Step [880/1751], Loss: 1.2214\n",
      "Epoch [18/100], Step [890/1751], Loss: 1.2152\n",
      "Epoch [18/100], Step [900/1751], Loss: 1.2759\n",
      "Epoch [18/100], Step [910/1751], Loss: 1.4315\n",
      "Epoch [18/100], Step [920/1751], Loss: 1.3257\n",
      "Epoch [18/100], Step [930/1751], Loss: 1.2966\n",
      "Epoch [18/100], Step [940/1751], Loss: 1.3436\n",
      "Epoch [18/100], Step [950/1751], Loss: 1.3165\n",
      "Epoch [18/100], Step [960/1751], Loss: 1.3616\n",
      "Epoch [18/100], Step [970/1751], Loss: 1.2753\n",
      "Epoch [18/100], Step [980/1751], Loss: 1.1335\n",
      "Epoch [18/100], Step [990/1751], Loss: 1.3092\n",
      "Epoch [18/100], Step [1000/1751], Loss: 1.2150\n",
      "Epoch [18/100], Step [1010/1751], Loss: 1.2306\n",
      "Epoch [18/100], Step [1020/1751], Loss: 1.3514\n",
      "Epoch [18/100], Step [1030/1751], Loss: 1.3471\n",
      "Epoch [18/100], Step [1040/1751], Loss: 1.2384\n",
      "Epoch [18/100], Step [1050/1751], Loss: 1.3495\n",
      "Epoch [18/100], Step [1060/1751], Loss: 1.2655\n",
      "Epoch [18/100], Step [1070/1751], Loss: 1.4087\n",
      "Epoch [18/100], Step [1080/1751], Loss: 1.0649\n",
      "Epoch [18/100], Step [1090/1751], Loss: 1.2965\n",
      "Epoch [18/100], Step [1100/1751], Loss: 1.3738\n",
      "Epoch [18/100], Step [1110/1751], Loss: 1.3309\n",
      "Epoch [18/100], Step [1120/1751], Loss: 1.4615\n",
      "Epoch [18/100], Step [1130/1751], Loss: 1.3764\n",
      "Epoch [18/100], Step [1140/1751], Loss: 1.2311\n",
      "Epoch [18/100], Step [1150/1751], Loss: 1.2800\n",
      "Epoch [18/100], Step [1160/1751], Loss: 1.3156\n",
      "Epoch [18/100], Step [1170/1751], Loss: 1.3109\n",
      "Epoch [18/100], Step [1180/1751], Loss: 1.3814\n",
      "Epoch [18/100], Step [1190/1751], Loss: 1.2780\n",
      "Epoch [18/100], Step [1200/1751], Loss: 1.2191\n",
      "Epoch [18/100], Step [1210/1751], Loss: 1.4314\n",
      "Epoch [18/100], Step [1220/1751], Loss: 1.2769\n",
      "Epoch [18/100], Step [1230/1751], Loss: 1.2163\n",
      "Epoch [18/100], Step [1240/1751], Loss: 1.2480\n",
      "Epoch [18/100], Step [1250/1751], Loss: 1.3485\n",
      "Epoch [18/100], Step [1260/1751], Loss: 1.2625\n",
      "Epoch [18/100], Step [1270/1751], Loss: 1.4052\n",
      "Epoch [18/100], Step [1280/1751], Loss: 1.3430\n",
      "Epoch [18/100], Step [1290/1751], Loss: 1.4240\n",
      "Epoch [18/100], Step [1300/1751], Loss: 1.2579\n",
      "Epoch [18/100], Step [1310/1751], Loss: 1.3377\n",
      "Epoch [18/100], Step [1320/1751], Loss: 1.2307\n",
      "Epoch [18/100], Step [1330/1751], Loss: 1.4132\n",
      "Epoch [18/100], Step [1340/1751], Loss: 1.3414\n",
      "Epoch [18/100], Step [1350/1751], Loss: 1.3397\n",
      "Epoch [18/100], Step [1360/1751], Loss: 1.3751\n",
      "Epoch [18/100], Step [1370/1751], Loss: 1.3739\n",
      "Epoch [18/100], Step [1380/1751], Loss: 1.2548\n",
      "Epoch [18/100], Step [1390/1751], Loss: 1.2174\n",
      "Epoch [18/100], Step [1400/1751], Loss: 1.1987\n",
      "Epoch [18/100], Step [1410/1751], Loss: 1.2694\n",
      "Epoch [18/100], Step [1420/1751], Loss: 1.3268\n",
      "Epoch [18/100], Step [1430/1751], Loss: 1.1769\n",
      "Epoch [18/100], Step [1440/1751], Loss: 1.5128\n",
      "Epoch [18/100], Step [1450/1751], Loss: 1.2773\n",
      "Epoch [18/100], Step [1460/1751], Loss: 1.2888\n",
      "Epoch [18/100], Step [1470/1751], Loss: 1.3514\n",
      "Epoch [18/100], Step [1480/1751], Loss: 1.2121\n",
      "Epoch [18/100], Step [1490/1751], Loss: 1.1909\n",
      "Epoch [18/100], Step [1500/1751], Loss: 1.2773\n",
      "Epoch [18/100], Step [1510/1751], Loss: 1.1871\n",
      "Epoch [18/100], Step [1520/1751], Loss: 1.2242\n",
      "Epoch [18/100], Step [1530/1751], Loss: 1.2962\n",
      "Epoch [18/100], Step [1540/1751], Loss: 1.3061\n",
      "Epoch [18/100], Step [1550/1751], Loss: 1.2610\n",
      "Epoch [18/100], Step [1560/1751], Loss: 1.3413\n",
      "Epoch [18/100], Step [1570/1751], Loss: 1.3732\n",
      "Epoch [18/100], Step [1580/1751], Loss: 1.3718\n",
      "Epoch [18/100], Step [1590/1751], Loss: 1.2918\n",
      "Epoch [18/100], Step [1600/1751], Loss: 1.3690\n",
      "Epoch [18/100], Step [1610/1751], Loss: 1.3438\n",
      "Epoch [18/100], Step [1620/1751], Loss: 1.1474\n",
      "Epoch [18/100], Step [1630/1751], Loss: 1.3371\n",
      "Epoch [18/100], Step [1640/1751], Loss: 1.3441\n",
      "Epoch [18/100], Step [1650/1751], Loss: 1.3260\n",
      "Epoch [18/100], Step [1660/1751], Loss: 1.3584\n",
      "Epoch [18/100], Step [1670/1751], Loss: 1.2940\n",
      "Epoch [18/100], Step [1680/1751], Loss: 1.2325\n",
      "Epoch [18/100], Step [1690/1751], Loss: 1.3173\n",
      "Epoch [18/100], Step [1700/1751], Loss: 1.3536\n",
      "Epoch [18/100], Step [1710/1751], Loss: 1.2451\n",
      "Epoch [18/100], Step [1720/1751], Loss: 1.3458\n",
      "Epoch [18/100], Step [1730/1751], Loss: 1.2975\n",
      "Epoch [18/100], Step [1740/1751], Loss: 1.1751\n",
      "Epoch [18/100], Step [1750/1751], Loss: 1.2253\n",
      "Epoch [18/100], Average Loss: 1.2964, Time: 1637.8732s\n",
      "Epoch [19/100], Step [10/1751], Loss: 1.2499\n",
      "Epoch [19/100], Step [20/1751], Loss: 1.0918\n",
      "Epoch [19/100], Step [30/1751], Loss: 1.2675\n",
      "Epoch [19/100], Step [40/1751], Loss: 1.2665\n",
      "Epoch [19/100], Step [50/1751], Loss: 1.3648\n",
      "Epoch [19/100], Step [60/1751], Loss: 1.4342\n",
      "Epoch [19/100], Step [70/1751], Loss: 1.3814\n",
      "Epoch [19/100], Step [80/1751], Loss: 1.3453\n",
      "Epoch [19/100], Step [90/1751], Loss: 1.2482\n",
      "Epoch [19/100], Step [100/1751], Loss: 1.3483\n",
      "Epoch [19/100], Step [110/1751], Loss: 1.2918\n",
      "Epoch [19/100], Step [120/1751], Loss: 1.2260\n",
      "Epoch [19/100], Step [130/1751], Loss: 1.2572\n",
      "Epoch [19/100], Step [140/1751], Loss: 1.1832\n",
      "Epoch [19/100], Step [150/1751], Loss: 1.3685\n",
      "Epoch [19/100], Step [160/1751], Loss: 1.3119\n",
      "Epoch [19/100], Step [170/1751], Loss: 1.3017\n",
      "Epoch [19/100], Step [180/1751], Loss: 1.2646\n",
      "Epoch [19/100], Step [190/1751], Loss: 1.3388\n",
      "Epoch [19/100], Step [200/1751], Loss: 1.4119\n",
      "Epoch [19/100], Step [210/1751], Loss: 1.2804\n",
      "Epoch [19/100], Step [220/1751], Loss: 1.3387\n",
      "Epoch [19/100], Step [230/1751], Loss: 1.2921\n",
      "Epoch [19/100], Step [240/1751], Loss: 1.2506\n",
      "Epoch [19/100], Step [250/1751], Loss: 1.1637\n",
      "Epoch [19/100], Step [260/1751], Loss: 1.2653\n",
      "Epoch [19/100], Step [270/1751], Loss: 1.2820\n",
      "Epoch [19/100], Step [280/1751], Loss: 1.3027\n",
      "Epoch [19/100], Step [290/1751], Loss: 1.3892\n",
      "Epoch [19/100], Step [300/1751], Loss: 1.2232\n",
      "Epoch [19/100], Step [310/1751], Loss: 1.4216\n",
      "Epoch [19/100], Step [320/1751], Loss: 1.2330\n",
      "Epoch [19/100], Step [330/1751], Loss: 1.1931\n",
      "Epoch [19/100], Step [340/1751], Loss: 1.2495\n",
      "Epoch [19/100], Step [350/1751], Loss: 1.3527\n",
      "Epoch [19/100], Step [360/1751], Loss: 1.2792\n",
      "Epoch [19/100], Step [370/1751], Loss: 1.2663\n",
      "Epoch [19/100], Step [380/1751], Loss: 1.3334\n",
      "Epoch [19/100], Step [390/1751], Loss: 1.3921\n",
      "Epoch [19/100], Step [400/1751], Loss: 1.2998\n",
      "Epoch [19/100], Step [410/1751], Loss: 1.1454\n",
      "Epoch [19/100], Step [420/1751], Loss: 1.2830\n",
      "Epoch [19/100], Step [430/1751], Loss: 1.2666\n",
      "Epoch [19/100], Step [440/1751], Loss: 1.2851\n",
      "Epoch [19/100], Step [450/1751], Loss: 1.3731\n",
      "Epoch [19/100], Step [460/1751], Loss: 1.1127\n",
      "Epoch [19/100], Step [470/1751], Loss: 1.2008\n",
      "Epoch [19/100], Step [480/1751], Loss: 1.4326\n",
      "Epoch [19/100], Step [490/1751], Loss: 1.3277\n",
      "Epoch [19/100], Step [500/1751], Loss: 1.3549\n",
      "Epoch [19/100], Step [510/1751], Loss: 1.2826\n",
      "Epoch [19/100], Step [520/1751], Loss: 1.2907\n",
      "Epoch [19/100], Step [530/1751], Loss: 1.2555\n",
      "Epoch [19/100], Step [540/1751], Loss: 1.2225\n",
      "Epoch [19/100], Step [550/1751], Loss: 1.2741\n",
      "Epoch [19/100], Step [560/1751], Loss: 1.3029\n",
      "Epoch [19/100], Step [570/1751], Loss: 1.3669\n",
      "Epoch [19/100], Step [580/1751], Loss: 1.2727\n",
      "Epoch [19/100], Step [590/1751], Loss: 1.1661\n",
      "Epoch [19/100], Step [600/1751], Loss: 1.3862\n",
      "Epoch [19/100], Step [610/1751], Loss: 1.2648\n",
      "Epoch [19/100], Step [620/1751], Loss: 1.2988\n",
      "Epoch [19/100], Step [630/1751], Loss: 1.2056\n",
      "Epoch [19/100], Step [640/1751], Loss: 1.2157\n",
      "Epoch [19/100], Step [650/1751], Loss: 1.2971\n",
      "Epoch [19/100], Step [660/1751], Loss: 1.3835\n",
      "Epoch [19/100], Step [670/1751], Loss: 1.4154\n",
      "Epoch [19/100], Step [680/1751], Loss: 1.2361\n",
      "Epoch [19/100], Step [690/1751], Loss: 1.2362\n",
      "Epoch [19/100], Step [700/1751], Loss: 1.3118\n",
      "Epoch [19/100], Step [710/1751], Loss: 1.2604\n",
      "Epoch [19/100], Step [720/1751], Loss: 1.3878\n",
      "Epoch [19/100], Step [730/1751], Loss: 1.3267\n",
      "Epoch [19/100], Step [740/1751], Loss: 1.4247\n",
      "Epoch [19/100], Step [750/1751], Loss: 1.1834\n",
      "Epoch [19/100], Step [760/1751], Loss: 1.2578\n",
      "Epoch [19/100], Step [770/1751], Loss: 1.2748\n",
      "Epoch [19/100], Step [780/1751], Loss: 1.3483\n",
      "Epoch [19/100], Step [790/1751], Loss: 1.4361\n",
      "Epoch [19/100], Step [800/1751], Loss: 1.2539\n",
      "Epoch [19/100], Step [810/1751], Loss: 1.3293\n",
      "Epoch [19/100], Step [820/1751], Loss: 1.1125\n",
      "Epoch [19/100], Step [830/1751], Loss: 1.2139\n",
      "Epoch [19/100], Step [840/1751], Loss: 1.1678\n",
      "Epoch [19/100], Step [850/1751], Loss: 1.4174\n",
      "Epoch [19/100], Step [860/1751], Loss: 1.1944\n",
      "Epoch [19/100], Step [870/1751], Loss: 1.2448\n",
      "Epoch [19/100], Step [880/1751], Loss: 1.3262\n",
      "Epoch [19/100], Step [890/1751], Loss: 1.1335\n",
      "Epoch [19/100], Step [900/1751], Loss: 1.2845\n",
      "Epoch [19/100], Step [910/1751], Loss: 1.1669\n",
      "Epoch [19/100], Step [920/1751], Loss: 1.4051\n",
      "Epoch [19/100], Step [930/1751], Loss: 1.2236\n",
      "Epoch [19/100], Step [940/1751], Loss: 1.3330\n",
      "Epoch [19/100], Step [950/1751], Loss: 1.5054\n",
      "Epoch [19/100], Step [960/1751], Loss: 1.2198\n",
      "Epoch [19/100], Step [970/1751], Loss: 1.2485\n",
      "Epoch [19/100], Step [980/1751], Loss: 1.2979\n",
      "Epoch [19/100], Step [990/1751], Loss: 1.2505\n",
      "Epoch [19/100], Step [1000/1751], Loss: 1.3401\n",
      "Epoch [19/100], Step [1010/1751], Loss: 1.4170\n",
      "Epoch [19/100], Step [1020/1751], Loss: 1.1404\n",
      "Epoch [19/100], Step [1030/1751], Loss: 1.2322\n",
      "Epoch [19/100], Step [1040/1751], Loss: 1.1705\n",
      "Epoch [19/100], Step [1050/1751], Loss: 1.4011\n",
      "Epoch [19/100], Step [1060/1751], Loss: 1.2197\n",
      "Epoch [19/100], Step [1070/1751], Loss: 1.2319\n",
      "Epoch [19/100], Step [1080/1751], Loss: 1.2111\n",
      "Epoch [19/100], Step [1090/1751], Loss: 1.2989\n",
      "Epoch [19/100], Step [1100/1751], Loss: 1.2704\n",
      "Epoch [19/100], Step [1110/1751], Loss: 1.2608\n",
      "Epoch [19/100], Step [1120/1751], Loss: 1.4311\n",
      "Epoch [19/100], Step [1130/1751], Loss: 1.1340\n",
      "Epoch [19/100], Step [1140/1751], Loss: 1.1534\n",
      "Epoch [19/100], Step [1150/1751], Loss: 1.3180\n",
      "Epoch [19/100], Step [1160/1751], Loss: 1.1446\n",
      "Epoch [19/100], Step [1170/1751], Loss: 1.2886\n",
      "Epoch [19/100], Step [1180/1751], Loss: 1.2538\n",
      "Epoch [19/100], Step [1190/1751], Loss: 1.2551\n",
      "Epoch [19/100], Step [1200/1751], Loss: 1.2580\n",
      "Epoch [19/100], Step [1210/1751], Loss: 1.4580\n",
      "Epoch [19/100], Step [1220/1751], Loss: 1.2126\n",
      "Epoch [19/100], Step [1230/1751], Loss: 1.1614\n",
      "Epoch [19/100], Step [1240/1751], Loss: 1.1668\n",
      "Epoch [19/100], Step [1250/1751], Loss: 1.2958\n",
      "Epoch [19/100], Step [1260/1751], Loss: 1.3277\n",
      "Epoch [19/100], Step [1270/1751], Loss: 1.1336\n",
      "Epoch [19/100], Step [1280/1751], Loss: 1.2302\n",
      "Epoch [19/100], Step [1290/1751], Loss: 1.2858\n",
      "Epoch [19/100], Step [1300/1751], Loss: 1.2952\n",
      "Epoch [19/100], Step [1310/1751], Loss: 1.1893\n",
      "Epoch [19/100], Step [1320/1751], Loss: 1.3060\n",
      "Epoch [19/100], Step [1330/1751], Loss: 1.1968\n",
      "Epoch [19/100], Step [1340/1751], Loss: 1.1302\n",
      "Epoch [19/100], Step [1350/1751], Loss: 1.3382\n",
      "Epoch [19/100], Step [1360/1751], Loss: 1.2739\n",
      "Epoch [19/100], Step [1370/1751], Loss: 1.3029\n",
      "Epoch [19/100], Step [1380/1751], Loss: 1.2194\n",
      "Epoch [19/100], Step [1390/1751], Loss: 1.2388\n",
      "Epoch [19/100], Step [1400/1751], Loss: 1.2492\n",
      "Epoch [19/100], Step [1410/1751], Loss: 1.2158\n",
      "Epoch [19/100], Step [1420/1751], Loss: 1.2941\n",
      "Epoch [19/100], Step [1430/1751], Loss: 1.2843\n",
      "Epoch [19/100], Step [1440/1751], Loss: 1.2972\n",
      "Epoch [19/100], Step [1450/1751], Loss: 1.3889\n",
      "Epoch [19/100], Step [1460/1751], Loss: 1.3614\n",
      "Epoch [19/100], Step [1470/1751], Loss: 1.3488\n",
      "Epoch [19/100], Step [1480/1751], Loss: 1.3251\n",
      "Epoch [19/100], Step [1490/1751], Loss: 1.4044\n",
      "Epoch [19/100], Step [1500/1751], Loss: 1.2368\n",
      "Epoch [19/100], Step [1510/1751], Loss: 1.1964\n",
      "Epoch [19/100], Step [1520/1751], Loss: 1.3637\n",
      "Epoch [19/100], Step [1530/1751], Loss: 1.1794\n",
      "Epoch [19/100], Step [1540/1751], Loss: 1.2003\n",
      "Epoch [19/100], Step [1550/1751], Loss: 1.2622\n",
      "Epoch [19/100], Step [1560/1751], Loss: 1.3415\n",
      "Epoch [19/100], Step [1570/1751], Loss: 1.2369\n",
      "Epoch [19/100], Step [1580/1751], Loss: 1.2927\n",
      "Epoch [19/100], Step [1590/1751], Loss: 1.3221\n",
      "Epoch [19/100], Step [1600/1751], Loss: 1.2049\n",
      "Epoch [19/100], Step [1610/1751], Loss: 1.2607\n",
      "Epoch [19/100], Step [1620/1751], Loss: 1.2225\n",
      "Epoch [19/100], Step [1630/1751], Loss: 1.1696\n",
      "Epoch [19/100], Step [1640/1751], Loss: 1.4113\n",
      "Epoch [19/100], Step [1650/1751], Loss: 1.2363\n",
      "Epoch [19/100], Step [1660/1751], Loss: 1.2864\n",
      "Epoch [19/100], Step [1670/1751], Loss: 1.3883\n",
      "Epoch [19/100], Step [1680/1751], Loss: 1.2562\n",
      "Epoch [19/100], Step [1690/1751], Loss: 1.3418\n",
      "Epoch [19/100], Step [1700/1751], Loss: 1.2886\n",
      "Epoch [19/100], Step [1710/1751], Loss: 1.2571\n",
      "Epoch [19/100], Step [1720/1751], Loss: 1.3802\n",
      "Epoch [19/100], Step [1730/1751], Loss: 1.2598\n",
      "Epoch [19/100], Step [1740/1751], Loss: 1.2193\n",
      "Epoch [19/100], Step [1750/1751], Loss: 1.2419\n",
      "Epoch [19/100], Average Loss: 1.2895, Time: 1638.1579s\n",
      "Epoch [20/100], Step [10/1751], Loss: 1.1296\n",
      "Epoch [20/100], Step [20/1751], Loss: 1.2529\n",
      "Epoch [20/100], Step [30/1751], Loss: 1.3970\n",
      "Epoch [20/100], Step [40/1751], Loss: 1.2719\n",
      "Epoch [20/100], Step [50/1751], Loss: 1.1635\n",
      "Epoch [20/100], Step [60/1751], Loss: 1.3352\n",
      "Epoch [20/100], Step [70/1751], Loss: 1.2394\n",
      "Epoch [20/100], Step [80/1751], Loss: 1.2614\n",
      "Epoch [20/100], Step [90/1751], Loss: 1.4368\n",
      "Epoch [20/100], Step [100/1751], Loss: 1.3135\n",
      "Epoch [20/100], Step [110/1751], Loss: 1.2860\n",
      "Epoch [20/100], Step [120/1751], Loss: 1.3652\n",
      "Epoch [20/100], Step [130/1751], Loss: 1.2816\n",
      "Epoch [20/100], Step [140/1751], Loss: 1.2860\n",
      "Epoch [20/100], Step [150/1751], Loss: 1.3332\n",
      "Epoch [20/100], Step [160/1751], Loss: 1.1960\n",
      "Epoch [20/100], Step [170/1751], Loss: 1.2942\n",
      "Epoch [20/100], Step [180/1751], Loss: 1.3918\n",
      "Epoch [20/100], Step [190/1751], Loss: 1.2383\n",
      "Epoch [20/100], Step [200/1751], Loss: 1.3675\n",
      "Epoch [20/100], Step [210/1751], Loss: 1.2273\n",
      "Epoch [20/100], Step [220/1751], Loss: 1.3445\n",
      "Epoch [20/100], Step [230/1751], Loss: 1.2899\n",
      "Epoch [20/100], Step [240/1751], Loss: 1.2012\n",
      "Epoch [20/100], Step [250/1751], Loss: 1.5109\n",
      "Epoch [20/100], Step [260/1751], Loss: 1.2993\n",
      "Epoch [20/100], Step [270/1751], Loss: 1.1978\n",
      "Epoch [20/100], Step [280/1751], Loss: 1.3501\n",
      "Epoch [20/100], Step [290/1751], Loss: 1.3702\n",
      "Epoch [20/100], Step [300/1751], Loss: 1.1633\n",
      "Epoch [20/100], Step [310/1751], Loss: 1.2141\n",
      "Epoch [20/100], Step [320/1751], Loss: 1.1543\n",
      "Epoch [20/100], Step [330/1751], Loss: 1.1867\n",
      "Epoch [20/100], Step [340/1751], Loss: 1.2571\n",
      "Epoch [20/100], Step [350/1751], Loss: 1.4313\n",
      "Epoch [20/100], Step [360/1751], Loss: 1.4961\n",
      "Epoch [20/100], Step [370/1751], Loss: 1.3263\n",
      "Epoch [20/100], Step [380/1751], Loss: 1.3628\n",
      "Epoch [20/100], Step [390/1751], Loss: 1.3191\n",
      "Epoch [20/100], Step [400/1751], Loss: 1.2923\n",
      "Epoch [20/100], Step [410/1751], Loss: 1.2638\n",
      "Epoch [20/100], Step [420/1751], Loss: 1.2201\n",
      "Epoch [20/100], Step [430/1751], Loss: 1.2080\n",
      "Epoch [20/100], Step [440/1751], Loss: 1.2374\n",
      "Epoch [20/100], Step [450/1751], Loss: 1.3272\n",
      "Epoch [20/100], Step [460/1751], Loss: 1.2634\n",
      "Epoch [20/100], Step [470/1751], Loss: 1.2743\n",
      "Epoch [20/100], Step [480/1751], Loss: 1.2142\n",
      "Epoch [20/100], Step [490/1751], Loss: 1.3041\n",
      "Epoch [20/100], Step [500/1751], Loss: 1.1994\n",
      "Epoch [20/100], Step [510/1751], Loss: 1.2034\n",
      "Epoch [20/100], Step [520/1751], Loss: 1.3958\n",
      "Epoch [20/100], Step [530/1751], Loss: 1.2497\n",
      "Epoch [20/100], Step [540/1751], Loss: 1.5036\n",
      "Epoch [20/100], Step [550/1751], Loss: 1.3167\n",
      "Epoch [20/100], Step [560/1751], Loss: 1.2898\n",
      "Epoch [20/100], Step [570/1751], Loss: 1.2872\n",
      "Epoch [20/100], Step [580/1751], Loss: 1.2767\n",
      "Epoch [20/100], Step [590/1751], Loss: 1.0573\n",
      "Epoch [20/100], Step [600/1751], Loss: 1.2625\n",
      "Epoch [20/100], Step [610/1751], Loss: 1.2640\n",
      "Epoch [20/100], Step [620/1751], Loss: 1.3278\n",
      "Epoch [20/100], Step [630/1751], Loss: 1.3269\n",
      "Epoch [20/100], Step [640/1751], Loss: 1.2394\n",
      "Epoch [20/100], Step [650/1751], Loss: 1.2844\n",
      "Epoch [20/100], Step [660/1751], Loss: 1.2772\n",
      "Epoch [20/100], Step [670/1751], Loss: 1.3063\n",
      "Epoch [20/100], Step [680/1751], Loss: 1.2942\n",
      "Epoch [20/100], Step [690/1751], Loss: 1.2049\n",
      "Epoch [20/100], Step [700/1751], Loss: 1.2785\n",
      "Epoch [20/100], Step [710/1751], Loss: 1.2963\n",
      "Epoch [20/100], Step [720/1751], Loss: 1.2434\n",
      "Epoch [20/100], Step [730/1751], Loss: 1.2512\n",
      "Epoch [20/100], Step [740/1751], Loss: 1.3088\n",
      "Epoch [20/100], Step [750/1751], Loss: 1.2988\n",
      "Epoch [20/100], Step [760/1751], Loss: 1.1852\n",
      "Epoch [20/100], Step [770/1751], Loss: 1.3833\n",
      "Epoch [20/100], Step [780/1751], Loss: 1.3681\n",
      "Epoch [20/100], Step [790/1751], Loss: 1.2690\n",
      "Epoch [20/100], Step [800/1751], Loss: 1.3642\n",
      "Epoch [20/100], Step [810/1751], Loss: 1.2297\n",
      "Epoch [20/100], Step [820/1751], Loss: 1.1410\n",
      "Epoch [20/100], Step [830/1751], Loss: 1.1524\n",
      "Epoch [20/100], Step [840/1751], Loss: 1.2948\n",
      "Epoch [20/100], Step [850/1751], Loss: 1.1417\n",
      "Epoch [20/100], Step [860/1751], Loss: 1.4098\n",
      "Epoch [20/100], Step [870/1751], Loss: 1.2072\n",
      "Epoch [20/100], Step [880/1751], Loss: 1.2900\n",
      "Epoch [20/100], Step [890/1751], Loss: 1.1673\n",
      "Epoch [20/100], Step [900/1751], Loss: 1.2615\n",
      "Epoch [20/100], Step [910/1751], Loss: 1.1889\n",
      "Epoch [20/100], Step [920/1751], Loss: 1.2909\n",
      "Epoch [20/100], Step [930/1751], Loss: 1.3273\n",
      "Epoch [20/100], Step [940/1751], Loss: 1.3215\n",
      "Epoch [20/100], Step [950/1751], Loss: 1.3693\n",
      "Epoch [20/100], Step [960/1751], Loss: 1.3102\n",
      "Epoch [20/100], Step [970/1751], Loss: 1.1737\n",
      "Epoch [20/100], Step [980/1751], Loss: 1.3031\n",
      "Epoch [20/100], Step [990/1751], Loss: 1.3713\n",
      "Epoch [20/100], Step [1000/1751], Loss: 1.3457\n",
      "Epoch [20/100], Step [1010/1751], Loss: 1.3322\n",
      "Epoch [20/100], Step [1020/1751], Loss: 1.4453\n",
      "Epoch [20/100], Step [1030/1751], Loss: 1.2392\n",
      "Epoch [20/100], Step [1040/1751], Loss: 1.3420\n",
      "Epoch [20/100], Step [1050/1751], Loss: 1.2106\n",
      "Epoch [20/100], Step [1060/1751], Loss: 1.2937\n",
      "Epoch [20/100], Step [1070/1751], Loss: 1.1967\n",
      "Epoch [20/100], Step [1080/1751], Loss: 1.3340\n",
      "Epoch [20/100], Step [1090/1751], Loss: 1.2693\n",
      "Epoch [20/100], Step [1100/1751], Loss: 1.3302\n",
      "Epoch [20/100], Step [1110/1751], Loss: 1.2520\n",
      "Epoch [20/100], Step [1120/1751], Loss: 1.2804\n",
      "Epoch [20/100], Step [1130/1751], Loss: 1.2140\n",
      "Epoch [20/100], Step [1140/1751], Loss: 1.2687\n",
      "Epoch [20/100], Step [1150/1751], Loss: 1.0278\n",
      "Epoch [20/100], Step [1160/1751], Loss: 1.2763\n",
      "Epoch [20/100], Step [1170/1751], Loss: 1.1774\n",
      "Epoch [20/100], Step [1180/1751], Loss: 1.2907\n",
      "Epoch [20/100], Step [1190/1751], Loss: 1.3708\n",
      "Epoch [20/100], Step [1200/1751], Loss: 1.4271\n",
      "Epoch [20/100], Step [1210/1751], Loss: 1.3252\n",
      "Epoch [20/100], Step [1220/1751], Loss: 1.0881\n",
      "Epoch [20/100], Step [1230/1751], Loss: 1.4439\n",
      "Epoch [20/100], Step [1240/1751], Loss: 1.3410\n",
      "Epoch [20/100], Step [1250/1751], Loss: 1.2563\n",
      "Epoch [20/100], Step [1260/1751], Loss: 1.4440\n",
      "Epoch [20/100], Step [1270/1751], Loss: 1.3879\n",
      "Epoch [20/100], Step [1280/1751], Loss: 1.2839\n",
      "Epoch [20/100], Step [1290/1751], Loss: 1.3413\n",
      "Epoch [20/100], Step [1300/1751], Loss: 1.2787\n",
      "Epoch [20/100], Step [1310/1751], Loss: 1.2245\n",
      "Epoch [20/100], Step [1320/1751], Loss: 1.3171\n",
      "Epoch [20/100], Step [1330/1751], Loss: 1.1975\n",
      "Epoch [20/100], Step [1340/1751], Loss: 1.2019\n",
      "Epoch [20/100], Step [1350/1751], Loss: 1.2928\n",
      "Epoch [20/100], Step [1360/1751], Loss: 1.2982\n",
      "Epoch [20/100], Step [1370/1751], Loss: 1.2993\n",
      "Epoch [20/100], Step [1380/1751], Loss: 1.4124\n",
      "Epoch [20/100], Step [1390/1751], Loss: 1.2050\n",
      "Epoch [20/100], Step [1400/1751], Loss: 1.3538\n",
      "Epoch [20/100], Step [1410/1751], Loss: 1.3605\n",
      "Epoch [20/100], Step [1420/1751], Loss: 1.2557\n",
      "Epoch [20/100], Step [1430/1751], Loss: 1.2597\n",
      "Epoch [20/100], Step [1440/1751], Loss: 1.3622\n",
      "Epoch [20/100], Step [1450/1751], Loss: 1.2807\n",
      "Epoch [20/100], Step [1460/1751], Loss: 1.1830\n",
      "Epoch [20/100], Step [1470/1751], Loss: 1.3728\n",
      "Epoch [20/100], Step [1480/1751], Loss: 1.3936\n",
      "Epoch [20/100], Step [1490/1751], Loss: 1.1678\n",
      "Epoch [20/100], Step [1500/1751], Loss: 1.4514\n",
      "Epoch [20/100], Step [1510/1751], Loss: 1.2210\n",
      "Epoch [20/100], Step [1520/1751], Loss: 1.2916\n",
      "Epoch [20/100], Step [1530/1751], Loss: 1.2740\n",
      "Epoch [20/100], Step [1540/1751], Loss: 1.3843\n",
      "Epoch [20/100], Step [1550/1751], Loss: 1.1295\n",
      "Epoch [20/100], Step [1560/1751], Loss: 1.1691\n",
      "Epoch [20/100], Step [1570/1751], Loss: 1.2323\n",
      "Epoch [20/100], Step [1580/1751], Loss: 1.1984\n",
      "Epoch [20/100], Step [1590/1751], Loss: 1.3338\n",
      "Epoch [20/100], Step [1600/1751], Loss: 1.3873\n",
      "Epoch [20/100], Step [1610/1751], Loss: 1.1833\n",
      "Epoch [20/100], Step [1620/1751], Loss: 1.3643\n",
      "Epoch [20/100], Step [1630/1751], Loss: 1.3283\n",
      "Epoch [20/100], Step [1640/1751], Loss: 1.3979\n",
      "Epoch [20/100], Step [1650/1751], Loss: 1.2786\n",
      "Epoch [20/100], Step [1660/1751], Loss: 1.2673\n",
      "Epoch [20/100], Step [1670/1751], Loss: 1.2331\n",
      "Epoch [20/100], Step [1680/1751], Loss: 1.1593\n",
      "Epoch [20/100], Step [1690/1751], Loss: 1.2243\n",
      "Epoch [20/100], Step [1700/1751], Loss: 1.4229\n",
      "Epoch [20/100], Step [1710/1751], Loss: 1.2778\n",
      "Epoch [20/100], Step [1720/1751], Loss: 1.3320\n",
      "Epoch [20/100], Step [1730/1751], Loss: 1.3151\n",
      "Epoch [20/100], Step [1740/1751], Loss: 1.4081\n",
      "Epoch [20/100], Step [1750/1751], Loss: 1.1675\n",
      "Epoch [20/100], Average Loss: 1.2833, Time: 1638.7214s\n",
      "Epoch [21/100], Step [10/1751], Loss: 1.3767\n",
      "Epoch [21/100], Step [20/1751], Loss: 1.1634\n",
      "Epoch [21/100], Step [30/1751], Loss: 1.2633\n",
      "Epoch [21/100], Step [40/1751], Loss: 1.2415\n",
      "Epoch [21/100], Step [50/1751], Loss: 1.4533\n",
      "Epoch [21/100], Step [60/1751], Loss: 1.2410\n",
      "Epoch [21/100], Step [70/1751], Loss: 1.5061\n",
      "Epoch [21/100], Step [80/1751], Loss: 1.2214\n",
      "Epoch [21/100], Step [90/1751], Loss: 1.2733\n",
      "Epoch [21/100], Step [100/1751], Loss: 1.2750\n",
      "Epoch [21/100], Step [110/1751], Loss: 1.3657\n",
      "Epoch [21/100], Step [120/1751], Loss: 1.2158\n",
      "Epoch [21/100], Step [130/1751], Loss: 1.3261\n",
      "Epoch [21/100], Step [140/1751], Loss: 1.4272\n",
      "Epoch [21/100], Step [150/1751], Loss: 1.2568\n",
      "Epoch [21/100], Step [160/1751], Loss: 1.4019\n",
      "Epoch [21/100], Step [170/1751], Loss: 1.3058\n",
      "Epoch [21/100], Step [180/1751], Loss: 1.3942\n",
      "Epoch [21/100], Step [190/1751], Loss: 1.3617\n",
      "Epoch [21/100], Step [200/1751], Loss: 1.3202\n",
      "Epoch [21/100], Step [210/1751], Loss: 1.3297\n",
      "Epoch [21/100], Step [220/1751], Loss: 1.3018\n",
      "Epoch [21/100], Step [230/1751], Loss: 1.3727\n",
      "Epoch [21/100], Step [240/1751], Loss: 1.2040\n",
      "Epoch [21/100], Step [250/1751], Loss: 1.5263\n",
      "Epoch [21/100], Step [260/1751], Loss: 1.2251\n",
      "Epoch [21/100], Step [270/1751], Loss: 1.3129\n",
      "Epoch [21/100], Step [280/1751], Loss: 1.2893\n",
      "Epoch [21/100], Step [290/1751], Loss: 1.2689\n",
      "Epoch [21/100], Step [300/1751], Loss: 1.3377\n",
      "Epoch [21/100], Step [310/1751], Loss: 1.1989\n",
      "Epoch [21/100], Step [320/1751], Loss: 1.2672\n",
      "Epoch [21/100], Step [330/1751], Loss: 1.2348\n",
      "Epoch [21/100], Step [340/1751], Loss: 1.3662\n",
      "Epoch [21/100], Step [350/1751], Loss: 1.1906\n",
      "Epoch [21/100], Step [360/1751], Loss: 1.2531\n",
      "Epoch [21/100], Step [370/1751], Loss: 1.3104\n",
      "Epoch [21/100], Step [380/1751], Loss: 1.3145\n",
      "Epoch [21/100], Step [390/1751], Loss: 1.3215\n",
      "Epoch [21/100], Step [400/1751], Loss: 1.2422\n",
      "Epoch [21/100], Step [410/1751], Loss: 1.2649\n",
      "Epoch [21/100], Step [420/1751], Loss: 1.3285\n",
      "Epoch [21/100], Step [430/1751], Loss: 1.3336\n",
      "Epoch [21/100], Step [440/1751], Loss: 1.2134\n",
      "Epoch [21/100], Step [450/1751], Loss: 1.3690\n",
      "Epoch [21/100], Step [460/1751], Loss: 1.2437\n",
      "Epoch [21/100], Step [470/1751], Loss: 1.2493\n",
      "Epoch [21/100], Step [480/1751], Loss: 1.2902\n",
      "Epoch [21/100], Step [490/1751], Loss: 1.3271\n",
      "Epoch [21/100], Step [500/1751], Loss: 1.3169\n",
      "Epoch [21/100], Step [510/1751], Loss: 1.2487\n",
      "Epoch [21/100], Step [520/1751], Loss: 1.2996\n",
      "Epoch [21/100], Step [530/1751], Loss: 1.2361\n",
      "Epoch [21/100], Step [540/1751], Loss: 1.3510\n",
      "Epoch [21/100], Step [550/1751], Loss: 1.3298\n",
      "Epoch [21/100], Step [560/1751], Loss: 1.3313\n",
      "Epoch [21/100], Step [570/1751], Loss: 1.4313\n",
      "Epoch [21/100], Step [580/1751], Loss: 1.2754\n",
      "Epoch [21/100], Step [590/1751], Loss: 1.2280\n",
      "Epoch [21/100], Step [600/1751], Loss: 1.2112\n",
      "Epoch [21/100], Step [610/1751], Loss: 1.2770\n",
      "Epoch [21/100], Step [620/1751], Loss: 1.3215\n",
      "Epoch [21/100], Step [630/1751], Loss: 1.2758\n",
      "Epoch [21/100], Step [640/1751], Loss: 1.2822\n",
      "Epoch [21/100], Step [650/1751], Loss: 1.3567\n",
      "Epoch [21/100], Step [660/1751], Loss: 1.3243\n",
      "Epoch [21/100], Step [670/1751], Loss: 1.3091\n",
      "Epoch [21/100], Step [680/1751], Loss: 1.3902\n",
      "Epoch [21/100], Step [690/1751], Loss: 1.3976\n",
      "Epoch [21/100], Step [700/1751], Loss: 1.3042\n",
      "Epoch [21/100], Step [710/1751], Loss: 1.2757\n",
      "Epoch [21/100], Step [720/1751], Loss: 1.2249\n",
      "Epoch [21/100], Step [730/1751], Loss: 1.1781\n",
      "Epoch [21/100], Step [740/1751], Loss: 1.2773\n",
      "Epoch [21/100], Step [750/1751], Loss: 1.3394\n",
      "Epoch [21/100], Step [760/1751], Loss: 1.2376\n",
      "Epoch [21/100], Step [770/1751], Loss: 1.4293\n",
      "Epoch [21/100], Step [780/1751], Loss: 1.2960\n",
      "Epoch [21/100], Step [790/1751], Loss: 1.1675\n",
      "Epoch [21/100], Step [800/1751], Loss: 1.2320\n",
      "Epoch [21/100], Step [810/1751], Loss: 1.3351\n",
      "Epoch [21/100], Step [820/1751], Loss: 1.2146\n",
      "Epoch [21/100], Step [830/1751], Loss: 1.2974\n",
      "Epoch [21/100], Step [840/1751], Loss: 1.2104\n",
      "Epoch [21/100], Step [850/1751], Loss: 1.3417\n",
      "Epoch [21/100], Step [860/1751], Loss: 1.2798\n",
      "Epoch [21/100], Step [870/1751], Loss: 1.2105\n",
      "Epoch [21/100], Step [880/1751], Loss: 1.3002\n",
      "Epoch [21/100], Step [890/1751], Loss: 1.2580\n",
      "Epoch [21/100], Step [900/1751], Loss: 1.2795\n",
      "Epoch [21/100], Step [910/1751], Loss: 1.3419\n",
      "Epoch [21/100], Step [920/1751], Loss: 1.3372\n",
      "Epoch [21/100], Step [930/1751], Loss: 1.2826\n",
      "Epoch [21/100], Step [940/1751], Loss: 1.3376\n",
      "Epoch [21/100], Step [950/1751], Loss: 1.2704\n",
      "Epoch [21/100], Step [960/1751], Loss: 1.3076\n",
      "Epoch [21/100], Step [970/1751], Loss: 1.4120\n",
      "Epoch [21/100], Step [980/1751], Loss: 1.3185\n",
      "Epoch [21/100], Step [990/1751], Loss: 1.2570\n",
      "Epoch [21/100], Step [1000/1751], Loss: 1.1648\n",
      "Epoch [21/100], Step [1010/1751], Loss: 1.2406\n",
      "Epoch [21/100], Step [1020/1751], Loss: 1.3286\n",
      "Epoch [21/100], Step [1030/1751], Loss: 1.1739\n",
      "Epoch [21/100], Step [1040/1751], Loss: 1.2341\n",
      "Epoch [21/100], Step [1050/1751], Loss: 1.3285\n",
      "Epoch [21/100], Step [1060/1751], Loss: 1.2412\n",
      "Epoch [21/100], Step [1070/1751], Loss: 1.2006\n",
      "Epoch [21/100], Step [1080/1751], Loss: 1.3273\n",
      "Epoch [21/100], Step [1090/1751], Loss: 1.4496\n",
      "Epoch [21/100], Step [1100/1751], Loss: 1.3373\n",
      "Epoch [21/100], Step [1110/1751], Loss: 1.2203\n",
      "Epoch [21/100], Step [1120/1751], Loss: 1.2743\n",
      "Epoch [21/100], Step [1130/1751], Loss: 1.4205\n",
      "Epoch [21/100], Step [1140/1751], Loss: 1.3202\n",
      "Epoch [21/100], Step [1150/1751], Loss: 1.2279\n",
      "Epoch [21/100], Step [1160/1751], Loss: 1.3058\n",
      "Epoch [21/100], Step [1170/1751], Loss: 1.1749\n",
      "Epoch [21/100], Step [1180/1751], Loss: 1.2781\n",
      "Epoch [21/100], Step [1190/1751], Loss: 1.2869\n",
      "Epoch [21/100], Step [1200/1751], Loss: 1.2903\n",
      "Epoch [21/100], Step [1210/1751], Loss: 1.1784\n",
      "Epoch [21/100], Step [1220/1751], Loss: 1.3048\n",
      "Epoch [21/100], Step [1230/1751], Loss: 1.2330\n",
      "Epoch [21/100], Step [1240/1751], Loss: 1.4200\n",
      "Epoch [21/100], Step [1250/1751], Loss: 1.3330\n",
      "Epoch [21/100], Step [1260/1751], Loss: 1.1260\n",
      "Epoch [21/100], Step [1270/1751], Loss: 1.2210\n",
      "Epoch [21/100], Step [1280/1751], Loss: 1.2872\n",
      "Epoch [21/100], Step [1290/1751], Loss: 1.3938\n",
      "Epoch [21/100], Step [1300/1751], Loss: 1.4024\n",
      "Epoch [21/100], Step [1310/1751], Loss: 1.1594\n",
      "Epoch [21/100], Step [1320/1751], Loss: 1.1873\n",
      "Epoch [21/100], Step [1330/1751], Loss: 1.4459\n",
      "Epoch [21/100], Step [1340/1751], Loss: 1.1720\n",
      "Epoch [21/100], Step [1350/1751], Loss: 1.5340\n",
      "Epoch [21/100], Step [1360/1751], Loss: 1.3577\n",
      "Epoch [21/100], Step [1370/1751], Loss: 1.2546\n",
      "Epoch [21/100], Step [1380/1751], Loss: 1.1789\n",
      "Epoch [21/100], Step [1390/1751], Loss: 1.2467\n",
      "Epoch [21/100], Step [1400/1751], Loss: 1.4597\n",
      "Epoch [21/100], Step [1410/1751], Loss: 1.1419\n",
      "Epoch [21/100], Step [1420/1751], Loss: 1.3933\n",
      "Epoch [21/100], Step [1430/1751], Loss: 1.3207\n",
      "Epoch [21/100], Step [1440/1751], Loss: 1.2993\n",
      "Epoch [21/100], Step [1450/1751], Loss: 1.1148\n",
      "Epoch [21/100], Step [1460/1751], Loss: 1.2313\n",
      "Epoch [21/100], Step [1470/1751], Loss: 1.2039\n",
      "Epoch [21/100], Step [1480/1751], Loss: 1.3448\n",
      "Epoch [21/100], Step [1490/1751], Loss: 1.2651\n",
      "Epoch [21/100], Step [1500/1751], Loss: 1.1923\n",
      "Epoch [21/100], Step [1510/1751], Loss: 1.0472\n",
      "Epoch [21/100], Step [1520/1751], Loss: 1.2887\n",
      "Epoch [21/100], Step [1530/1751], Loss: 1.1952\n",
      "Epoch [21/100], Step [1540/1751], Loss: 1.3532\n",
      "Epoch [21/100], Step [1550/1751], Loss: 1.1693\n",
      "Epoch [21/100], Step [1560/1751], Loss: 1.2501\n",
      "Epoch [21/100], Step [1570/1751], Loss: 1.2481\n",
      "Epoch [21/100], Step [1580/1751], Loss: 1.3556\n",
      "Epoch [21/100], Step [1590/1751], Loss: 1.2395\n",
      "Epoch [21/100], Step [1600/1751], Loss: 1.2618\n",
      "Epoch [21/100], Step [1610/1751], Loss: 1.2816\n",
      "Epoch [21/100], Step [1620/1751], Loss: 1.2569\n",
      "Epoch [21/100], Step [1630/1751], Loss: 1.3048\n",
      "Epoch [21/100], Step [1640/1751], Loss: 1.3795\n",
      "Epoch [21/100], Step [1650/1751], Loss: 1.2052\n",
      "Epoch [21/100], Step [1660/1751], Loss: 1.2520\n",
      "Epoch [21/100], Step [1670/1751], Loss: 1.2319\n",
      "Epoch [21/100], Step [1680/1751], Loss: 1.2432\n",
      "Epoch [21/100], Step [1690/1751], Loss: 1.3343\n",
      "Epoch [21/100], Step [1700/1751], Loss: 1.2275\n",
      "Epoch [21/100], Step [1710/1751], Loss: 1.2816\n",
      "Epoch [21/100], Step [1720/1751], Loss: 1.3619\n",
      "Epoch [21/100], Step [1730/1751], Loss: 1.4105\n",
      "Epoch [21/100], Step [1740/1751], Loss: 1.1828\n",
      "Epoch [21/100], Step [1750/1751], Loss: 1.4042\n",
      "Epoch [21/100], Average Loss: 1.2780, Time: 1637.4232s\n",
      "Epoch [22/100], Step [10/1751], Loss: 1.1899\n",
      "Epoch [22/100], Step [20/1751], Loss: 1.2854\n",
      "Epoch [22/100], Step [30/1751], Loss: 1.3538\n",
      "Epoch [22/100], Step [40/1751], Loss: 1.3737\n",
      "Epoch [22/100], Step [50/1751], Loss: 1.2680\n",
      "Epoch [22/100], Step [60/1751], Loss: 1.2561\n",
      "Epoch [22/100], Step [70/1751], Loss: 1.1219\n",
      "Epoch [22/100], Step [80/1751], Loss: 1.3245\n",
      "Epoch [22/100], Step [90/1751], Loss: 1.2851\n",
      "Epoch [22/100], Step [100/1751], Loss: 1.1660\n",
      "Epoch [22/100], Step [110/1751], Loss: 1.2836\n",
      "Epoch [22/100], Step [120/1751], Loss: 1.2323\n",
      "Epoch [22/100], Step [130/1751], Loss: 1.3691\n",
      "Epoch [22/100], Step [140/1751], Loss: 1.2548\n",
      "Epoch [22/100], Step [150/1751], Loss: 1.3031\n",
      "Epoch [22/100], Step [160/1751], Loss: 1.3699\n",
      "Epoch [22/100], Step [170/1751], Loss: 1.2321\n",
      "Epoch [22/100], Step [180/1751], Loss: 1.1482\n",
      "Epoch [22/100], Step [190/1751], Loss: 1.1659\n",
      "Epoch [22/100], Step [200/1751], Loss: 1.2271\n",
      "Epoch [22/100], Step [210/1751], Loss: 1.2170\n",
      "Epoch [22/100], Step [220/1751], Loss: 1.4171\n",
      "Epoch [22/100], Step [230/1751], Loss: 1.2893\n",
      "Epoch [22/100], Step [240/1751], Loss: 1.1522\n",
      "Epoch [22/100], Step [250/1751], Loss: 1.2352\n",
      "Epoch [22/100], Step [260/1751], Loss: 1.1603\n",
      "Epoch [22/100], Step [270/1751], Loss: 1.3494\n",
      "Epoch [22/100], Step [280/1751], Loss: 1.1799\n",
      "Epoch [22/100], Step [290/1751], Loss: 1.2968\n",
      "Epoch [22/100], Step [300/1751], Loss: 1.2081\n",
      "Epoch [22/100], Step [310/1751], Loss: 1.2396\n",
      "Epoch [22/100], Step [320/1751], Loss: 1.3600\n",
      "Epoch [22/100], Step [330/1751], Loss: 1.2642\n",
      "Epoch [22/100], Step [340/1751], Loss: 1.2000\n",
      "Epoch [22/100], Step [350/1751], Loss: 1.1298\n",
      "Epoch [22/100], Step [360/1751], Loss: 1.2110\n",
      "Epoch [22/100], Step [370/1751], Loss: 1.2618\n",
      "Epoch [22/100], Step [380/1751], Loss: 1.1875\n",
      "Epoch [22/100], Step [390/1751], Loss: 0.9422\n",
      "Epoch [22/100], Step [400/1751], Loss: 1.2584\n",
      "Epoch [22/100], Step [410/1751], Loss: 1.2118\n",
      "Epoch [22/100], Step [420/1751], Loss: 1.2743\n",
      "Epoch [22/100], Step [430/1751], Loss: 1.4143\n",
      "Epoch [22/100], Step [440/1751], Loss: 1.1803\n",
      "Epoch [22/100], Step [450/1751], Loss: 1.2426\n",
      "Epoch [22/100], Step [460/1751], Loss: 1.4325\n",
      "Epoch [22/100], Step [470/1751], Loss: 1.1640\n",
      "Epoch [22/100], Step [480/1751], Loss: 1.1984\n",
      "Epoch [22/100], Step [490/1751], Loss: 1.1571\n",
      "Epoch [22/100], Step [500/1751], Loss: 1.3099\n",
      "Epoch [22/100], Step [510/1751], Loss: 1.1330\n",
      "Epoch [22/100], Step [520/1751], Loss: 1.2307\n",
      "Epoch [22/100], Step [530/1751], Loss: 1.3373\n",
      "Epoch [22/100], Step [540/1751], Loss: 1.2929\n",
      "Epoch [22/100], Step [550/1751], Loss: 1.3472\n",
      "Epoch [22/100], Step [560/1751], Loss: 1.4721\n",
      "Epoch [22/100], Step [570/1751], Loss: 1.3972\n",
      "Epoch [22/100], Step [580/1751], Loss: 1.3909\n",
      "Epoch [22/100], Step [590/1751], Loss: 1.3282\n",
      "Epoch [22/100], Step [600/1751], Loss: 1.1481\n",
      "Epoch [22/100], Step [610/1751], Loss: 1.3738\n",
      "Epoch [22/100], Step [620/1751], Loss: 1.2577\n",
      "Epoch [22/100], Step [630/1751], Loss: 1.3557\n",
      "Epoch [22/100], Step [640/1751], Loss: 1.3001\n",
      "Epoch [22/100], Step [650/1751], Loss: 1.2724\n",
      "Epoch [22/100], Step [660/1751], Loss: 1.3365\n",
      "Epoch [22/100], Step [670/1751], Loss: 1.2771\n",
      "Epoch [22/100], Step [680/1751], Loss: 1.2444\n",
      "Epoch [22/100], Step [690/1751], Loss: 1.4945\n",
      "Epoch [22/100], Step [700/1751], Loss: 1.2421\n",
      "Epoch [22/100], Step [710/1751], Loss: 1.4237\n",
      "Epoch [22/100], Step [720/1751], Loss: 1.4213\n",
      "Epoch [22/100], Step [730/1751], Loss: 1.2853\n",
      "Epoch [22/100], Step [740/1751], Loss: 1.4437\n",
      "Epoch [22/100], Step [750/1751], Loss: 1.4053\n",
      "Epoch [22/100], Step [760/1751], Loss: 1.2629\n",
      "Epoch [22/100], Step [770/1751], Loss: 1.3787\n",
      "Epoch [22/100], Step [780/1751], Loss: 1.2229\n",
      "Epoch [22/100], Step [790/1751], Loss: 1.2790\n",
      "Epoch [22/100], Step [800/1751], Loss: 1.2003\n",
      "Epoch [22/100], Step [810/1751], Loss: 1.2858\n",
      "Epoch [22/100], Step [820/1751], Loss: 1.2013\n",
      "Epoch [22/100], Step [830/1751], Loss: 1.1751\n",
      "Epoch [22/100], Step [840/1751], Loss: 1.2116\n",
      "Epoch [22/100], Step [850/1751], Loss: 1.2601\n",
      "Epoch [22/100], Step [860/1751], Loss: 1.3292\n",
      "Epoch [22/100], Step [870/1751], Loss: 1.3484\n",
      "Epoch [22/100], Step [880/1751], Loss: 1.3467\n",
      "Epoch [22/100], Step [890/1751], Loss: 1.2553\n",
      "Epoch [22/100], Step [900/1751], Loss: 1.1892\n",
      "Epoch [22/100], Step [910/1751], Loss: 1.3267\n",
      "Epoch [22/100], Step [920/1751], Loss: 1.1840\n",
      "Epoch [22/100], Step [930/1751], Loss: 1.1995\n",
      "Epoch [22/100], Step [940/1751], Loss: 1.3396\n",
      "Epoch [22/100], Step [950/1751], Loss: 1.2043\n",
      "Epoch [22/100], Step [960/1751], Loss: 1.3809\n",
      "Epoch [22/100], Step [970/1751], Loss: 1.4206\n",
      "Epoch [22/100], Step [980/1751], Loss: 1.3634\n",
      "Epoch [22/100], Step [990/1751], Loss: 1.4953\n",
      "Epoch [22/100], Step [1000/1751], Loss: 1.4152\n",
      "Epoch [22/100], Step [1010/1751], Loss: 1.0941\n",
      "Epoch [22/100], Step [1020/1751], Loss: 1.2081\n",
      "Epoch [22/100], Step [1030/1751], Loss: 1.2185\n",
      "Epoch [22/100], Step [1040/1751], Loss: 1.1960\n",
      "Epoch [22/100], Step [1050/1751], Loss: 1.3343\n",
      "Epoch [22/100], Step [1060/1751], Loss: 1.3492\n",
      "Epoch [22/100], Step [1070/1751], Loss: 1.2835\n",
      "Epoch [22/100], Step [1080/1751], Loss: 1.3619\n",
      "Epoch [22/100], Step [1090/1751], Loss: 1.3498\n",
      "Epoch [22/100], Step [1100/1751], Loss: 1.3194\n",
      "Epoch [22/100], Step [1110/1751], Loss: 1.2654\n",
      "Epoch [22/100], Step [1120/1751], Loss: 1.2794\n",
      "Epoch [22/100], Step [1130/1751], Loss: 1.2508\n",
      "Epoch [22/100], Step [1140/1751], Loss: 1.3103\n",
      "Epoch [22/100], Step [1150/1751], Loss: 1.2856\n",
      "Epoch [22/100], Step [1160/1751], Loss: 1.2917\n",
      "Epoch [22/100], Step [1170/1751], Loss: 1.4430\n",
      "Epoch [22/100], Step [1180/1751], Loss: 1.3499\n",
      "Epoch [22/100], Step [1190/1751], Loss: 1.2420\n",
      "Epoch [22/100], Step [1200/1751], Loss: 1.2139\n",
      "Epoch [22/100], Step [1210/1751], Loss: 1.3080\n",
      "Epoch [22/100], Step [1220/1751], Loss: 1.3111\n",
      "Epoch [22/100], Step [1230/1751], Loss: 1.1745\n",
      "Epoch [22/100], Step [1240/1751], Loss: 1.3516\n",
      "Epoch [22/100], Step [1250/1751], Loss: 1.3076\n",
      "Epoch [22/100], Step [1260/1751], Loss: 1.3654\n",
      "Epoch [22/100], Step [1270/1751], Loss: 1.2437\n",
      "Epoch [22/100], Step [1280/1751], Loss: 1.1343\n",
      "Epoch [22/100], Step [1290/1751], Loss: 1.2075\n",
      "Epoch [22/100], Step [1300/1751], Loss: 1.1407\n",
      "Epoch [22/100], Step [1310/1751], Loss: 1.5251\n",
      "Epoch [22/100], Step [1320/1751], Loss: 1.2493\n",
      "Epoch [22/100], Step [1330/1751], Loss: 1.2876\n",
      "Epoch [22/100], Step [1340/1751], Loss: 1.2988\n",
      "Epoch [22/100], Step [1350/1751], Loss: 1.4315\n",
      "Epoch [22/100], Step [1360/1751], Loss: 1.2382\n",
      "Epoch [22/100], Step [1370/1751], Loss: 1.4007\n",
      "Epoch [22/100], Step [1380/1751], Loss: 1.3496\n",
      "Epoch [22/100], Step [1390/1751], Loss: 1.4188\n",
      "Epoch [22/100], Step [1400/1751], Loss: 1.2248\n",
      "Epoch [22/100], Step [1410/1751], Loss: 1.3022\n",
      "Epoch [22/100], Step [1420/1751], Loss: 1.3544\n",
      "Epoch [22/100], Step [1430/1751], Loss: 1.2338\n",
      "Epoch [22/100], Step [1440/1751], Loss: 1.2460\n",
      "Epoch [22/100], Step [1450/1751], Loss: 1.1654\n",
      "Epoch [22/100], Step [1460/1751], Loss: 1.2724\n",
      "Epoch [22/100], Step [1470/1751], Loss: 1.2973\n",
      "Epoch [22/100], Step [1480/1751], Loss: 1.2033\n",
      "Epoch [22/100], Step [1490/1751], Loss: 1.3013\n",
      "Epoch [22/100], Step [1500/1751], Loss: 1.2836\n",
      "Epoch [22/100], Step [1510/1751], Loss: 1.3027\n",
      "Epoch [22/100], Step [1520/1751], Loss: 1.0860\n",
      "Epoch [22/100], Step [1530/1751], Loss: 1.2343\n",
      "Epoch [22/100], Step [1540/1751], Loss: 1.2862\n",
      "Epoch [22/100], Step [1550/1751], Loss: 1.1682\n",
      "Epoch [22/100], Step [1560/1751], Loss: 1.0815\n",
      "Epoch [22/100], Step [1570/1751], Loss: 1.1681\n",
      "Epoch [22/100], Step [1580/1751], Loss: 1.2936\n",
      "Epoch [22/100], Step [1590/1751], Loss: 1.2014\n",
      "Epoch [22/100], Step [1600/1751], Loss: 1.4544\n",
      "Epoch [22/100], Step [1610/1751], Loss: 1.3266\n",
      "Epoch [22/100], Step [1620/1751], Loss: 1.1785\n",
      "Epoch [22/100], Step [1630/1751], Loss: 1.3729\n",
      "Epoch [22/100], Step [1640/1751], Loss: 1.1388\n",
      "Epoch [22/100], Step [1650/1751], Loss: 1.1632\n",
      "Epoch [22/100], Step [1660/1751], Loss: 1.1899\n",
      "Epoch [22/100], Step [1670/1751], Loss: 1.2626\n",
      "Epoch [22/100], Step [1680/1751], Loss: 1.1588\n",
      "Epoch [22/100], Step [1690/1751], Loss: 1.2261\n",
      "Epoch [22/100], Step [1700/1751], Loss: 1.1708\n",
      "Epoch [22/100], Step [1710/1751], Loss: 1.2759\n",
      "Epoch [22/100], Step [1720/1751], Loss: 1.2292\n",
      "Epoch [22/100], Step [1730/1751], Loss: 1.3044\n",
      "Epoch [22/100], Step [1740/1751], Loss: 1.3914\n",
      "Epoch [22/100], Step [1750/1751], Loss: 1.2289\n",
      "Epoch [22/100], Average Loss: 1.2719, Time: 1638.2835s\n",
      "Epoch [23/100], Step [10/1751], Loss: 1.2451\n",
      "Epoch [23/100], Step [20/1751], Loss: 1.3997\n",
      "Epoch [23/100], Step [30/1751], Loss: 1.2328\n",
      "Epoch [23/100], Step [40/1751], Loss: 1.2614\n",
      "Epoch [23/100], Step [50/1751], Loss: 1.1860\n",
      "Epoch [23/100], Step [60/1751], Loss: 1.2518\n",
      "Epoch [23/100], Step [70/1751], Loss: 1.3788\n",
      "Epoch [23/100], Step [80/1751], Loss: 1.1668\n",
      "Epoch [23/100], Step [90/1751], Loss: 1.1998\n",
      "Epoch [23/100], Step [100/1751], Loss: 1.2788\n",
      "Epoch [23/100], Step [110/1751], Loss: 1.0989\n",
      "Epoch [23/100], Step [120/1751], Loss: 1.3048\n",
      "Epoch [23/100], Step [130/1751], Loss: 1.3464\n",
      "Epoch [23/100], Step [140/1751], Loss: 1.3114\n",
      "Epoch [23/100], Step [150/1751], Loss: 1.0808\n",
      "Epoch [23/100], Step [160/1751], Loss: 1.2282\n",
      "Epoch [23/100], Step [170/1751], Loss: 1.2935\n",
      "Epoch [23/100], Step [180/1751], Loss: 1.4154\n",
      "Epoch [23/100], Step [190/1751], Loss: 1.3423\n",
      "Epoch [23/100], Step [200/1751], Loss: 1.0621\n",
      "Epoch [23/100], Step [210/1751], Loss: 1.3918\n",
      "Epoch [23/100], Step [220/1751], Loss: 1.3152\n",
      "Epoch [23/100], Step [230/1751], Loss: 1.2789\n",
      "Epoch [23/100], Step [240/1751], Loss: 1.1715\n",
      "Epoch [23/100], Step [250/1751], Loss: 1.1170\n",
      "Epoch [23/100], Step [260/1751], Loss: 1.1915\n",
      "Epoch [23/100], Step [270/1751], Loss: 1.4089\n",
      "Epoch [23/100], Step [280/1751], Loss: 1.1877\n",
      "Epoch [23/100], Step [290/1751], Loss: 1.2602\n",
      "Epoch [23/100], Step [300/1751], Loss: 1.3646\n",
      "Epoch [23/100], Step [310/1751], Loss: 1.0837\n",
      "Epoch [23/100], Step [320/1751], Loss: 1.4044\n",
      "Epoch [23/100], Step [330/1751], Loss: 1.3416\n",
      "Epoch [23/100], Step [340/1751], Loss: 1.2807\n",
      "Epoch [23/100], Step [350/1751], Loss: 1.1870\n",
      "Epoch [23/100], Step [360/1751], Loss: 1.3872\n",
      "Epoch [23/100], Step [370/1751], Loss: 1.1598\n",
      "Epoch [23/100], Step [380/1751], Loss: 1.3229\n",
      "Epoch [23/100], Step [390/1751], Loss: 1.3791\n",
      "Epoch [23/100], Step [400/1751], Loss: 1.3478\n",
      "Epoch [23/100], Step [410/1751], Loss: 1.2861\n",
      "Epoch [23/100], Step [420/1751], Loss: 1.2823\n",
      "Epoch [23/100], Step [430/1751], Loss: 1.2582\n",
      "Epoch [23/100], Step [440/1751], Loss: 1.1905\n",
      "Epoch [23/100], Step [450/1751], Loss: 1.2745\n",
      "Epoch [23/100], Step [460/1751], Loss: 1.2273\n",
      "Epoch [23/100], Step [470/1751], Loss: 1.2411\n",
      "Epoch [23/100], Step [480/1751], Loss: 1.2536\n",
      "Epoch [23/100], Step [490/1751], Loss: 1.2554\n",
      "Epoch [23/100], Step [500/1751], Loss: 1.4114\n",
      "Epoch [23/100], Step [510/1751], Loss: 1.2992\n",
      "Epoch [23/100], Step [520/1751], Loss: 1.4690\n",
      "Epoch [23/100], Step [530/1751], Loss: 1.1488\n",
      "Epoch [23/100], Step [540/1751], Loss: 1.2831\n",
      "Epoch [23/100], Step [550/1751], Loss: 1.3210\n",
      "Epoch [23/100], Step [560/1751], Loss: 1.1797\n",
      "Epoch [23/100], Step [570/1751], Loss: 1.1407\n",
      "Epoch [23/100], Step [580/1751], Loss: 1.2491\n",
      "Epoch [23/100], Step [590/1751], Loss: 1.2315\n",
      "Epoch [23/100], Step [600/1751], Loss: 1.2573\n",
      "Epoch [23/100], Step [610/1751], Loss: 1.4156\n",
      "Epoch [23/100], Step [620/1751], Loss: 1.3393\n",
      "Epoch [23/100], Step [630/1751], Loss: 1.2490\n",
      "Epoch [23/100], Step [640/1751], Loss: 1.1157\n",
      "Epoch [23/100], Step [650/1751], Loss: 1.3216\n",
      "Epoch [23/100], Step [660/1751], Loss: 1.3766\n",
      "Epoch [23/100], Step [670/1751], Loss: 1.0630\n",
      "Epoch [23/100], Step [680/1751], Loss: 1.1433\n",
      "Epoch [23/100], Step [690/1751], Loss: 1.1958\n",
      "Epoch [23/100], Step [700/1751], Loss: 1.2546\n",
      "Epoch [23/100], Step [710/1751], Loss: 1.2578\n",
      "Epoch [23/100], Step [720/1751], Loss: 1.2396\n",
      "Epoch [23/100], Step [730/1751], Loss: 1.3745\n",
      "Epoch [23/100], Step [740/1751], Loss: 1.2330\n",
      "Epoch [23/100], Step [750/1751], Loss: 1.3460\n",
      "Epoch [23/100], Step [760/1751], Loss: 1.2727\n",
      "Epoch [23/100], Step [770/1751], Loss: 1.2540\n",
      "Epoch [23/100], Step [780/1751], Loss: 1.2341\n",
      "Epoch [23/100], Step [790/1751], Loss: 1.0882\n",
      "Epoch [23/100], Step [800/1751], Loss: 1.2467\n",
      "Epoch [23/100], Step [810/1751], Loss: 1.2652\n",
      "Epoch [23/100], Step [820/1751], Loss: 1.2120\n",
      "Epoch [23/100], Step [830/1751], Loss: 1.2257\n",
      "Epoch [23/100], Step [840/1751], Loss: 1.2429\n",
      "Epoch [23/100], Step [850/1751], Loss: 1.2643\n",
      "Epoch [23/100], Step [860/1751], Loss: 1.1892\n",
      "Epoch [23/100], Step [870/1751], Loss: 1.4386\n",
      "Epoch [23/100], Step [880/1751], Loss: 1.2534\n",
      "Epoch [23/100], Step [890/1751], Loss: 1.4045\n",
      "Epoch [23/100], Step [900/1751], Loss: 1.3447\n",
      "Epoch [23/100], Step [910/1751], Loss: 1.2492\n",
      "Epoch [23/100], Step [920/1751], Loss: 1.2372\n",
      "Epoch [23/100], Step [930/1751], Loss: 1.3584\n",
      "Epoch [23/100], Step [940/1751], Loss: 1.2476\n",
      "Epoch [23/100], Step [950/1751], Loss: 1.1959\n",
      "Epoch [23/100], Step [960/1751], Loss: 1.2583\n",
      "Epoch [23/100], Step [970/1751], Loss: 1.1765\n",
      "Epoch [23/100], Step [980/1751], Loss: 1.1402\n",
      "Epoch [23/100], Step [990/1751], Loss: 1.2394\n",
      "Epoch [23/100], Step [1000/1751], Loss: 1.2467\n",
      "Epoch [23/100], Step [1010/1751], Loss: 1.2287\n",
      "Epoch [23/100], Step [1020/1751], Loss: 1.1992\n",
      "Epoch [23/100], Step [1030/1751], Loss: 1.2122\n",
      "Epoch [23/100], Step [1040/1751], Loss: 1.2654\n",
      "Epoch [23/100], Step [1050/1751], Loss: 1.4178\n",
      "Epoch [23/100], Step [1060/1751], Loss: 1.2555\n",
      "Epoch [23/100], Step [1070/1751], Loss: 1.1931\n",
      "Epoch [23/100], Step [1080/1751], Loss: 1.2720\n",
      "Epoch [23/100], Step [1090/1751], Loss: 1.3725\n",
      "Epoch [23/100], Step [1100/1751], Loss: 1.3204\n",
      "Epoch [23/100], Step [1110/1751], Loss: 1.3425\n",
      "Epoch [23/100], Step [1120/1751], Loss: 1.2275\n",
      "Epoch [23/100], Step [1130/1751], Loss: 1.1754\n",
      "Epoch [23/100], Step [1140/1751], Loss: 1.2481\n",
      "Epoch [23/100], Step [1150/1751], Loss: 1.1921\n",
      "Epoch [23/100], Step [1160/1751], Loss: 1.2299\n",
      "Epoch [23/100], Step [1170/1751], Loss: 1.1646\n",
      "Epoch [23/100], Step [1180/1751], Loss: 1.3281\n",
      "Epoch [23/100], Step [1190/1751], Loss: 1.3060\n",
      "Epoch [23/100], Step [1200/1751], Loss: 1.3978\n",
      "Epoch [23/100], Step [1210/1751], Loss: 1.3334\n",
      "Epoch [23/100], Step [1220/1751], Loss: 1.2783\n",
      "Epoch [23/100], Step [1230/1751], Loss: 1.2194\n",
      "Epoch [23/100], Step [1240/1751], Loss: 1.3214\n",
      "Epoch [23/100], Step [1250/1751], Loss: 1.3183\n",
      "Epoch [23/100], Step [1260/1751], Loss: 1.4284\n",
      "Epoch [23/100], Step [1270/1751], Loss: 1.2841\n",
      "Epoch [23/100], Step [1280/1751], Loss: 1.3878\n",
      "Epoch [23/100], Step [1290/1751], Loss: 1.2531\n",
      "Epoch [23/100], Step [1300/1751], Loss: 1.2983\n",
      "Epoch [23/100], Step [1310/1751], Loss: 1.4769\n",
      "Epoch [23/100], Step [1320/1751], Loss: 1.2742\n",
      "Epoch [23/100], Step [1330/1751], Loss: 1.1381\n",
      "Epoch [23/100], Step [1340/1751], Loss: 1.2437\n",
      "Epoch [23/100], Step [1350/1751], Loss: 1.3289\n",
      "Epoch [23/100], Step [1360/1751], Loss: 1.3178\n",
      "Epoch [23/100], Step [1370/1751], Loss: 1.4029\n",
      "Epoch [23/100], Step [1380/1751], Loss: 1.2384\n",
      "Epoch [23/100], Step [1390/1751], Loss: 1.4124\n",
      "Epoch [23/100], Step [1400/1751], Loss: 1.3907\n",
      "Epoch [23/100], Step [1410/1751], Loss: 1.1676\n",
      "Epoch [23/100], Step [1420/1751], Loss: 1.2186\n",
      "Epoch [23/100], Step [1430/1751], Loss: 1.2599\n",
      "Epoch [23/100], Step [1440/1751], Loss: 1.3222\n",
      "Epoch [23/100], Step [1450/1751], Loss: 1.3702\n",
      "Epoch [23/100], Step [1460/1751], Loss: 1.1887\n",
      "Epoch [23/100], Step [1470/1751], Loss: 1.3631\n",
      "Epoch [23/100], Step [1480/1751], Loss: 1.0077\n",
      "Epoch [23/100], Step [1490/1751], Loss: 1.2466\n",
      "Epoch [23/100], Step [1500/1751], Loss: 1.3108\n",
      "Epoch [23/100], Step [1510/1751], Loss: 1.2714\n",
      "Epoch [23/100], Step [1520/1751], Loss: 1.3889\n",
      "Epoch [23/100], Step [1530/1751], Loss: 1.3096\n",
      "Epoch [23/100], Step [1540/1751], Loss: 1.1023\n",
      "Epoch [23/100], Step [1550/1751], Loss: 1.3089\n",
      "Epoch [23/100], Step [1560/1751], Loss: 1.2722\n",
      "Epoch [23/100], Step [1570/1751], Loss: 1.2063\n",
      "Epoch [23/100], Step [1580/1751], Loss: 1.3320\n",
      "Epoch [23/100], Step [1590/1751], Loss: 1.3384\n",
      "Epoch [23/100], Step [1600/1751], Loss: 1.2182\n",
      "Epoch [23/100], Step [1610/1751], Loss: 1.2180\n",
      "Epoch [23/100], Step [1620/1751], Loss: 1.1851\n",
      "Epoch [23/100], Step [1630/1751], Loss: 1.2848\n",
      "Epoch [23/100], Step [1640/1751], Loss: 1.2366\n",
      "Epoch [23/100], Step [1650/1751], Loss: 1.1921\n",
      "Epoch [23/100], Step [1660/1751], Loss: 1.3916\n",
      "Epoch [23/100], Step [1670/1751], Loss: 1.3834\n",
      "Epoch [23/100], Step [1680/1751], Loss: 1.2718\n",
      "Epoch [23/100], Step [1690/1751], Loss: 1.3935\n",
      "Epoch [23/100], Step [1700/1751], Loss: 1.1833\n",
      "Epoch [23/100], Step [1710/1751], Loss: 1.3821\n",
      "Epoch [23/100], Step [1720/1751], Loss: 1.2882\n",
      "Epoch [23/100], Step [1730/1751], Loss: 1.3029\n",
      "Epoch [23/100], Step [1740/1751], Loss: 1.2919\n",
      "Epoch [23/100], Step [1750/1751], Loss: 1.2285\n",
      "Epoch [23/100], Average Loss: 1.2672, Time: 1637.1205s\n",
      "Epoch [24/100], Step [10/1751], Loss: 1.3445\n",
      "Epoch [24/100], Step [20/1751], Loss: 1.2950\n",
      "Epoch [24/100], Step [30/1751], Loss: 1.2803\n",
      "Epoch [24/100], Step [40/1751], Loss: 1.3800\n",
      "Epoch [24/100], Step [50/1751], Loss: 1.2529\n",
      "Epoch [24/100], Step [60/1751], Loss: 1.3810\n",
      "Epoch [24/100], Step [70/1751], Loss: 1.3718\n",
      "Epoch [24/100], Step [80/1751], Loss: 1.1596\n",
      "Epoch [24/100], Step [90/1751], Loss: 1.3711\n",
      "Epoch [24/100], Step [100/1751], Loss: 1.2266\n",
      "Epoch [24/100], Step [110/1751], Loss: 1.2077\n",
      "Epoch [24/100], Step [120/1751], Loss: 1.1794\n",
      "Epoch [24/100], Step [130/1751], Loss: 1.3325\n",
      "Epoch [24/100], Step [140/1751], Loss: 1.2460\n",
      "Epoch [24/100], Step [150/1751], Loss: 1.3434\n",
      "Epoch [24/100], Step [160/1751], Loss: 1.2580\n",
      "Epoch [24/100], Step [170/1751], Loss: 1.3759\n",
      "Epoch [24/100], Step [180/1751], Loss: 1.1534\n",
      "Epoch [24/100], Step [190/1751], Loss: 1.3021\n",
      "Epoch [24/100], Step [200/1751], Loss: 1.2347\n",
      "Epoch [24/100], Step [210/1751], Loss: 1.1458\n",
      "Epoch [24/100], Step [220/1751], Loss: 1.1713\n",
      "Epoch [24/100], Step [230/1751], Loss: 1.2000\n",
      "Epoch [24/100], Step [240/1751], Loss: 1.3327\n",
      "Epoch [24/100], Step [250/1751], Loss: 1.2879\n",
      "Epoch [24/100], Step [260/1751], Loss: 1.2320\n",
      "Epoch [24/100], Step [270/1751], Loss: 1.2287\n",
      "Epoch [24/100], Step [280/1751], Loss: 1.3842\n",
      "Epoch [24/100], Step [290/1751], Loss: 1.2870\n",
      "Epoch [24/100], Step [300/1751], Loss: 1.3598\n",
      "Epoch [24/100], Step [310/1751], Loss: 1.1335\n",
      "Epoch [24/100], Step [320/1751], Loss: 1.1955\n",
      "Epoch [24/100], Step [330/1751], Loss: 1.1625\n",
      "Epoch [24/100], Step [340/1751], Loss: 1.3533\n",
      "Epoch [24/100], Step [350/1751], Loss: 1.2884\n",
      "Epoch [24/100], Step [360/1751], Loss: 1.3689\n",
      "Epoch [24/100], Step [370/1751], Loss: 1.2722\n",
      "Epoch [24/100], Step [380/1751], Loss: 1.3356\n",
      "Epoch [24/100], Step [390/1751], Loss: 1.1328\n",
      "Epoch [24/100], Step [400/1751], Loss: 1.1664\n",
      "Epoch [24/100], Step [410/1751], Loss: 1.2404\n",
      "Epoch [24/100], Step [420/1751], Loss: 1.1203\n",
      "Epoch [24/100], Step [430/1751], Loss: 1.1997\n",
      "Epoch [24/100], Step [440/1751], Loss: 1.1907\n",
      "Epoch [24/100], Step [450/1751], Loss: 1.2313\n",
      "Epoch [24/100], Step [460/1751], Loss: 1.2403\n",
      "Epoch [24/100], Step [470/1751], Loss: 1.1394\n",
      "Epoch [24/100], Step [480/1751], Loss: 1.2682\n",
      "Epoch [24/100], Step [490/1751], Loss: 1.2291\n",
      "Epoch [24/100], Step [500/1751], Loss: 1.2544\n",
      "Epoch [24/100], Step [510/1751], Loss: 1.2126\n",
      "Epoch [24/100], Step [520/1751], Loss: 1.1032\n",
      "Epoch [24/100], Step [530/1751], Loss: 1.2293\n",
      "Epoch [24/100], Step [540/1751], Loss: 1.2887\n",
      "Epoch [24/100], Step [550/1751], Loss: 1.3743\n",
      "Epoch [24/100], Step [560/1751], Loss: 1.3052\n",
      "Epoch [24/100], Step [570/1751], Loss: 1.2999\n",
      "Epoch [24/100], Step [580/1751], Loss: 1.3247\n",
      "Epoch [24/100], Step [590/1751], Loss: 1.1420\n",
      "Epoch [24/100], Step [600/1751], Loss: 1.4409\n",
      "Epoch [24/100], Step [610/1751], Loss: 1.4513\n",
      "Epoch [24/100], Step [620/1751], Loss: 1.1308\n",
      "Epoch [24/100], Step [630/1751], Loss: 1.2692\n",
      "Epoch [24/100], Step [640/1751], Loss: 1.3037\n",
      "Epoch [24/100], Step [650/1751], Loss: 1.3285\n",
      "Epoch [24/100], Step [660/1751], Loss: 1.4011\n",
      "Epoch [24/100], Step [670/1751], Loss: 1.1896\n",
      "Epoch [24/100], Step [680/1751], Loss: 1.3266\n",
      "Epoch [24/100], Step [690/1751], Loss: 1.3043\n",
      "Epoch [24/100], Step [700/1751], Loss: 1.2093\n",
      "Epoch [24/100], Step [710/1751], Loss: 1.3849\n",
      "Epoch [24/100], Step [720/1751], Loss: 1.5253\n",
      "Epoch [24/100], Step [730/1751], Loss: 1.2713\n",
      "Epoch [24/100], Step [740/1751], Loss: 1.2456\n",
      "Epoch [24/100], Step [750/1751], Loss: 1.2701\n",
      "Epoch [24/100], Step [760/1751], Loss: 1.3321\n",
      "Epoch [24/100], Step [770/1751], Loss: 1.2689\n",
      "Epoch [24/100], Step [780/1751], Loss: 1.2503\n",
      "Epoch [24/100], Step [790/1751], Loss: 1.2679\n",
      "Epoch [24/100], Step [800/1751], Loss: 1.3704\n",
      "Epoch [24/100], Step [810/1751], Loss: 1.3570\n",
      "Epoch [24/100], Step [820/1751], Loss: 1.2438\n",
      "Epoch [24/100], Step [830/1751], Loss: 1.3224\n",
      "Epoch [24/100], Step [840/1751], Loss: 1.2072\n",
      "Epoch [24/100], Step [850/1751], Loss: 1.2774\n",
      "Epoch [24/100], Step [860/1751], Loss: 1.0424\n",
      "Epoch [24/100], Step [870/1751], Loss: 1.3234\n",
      "Epoch [24/100], Step [880/1751], Loss: 1.3842\n",
      "Epoch [24/100], Step [890/1751], Loss: 1.1148\n",
      "Epoch [24/100], Step [900/1751], Loss: 1.1687\n",
      "Epoch [24/100], Step [910/1751], Loss: 1.3289\n",
      "Epoch [24/100], Step [920/1751], Loss: 1.2995\n",
      "Epoch [24/100], Step [930/1751], Loss: 1.2492\n",
      "Epoch [24/100], Step [940/1751], Loss: 1.2615\n",
      "Epoch [24/100], Step [950/1751], Loss: 1.2898\n",
      "Epoch [24/100], Step [960/1751], Loss: 1.3657\n",
      "Epoch [24/100], Step [970/1751], Loss: 1.1708\n",
      "Epoch [24/100], Step [980/1751], Loss: 1.1267\n",
      "Epoch [24/100], Step [990/1751], Loss: 1.3159\n",
      "Epoch [24/100], Step [1000/1751], Loss: 1.1586\n",
      "Epoch [24/100], Step [1010/1751], Loss: 1.1058\n",
      "Epoch [24/100], Step [1020/1751], Loss: 1.2817\n",
      "Epoch [24/100], Step [1030/1751], Loss: 1.2338\n",
      "Epoch [24/100], Step [1040/1751], Loss: 1.2804\n",
      "Epoch [24/100], Step [1050/1751], Loss: 1.3233\n",
      "Epoch [24/100], Step [1060/1751], Loss: 1.1205\n",
      "Epoch [24/100], Step [1070/1751], Loss: 1.2706\n",
      "Epoch [24/100], Step [1080/1751], Loss: 1.1881\n",
      "Epoch [24/100], Step [1090/1751], Loss: 1.2783\n",
      "Epoch [24/100], Step [1100/1751], Loss: 1.3466\n",
      "Epoch [24/100], Step [1110/1751], Loss: 1.2950\n",
      "Epoch [24/100], Step [1120/1751], Loss: 1.2842\n",
      "Epoch [24/100], Step [1130/1751], Loss: 1.2702\n",
      "Epoch [24/100], Step [1140/1751], Loss: 1.3763\n",
      "Epoch [24/100], Step [1150/1751], Loss: 1.2083\n",
      "Epoch [24/100], Step [1160/1751], Loss: 1.2708\n",
      "Epoch [24/100], Step [1170/1751], Loss: 1.3693\n",
      "Epoch [24/100], Step [1180/1751], Loss: 1.3389\n",
      "Epoch [24/100], Step [1190/1751], Loss: 1.3546\n",
      "Epoch [24/100], Step [1200/1751], Loss: 1.1133\n",
      "Epoch [24/100], Step [1210/1751], Loss: 1.2571\n",
      "Epoch [24/100], Step [1220/1751], Loss: 1.4052\n",
      "Epoch [24/100], Step [1230/1751], Loss: 1.2212\n",
      "Epoch [24/100], Step [1240/1751], Loss: 1.1492\n",
      "Epoch [24/100], Step [1250/1751], Loss: 1.3177\n",
      "Epoch [24/100], Step [1260/1751], Loss: 1.4649\n",
      "Epoch [24/100], Step [1270/1751], Loss: 1.1820\n",
      "Epoch [24/100], Step [1280/1751], Loss: 1.1389\n",
      "Epoch [24/100], Step [1290/1751], Loss: 1.2868\n",
      "Epoch [24/100], Step [1300/1751], Loss: 1.2346\n",
      "Epoch [24/100], Step [1310/1751], Loss: 1.2693\n",
      "Epoch [24/100], Step [1320/1751], Loss: 1.0876\n",
      "Epoch [24/100], Step [1330/1751], Loss: 1.1859\n",
      "Epoch [24/100], Step [1340/1751], Loss: 1.2697\n",
      "Epoch [24/100], Step [1350/1751], Loss: 1.2411\n",
      "Epoch [24/100], Step [1360/1751], Loss: 1.2025\n",
      "Epoch [24/100], Step [1370/1751], Loss: 1.1619\n",
      "Epoch [24/100], Step [1380/1751], Loss: 1.3280\n",
      "Epoch [24/100], Step [1390/1751], Loss: 1.3597\n",
      "Epoch [24/100], Step [1400/1751], Loss: 1.4303\n",
      "Epoch [24/100], Step [1410/1751], Loss: 1.2637\n",
      "Epoch [24/100], Step [1420/1751], Loss: 1.2537\n",
      "Epoch [24/100], Step [1430/1751], Loss: 1.3324\n",
      "Epoch [24/100], Step [1440/1751], Loss: 1.3086\n",
      "Epoch [24/100], Step [1450/1751], Loss: 1.2107\n",
      "Epoch [24/100], Step [1460/1751], Loss: 1.2104\n",
      "Epoch [24/100], Step [1470/1751], Loss: 1.3505\n",
      "Epoch [24/100], Step [1480/1751], Loss: 1.1958\n",
      "Epoch [24/100], Step [1490/1751], Loss: 1.1678\n",
      "Epoch [24/100], Step [1500/1751], Loss: 1.2395\n",
      "Epoch [24/100], Step [1510/1751], Loss: 1.2015\n",
      "Epoch [24/100], Step [1520/1751], Loss: 1.3964\n",
      "Epoch [24/100], Step [1530/1751], Loss: 1.0935\n",
      "Epoch [24/100], Step [1540/1751], Loss: 1.2812\n",
      "Epoch [24/100], Step [1550/1751], Loss: 1.2552\n",
      "Epoch [24/100], Step [1560/1751], Loss: 1.2826\n",
      "Epoch [24/100], Step [1570/1751], Loss: 1.1441\n",
      "Epoch [24/100], Step [1580/1751], Loss: 1.3778\n",
      "Epoch [24/100], Step [1590/1751], Loss: 1.4481\n",
      "Epoch [24/100], Step [1600/1751], Loss: 1.1964\n",
      "Epoch [24/100], Step [1610/1751], Loss: 1.2299\n",
      "Epoch [24/100], Step [1620/1751], Loss: 1.2834\n",
      "Epoch [24/100], Step [1630/1751], Loss: 1.2148\n",
      "Epoch [24/100], Step [1640/1751], Loss: 1.2215\n",
      "Epoch [24/100], Step [1650/1751], Loss: 1.1730\n",
      "Epoch [24/100], Step [1660/1751], Loss: 1.2529\n",
      "Epoch [24/100], Step [1670/1751], Loss: 1.1853\n",
      "Epoch [24/100], Step [1680/1751], Loss: 1.2730\n",
      "Epoch [24/100], Step [1690/1751], Loss: 1.3039\n",
      "Epoch [24/100], Step [1700/1751], Loss: 1.3310\n",
      "Epoch [24/100], Step [1710/1751], Loss: 1.0053\n",
      "Epoch [24/100], Step [1720/1751], Loss: 1.3161\n",
      "Epoch [24/100], Step [1730/1751], Loss: 1.2249\n",
      "Epoch [24/100], Step [1740/1751], Loss: 1.0839\n",
      "Epoch [24/100], Step [1750/1751], Loss: 1.4348\n",
      "Epoch [24/100], Average Loss: 1.2624, Time: 1638.1449s\n",
      "Epoch [25/100], Step [10/1751], Loss: 1.4662\n",
      "Epoch [25/100], Step [20/1751], Loss: 1.3718\n",
      "Epoch [25/100], Step [30/1751], Loss: 1.3516\n",
      "Epoch [25/100], Step [40/1751], Loss: 1.2635\n",
      "Epoch [25/100], Step [50/1751], Loss: 1.3316\n",
      "Epoch [25/100], Step [60/1751], Loss: 1.2712\n",
      "Epoch [25/100], Step [70/1751], Loss: 1.2340\n",
      "Epoch [25/100], Step [80/1751], Loss: 1.3027\n",
      "Epoch [25/100], Step [90/1751], Loss: 1.2602\n",
      "Epoch [25/100], Step [100/1751], Loss: 1.2564\n",
      "Epoch [25/100], Step [110/1751], Loss: 1.2514\n",
      "Epoch [25/100], Step [120/1751], Loss: 1.3060\n",
      "Epoch [25/100], Step [130/1751], Loss: 1.1691\n",
      "Epoch [25/100], Step [140/1751], Loss: 1.3769\n",
      "Epoch [25/100], Step [150/1751], Loss: 1.0723\n",
      "Epoch [25/100], Step [160/1751], Loss: 1.2780\n",
      "Epoch [25/100], Step [170/1751], Loss: 1.2796\n",
      "Epoch [25/100], Step [180/1751], Loss: 1.3228\n",
      "Epoch [25/100], Step [190/1751], Loss: 1.3475\n",
      "Epoch [25/100], Step [200/1751], Loss: 1.2084\n",
      "Epoch [25/100], Step [210/1751], Loss: 1.2785\n",
      "Epoch [25/100], Step [220/1751], Loss: 1.2037\n",
      "Epoch [25/100], Step [230/1751], Loss: 1.2270\n",
      "Epoch [25/100], Step [240/1751], Loss: 1.4323\n",
      "Epoch [25/100], Step [250/1751], Loss: 1.3491\n",
      "Epoch [25/100], Step [260/1751], Loss: 1.1659\n",
      "Epoch [25/100], Step [270/1751], Loss: 1.2949\n",
      "Epoch [25/100], Step [280/1751], Loss: 1.2701\n",
      "Epoch [25/100], Step [290/1751], Loss: 1.2426\n",
      "Epoch [25/100], Step [300/1751], Loss: 1.0809\n",
      "Epoch [25/100], Step [310/1751], Loss: 1.2478\n",
      "Epoch [25/100], Step [320/1751], Loss: 1.2640\n",
      "Epoch [25/100], Step [330/1751], Loss: 1.2224\n",
      "Epoch [25/100], Step [340/1751], Loss: 1.2667\n",
      "Epoch [25/100], Step [350/1751], Loss: 1.0582\n",
      "Epoch [25/100], Step [360/1751], Loss: 1.1434\n",
      "Epoch [25/100], Step [370/1751], Loss: 1.2334\n",
      "Epoch [25/100], Step [380/1751], Loss: 1.3946\n",
      "Epoch [25/100], Step [390/1751], Loss: 1.2915\n",
      "Epoch [25/100], Step [400/1751], Loss: 1.2018\n",
      "Epoch [25/100], Step [410/1751], Loss: 1.1488\n",
      "Epoch [25/100], Step [420/1751], Loss: 1.2741\n",
      "Epoch [25/100], Step [430/1751], Loss: 1.2249\n",
      "Epoch [25/100], Step [440/1751], Loss: 1.2338\n",
      "Epoch [25/100], Step [450/1751], Loss: 1.0966\n",
      "Epoch [25/100], Step [460/1751], Loss: 1.1365\n",
      "Epoch [25/100], Step [470/1751], Loss: 1.2294\n",
      "Epoch [25/100], Step [480/1751], Loss: 1.4248\n",
      "Epoch [25/100], Step [490/1751], Loss: 1.1710\n",
      "Epoch [25/100], Step [500/1751], Loss: 1.2826\n",
      "Epoch [25/100], Step [510/1751], Loss: 1.2305\n",
      "Epoch [25/100], Step [520/1751], Loss: 1.3127\n",
      "Epoch [25/100], Step [530/1751], Loss: 1.2042\n",
      "Epoch [25/100], Step [540/1751], Loss: 1.2673\n",
      "Epoch [25/100], Step [550/1751], Loss: 1.2827\n",
      "Epoch [25/100], Step [560/1751], Loss: 1.2482\n",
      "Epoch [25/100], Step [570/1751], Loss: 1.2063\n",
      "Epoch [25/100], Step [580/1751], Loss: 1.4072\n",
      "Epoch [25/100], Step [590/1751], Loss: 1.3318\n",
      "Epoch [25/100], Step [600/1751], Loss: 1.2152\n",
      "Epoch [25/100], Step [610/1751], Loss: 1.1912\n",
      "Epoch [25/100], Step [620/1751], Loss: 1.3011\n",
      "Epoch [25/100], Step [630/1751], Loss: 1.2941\n",
      "Epoch [25/100], Step [640/1751], Loss: 1.2663\n",
      "Epoch [25/100], Step [650/1751], Loss: 1.1458\n",
      "Epoch [25/100], Step [660/1751], Loss: 1.2277\n",
      "Epoch [25/100], Step [670/1751], Loss: 1.3302\n",
      "Epoch [25/100], Step [680/1751], Loss: 1.3796\n",
      "Epoch [25/100], Step [690/1751], Loss: 1.2783\n",
      "Epoch [25/100], Step [700/1751], Loss: 1.1430\n",
      "Epoch [25/100], Step [710/1751], Loss: 1.2710\n",
      "Epoch [25/100], Step [720/1751], Loss: 1.2756\n",
      "Epoch [25/100], Step [730/1751], Loss: 1.3504\n",
      "Epoch [25/100], Step [740/1751], Loss: 1.1896\n",
      "Epoch [25/100], Step [750/1751], Loss: 1.2085\n",
      "Epoch [25/100], Step [760/1751], Loss: 1.3200\n",
      "Epoch [25/100], Step [770/1751], Loss: 1.4151\n",
      "Epoch [25/100], Step [780/1751], Loss: 1.2510\n",
      "Epoch [25/100], Step [790/1751], Loss: 1.2601\n",
      "Epoch [25/100], Step [800/1751], Loss: 1.3377\n",
      "Epoch [25/100], Step [810/1751], Loss: 1.3905\n",
      "Epoch [25/100], Step [820/1751], Loss: 1.2249\n",
      "Epoch [25/100], Step [830/1751], Loss: 1.1563\n",
      "Epoch [25/100], Step [840/1751], Loss: 1.3582\n",
      "Epoch [25/100], Step [850/1751], Loss: 1.0949\n",
      "Epoch [25/100], Step [860/1751], Loss: 1.2379\n",
      "Epoch [25/100], Step [870/1751], Loss: 1.2368\n",
      "Epoch [25/100], Step [880/1751], Loss: 1.2155\n",
      "Epoch [25/100], Step [890/1751], Loss: 1.3998\n",
      "Epoch [25/100], Step [900/1751], Loss: 1.1708\n",
      "Epoch [25/100], Step [910/1751], Loss: 1.2101\n",
      "Epoch [25/100], Step [920/1751], Loss: 1.2234\n",
      "Epoch [25/100], Step [930/1751], Loss: 1.2511\n",
      "Epoch [25/100], Step [940/1751], Loss: 1.3793\n",
      "Epoch [25/100], Step [950/1751], Loss: 1.3196\n",
      "Epoch [25/100], Step [960/1751], Loss: 1.3659\n",
      "Epoch [25/100], Step [970/1751], Loss: 1.3750\n",
      "Epoch [25/100], Step [980/1751], Loss: 1.1500\n",
      "Epoch [25/100], Step [990/1751], Loss: 1.2381\n",
      "Epoch [25/100], Step [1000/1751], Loss: 1.4176\n",
      "Epoch [25/100], Step [1010/1751], Loss: 1.3432\n",
      "Epoch [25/100], Step [1020/1751], Loss: 1.3246\n",
      "Epoch [25/100], Step [1030/1751], Loss: 1.2903\n",
      "Epoch [25/100], Step [1040/1751], Loss: 1.3318\n",
      "Epoch [25/100], Step [1050/1751], Loss: 1.2046\n",
      "Epoch [25/100], Step [1060/1751], Loss: 1.2597\n",
      "Epoch [25/100], Step [1070/1751], Loss: 1.3317\n",
      "Epoch [25/100], Step [1080/1751], Loss: 1.1619\n",
      "Epoch [25/100], Step [1090/1751], Loss: 1.2122\n",
      "Epoch [25/100], Step [1100/1751], Loss: 1.1493\n",
      "Epoch [25/100], Step [1110/1751], Loss: 1.1461\n",
      "Epoch [25/100], Step [1120/1751], Loss: 1.3629\n",
      "Epoch [25/100], Step [1130/1751], Loss: 1.3666\n",
      "Epoch [25/100], Step [1140/1751], Loss: 1.2940\n",
      "Epoch [25/100], Step [1150/1751], Loss: 1.1692\n",
      "Epoch [25/100], Step [1160/1751], Loss: 1.1253\n",
      "Epoch [25/100], Step [1170/1751], Loss: 1.3293\n",
      "Epoch [25/100], Step [1180/1751], Loss: 1.3252\n",
      "Epoch [25/100], Step [1190/1751], Loss: 1.2582\n",
      "Epoch [25/100], Step [1200/1751], Loss: 1.3208\n",
      "Epoch [25/100], Step [1210/1751], Loss: 1.2134\n",
      "Epoch [25/100], Step [1220/1751], Loss: 1.2472\n",
      "Epoch [25/100], Step [1230/1751], Loss: 1.3973\n",
      "Epoch [25/100], Step [1240/1751], Loss: 1.2774\n",
      "Epoch [25/100], Step [1250/1751], Loss: 1.1051\n",
      "Epoch [25/100], Step [1260/1751], Loss: 1.3226\n",
      "Epoch [25/100], Step [1270/1751], Loss: 1.1993\n",
      "Epoch [25/100], Step [1280/1751], Loss: 1.1594\n",
      "Epoch [25/100], Step [1290/1751], Loss: 1.1017\n",
      "Epoch [25/100], Step [1300/1751], Loss: 1.3037\n",
      "Epoch [25/100], Step [1310/1751], Loss: 1.2643\n",
      "Epoch [25/100], Step [1320/1751], Loss: 1.2221\n",
      "Epoch [25/100], Step [1330/1751], Loss: 1.1931\n",
      "Epoch [25/100], Step [1340/1751], Loss: 1.2505\n",
      "Epoch [25/100], Step [1350/1751], Loss: 1.1161\n",
      "Epoch [25/100], Step [1360/1751], Loss: 1.2760\n",
      "Epoch [25/100], Step [1370/1751], Loss: 1.2941\n",
      "Epoch [25/100], Step [1380/1751], Loss: 1.3057\n",
      "Epoch [25/100], Step [1390/1751], Loss: 1.2175\n",
      "Epoch [25/100], Step [1400/1751], Loss: 1.2853\n",
      "Epoch [25/100], Step [1410/1751], Loss: 1.1365\n",
      "Epoch [25/100], Step [1420/1751], Loss: 1.2591\n",
      "Epoch [25/100], Step [1430/1751], Loss: 1.3609\n",
      "Epoch [25/100], Step [1440/1751], Loss: 1.1496\n",
      "Epoch [25/100], Step [1450/1751], Loss: 1.3186\n",
      "Epoch [25/100], Step [1460/1751], Loss: 1.1836\n",
      "Epoch [25/100], Step [1470/1751], Loss: 1.4215\n",
      "Epoch [25/100], Step [1480/1751], Loss: 1.3307\n",
      "Epoch [25/100], Step [1490/1751], Loss: 1.1336\n",
      "Epoch [25/100], Step [1500/1751], Loss: 1.2966\n",
      "Epoch [25/100], Step [1510/1751], Loss: 1.3437\n",
      "Epoch [25/100], Step [1520/1751], Loss: 1.3032\n",
      "Epoch [25/100], Step [1530/1751], Loss: 1.2960\n",
      "Epoch [25/100], Step [1540/1751], Loss: 1.1716\n",
      "Epoch [25/100], Step [1550/1751], Loss: 1.2511\n",
      "Epoch [25/100], Step [1560/1751], Loss: 1.2500\n",
      "Epoch [25/100], Step [1570/1751], Loss: 1.2692\n",
      "Epoch [25/100], Step [1580/1751], Loss: 1.1946\n",
      "Epoch [25/100], Step [1590/1751], Loss: 1.3835\n",
      "Epoch [25/100], Step [1600/1751], Loss: 1.0951\n",
      "Epoch [25/100], Step [1610/1751], Loss: 1.2897\n",
      "Epoch [25/100], Step [1620/1751], Loss: 1.2362\n",
      "Epoch [25/100], Step [1630/1751], Loss: 1.1942\n",
      "Epoch [25/100], Step [1640/1751], Loss: 1.3971\n",
      "Epoch [25/100], Step [1650/1751], Loss: 1.3590\n",
      "Epoch [25/100], Step [1660/1751], Loss: 1.2090\n",
      "Epoch [25/100], Step [1670/1751], Loss: 1.1122\n",
      "Epoch [25/100], Step [1680/1751], Loss: 1.1651\n",
      "Epoch [25/100], Step [1690/1751], Loss: 1.2219\n",
      "Epoch [25/100], Step [1700/1751], Loss: 1.3225\n",
      "Epoch [25/100], Step [1710/1751], Loss: 1.2005\n",
      "Epoch [25/100], Step [1720/1751], Loss: 1.2433\n",
      "Epoch [25/100], Step [1730/1751], Loss: 1.1667\n",
      "Epoch [25/100], Step [1740/1751], Loss: 1.1935\n",
      "Epoch [25/100], Step [1750/1751], Loss: 1.4241\n",
      "Epoch [25/100], Average Loss: 1.2588, Time: 1637.9406s\n",
      "Epoch [26/100], Step [10/1751], Loss: 1.2798\n",
      "Epoch [26/100], Step [20/1751], Loss: 1.3449\n",
      "Epoch [26/100], Step [30/1751], Loss: 1.2617\n",
      "Epoch [26/100], Step [40/1751], Loss: 1.3733\n",
      "Epoch [26/100], Step [50/1751], Loss: 1.4430\n",
      "Epoch [26/100], Step [60/1751], Loss: 1.3065\n",
      "Epoch [26/100], Step [70/1751], Loss: 1.2914\n",
      "Epoch [26/100], Step [80/1751], Loss: 1.2551\n",
      "Epoch [26/100], Step [90/1751], Loss: 1.1615\n",
      "Epoch [26/100], Step [100/1751], Loss: 1.2119\n",
      "Epoch [26/100], Step [110/1751], Loss: 1.3979\n",
      "Epoch [26/100], Step [120/1751], Loss: 1.2056\n",
      "Epoch [26/100], Step [130/1751], Loss: 1.2642\n",
      "Epoch [26/100], Step [140/1751], Loss: 1.2496\n",
      "Epoch [26/100], Step [150/1751], Loss: 1.2059\n",
      "Epoch [26/100], Step [160/1751], Loss: 1.2406\n",
      "Epoch [26/100], Step [170/1751], Loss: 1.0163\n",
      "Epoch [26/100], Step [180/1751], Loss: 1.2292\n",
      "Epoch [26/100], Step [190/1751], Loss: 1.2107\n",
      "Epoch [26/100], Step [200/1751], Loss: 1.1089\n",
      "Epoch [26/100], Step [210/1751], Loss: 1.4804\n",
      "Epoch [26/100], Step [220/1751], Loss: 1.3501\n",
      "Epoch [26/100], Step [230/1751], Loss: 1.3508\n",
      "Epoch [26/100], Step [240/1751], Loss: 1.2857\n",
      "Epoch [26/100], Step [250/1751], Loss: 1.2895\n",
      "Epoch [26/100], Step [260/1751], Loss: 1.1802\n",
      "Epoch [26/100], Step [270/1751], Loss: 1.3132\n",
      "Epoch [26/100], Step [280/1751], Loss: 1.2841\n",
      "Epoch [26/100], Step [290/1751], Loss: 1.2207\n",
      "Epoch [26/100], Step [300/1751], Loss: 1.2039\n",
      "Epoch [26/100], Step [310/1751], Loss: 1.2169\n",
      "Epoch [26/100], Step [320/1751], Loss: 1.2655\n",
      "Epoch [26/100], Step [330/1751], Loss: 1.4139\n",
      "Epoch [26/100], Step [340/1751], Loss: 1.2859\n",
      "Epoch [26/100], Step [350/1751], Loss: 1.3585\n",
      "Epoch [26/100], Step [360/1751], Loss: 1.0988\n",
      "Epoch [26/100], Step [370/1751], Loss: 1.3100\n",
      "Epoch [26/100], Step [380/1751], Loss: 1.1104\n",
      "Epoch [26/100], Step [390/1751], Loss: 1.2043\n",
      "Epoch [26/100], Step [400/1751], Loss: 1.1639\n",
      "Epoch [26/100], Step [410/1751], Loss: 1.2580\n",
      "Epoch [26/100], Step [420/1751], Loss: 1.3021\n",
      "Epoch [26/100], Step [430/1751], Loss: 1.3694\n",
      "Epoch [26/100], Step [440/1751], Loss: 1.1958\n",
      "Epoch [26/100], Step [450/1751], Loss: 1.2766\n",
      "Epoch [26/100], Step [460/1751], Loss: 1.1757\n",
      "Epoch [26/100], Step [470/1751], Loss: 1.2106\n",
      "Epoch [26/100], Step [480/1751], Loss: 1.2307\n",
      "Epoch [26/100], Step [490/1751], Loss: 1.3577\n",
      "Epoch [26/100], Step [500/1751], Loss: 1.2145\n",
      "Epoch [26/100], Step [510/1751], Loss: 1.4353\n",
      "Epoch [26/100], Step [520/1751], Loss: 1.2590\n",
      "Epoch [26/100], Step [530/1751], Loss: 1.1833\n",
      "Epoch [26/100], Step [540/1751], Loss: 1.4443\n",
      "Epoch [26/100], Step [550/1751], Loss: 1.3282\n",
      "Epoch [26/100], Step [560/1751], Loss: 1.1733\n",
      "Epoch [26/100], Step [570/1751], Loss: 1.2882\n",
      "Epoch [26/100], Step [580/1751], Loss: 1.2375\n",
      "Epoch [26/100], Step [590/1751], Loss: 1.2498\n",
      "Epoch [26/100], Step [600/1751], Loss: 1.2620\n",
      "Epoch [26/100], Step [610/1751], Loss: 1.2965\n",
      "Epoch [26/100], Step [620/1751], Loss: 1.3207\n",
      "Epoch [26/100], Step [630/1751], Loss: 1.0727\n",
      "Epoch [26/100], Step [640/1751], Loss: 1.3219\n",
      "Epoch [26/100], Step [650/1751], Loss: 1.2377\n",
      "Epoch [26/100], Step [660/1751], Loss: 1.1697\n",
      "Epoch [26/100], Step [670/1751], Loss: 1.1403\n",
      "Epoch [26/100], Step [680/1751], Loss: 1.1656\n",
      "Epoch [26/100], Step [690/1751], Loss: 1.1481\n",
      "Epoch [26/100], Step [700/1751], Loss: 1.3273\n",
      "Epoch [26/100], Step [710/1751], Loss: 1.2175\n",
      "Epoch [26/100], Step [720/1751], Loss: 1.3556\n",
      "Epoch [26/100], Step [730/1751], Loss: 1.2809\n",
      "Epoch [26/100], Step [740/1751], Loss: 1.1115\n",
      "Epoch [26/100], Step [750/1751], Loss: 1.4032\n",
      "Epoch [26/100], Step [760/1751], Loss: 1.3097\n",
      "Epoch [26/100], Step [770/1751], Loss: 1.3098\n",
      "Epoch [26/100], Step [780/1751], Loss: 1.1392\n",
      "Epoch [26/100], Step [790/1751], Loss: 1.2584\n",
      "Epoch [26/100], Step [800/1751], Loss: 1.2213\n",
      "Epoch [26/100], Step [810/1751], Loss: 1.4537\n",
      "Epoch [26/100], Step [820/1751], Loss: 1.3818\n",
      "Epoch [26/100], Step [830/1751], Loss: 1.3157\n",
      "Epoch [26/100], Step [840/1751], Loss: 1.0874\n",
      "Epoch [26/100], Step [850/1751], Loss: 1.3011\n",
      "Epoch [26/100], Step [860/1751], Loss: 1.1293\n",
      "Epoch [26/100], Step [870/1751], Loss: 1.1526\n",
      "Epoch [26/100], Step [880/1751], Loss: 1.2949\n",
      "Epoch [26/100], Step [890/1751], Loss: 1.3348\n",
      "Epoch [26/100], Step [900/1751], Loss: 1.2994\n",
      "Epoch [26/100], Step [910/1751], Loss: 1.3440\n",
      "Epoch [26/100], Step [920/1751], Loss: 1.2114\n",
      "Epoch [26/100], Step [930/1751], Loss: 1.1939\n",
      "Epoch [26/100], Step [940/1751], Loss: 1.1790\n",
      "Epoch [26/100], Step [950/1751], Loss: 1.0838\n",
      "Epoch [26/100], Step [960/1751], Loss: 1.3016\n",
      "Epoch [26/100], Step [970/1751], Loss: 1.2666\n",
      "Epoch [26/100], Step [980/1751], Loss: 1.2121\n",
      "Epoch [26/100], Step [990/1751], Loss: 1.1512\n",
      "Epoch [26/100], Step [1000/1751], Loss: 1.1916\n",
      "Epoch [26/100], Step [1010/1751], Loss: 1.2531\n",
      "Epoch [26/100], Step [1020/1751], Loss: 1.3075\n",
      "Epoch [26/100], Step [1030/1751], Loss: 1.1733\n",
      "Epoch [26/100], Step [1040/1751], Loss: 1.3843\n",
      "Epoch [26/100], Step [1050/1751], Loss: 1.3577\n",
      "Epoch [26/100], Step [1060/1751], Loss: 1.3569\n",
      "Epoch [26/100], Step [1070/1751], Loss: 1.2299\n",
      "Epoch [26/100], Step [1080/1751], Loss: 1.1658\n",
      "Epoch [26/100], Step [1090/1751], Loss: 1.2394\n",
      "Epoch [26/100], Step [1100/1751], Loss: 1.2910\n",
      "Epoch [26/100], Step [1110/1751], Loss: 1.2589\n",
      "Epoch [26/100], Step [1120/1751], Loss: 1.2979\n",
      "Epoch [26/100], Step [1130/1751], Loss: 1.2564\n",
      "Epoch [26/100], Step [1140/1751], Loss: 1.3003\n",
      "Epoch [26/100], Step [1150/1751], Loss: 1.2680\n",
      "Epoch [26/100], Step [1160/1751], Loss: 1.3259\n",
      "Epoch [26/100], Step [1170/1751], Loss: 1.3440\n",
      "Epoch [26/100], Step [1180/1751], Loss: 1.0374\n",
      "Epoch [26/100], Step [1190/1751], Loss: 1.2324\n",
      "Epoch [26/100], Step [1200/1751], Loss: 1.3460\n",
      "Epoch [26/100], Step [1210/1751], Loss: 1.2479\n",
      "Epoch [26/100], Step [1220/1751], Loss: 1.2844\n",
      "Epoch [26/100], Step [1230/1751], Loss: 1.2717\n",
      "Epoch [26/100], Step [1240/1751], Loss: 1.2864\n",
      "Epoch [26/100], Step [1250/1751], Loss: 1.2056\n",
      "Epoch [26/100], Step [1260/1751], Loss: 1.0806\n",
      "Epoch [26/100], Step [1270/1751], Loss: 1.3550\n",
      "Epoch [26/100], Step [1280/1751], Loss: 1.3017\n",
      "Epoch [26/100], Step [1290/1751], Loss: 1.2403\n",
      "Epoch [26/100], Step [1300/1751], Loss: 1.3191\n",
      "Epoch [26/100], Step [1310/1751], Loss: 1.3777\n",
      "Epoch [26/100], Step [1320/1751], Loss: 1.1869\n",
      "Epoch [26/100], Step [1330/1751], Loss: 1.3039\n",
      "Epoch [26/100], Step [1340/1751], Loss: 1.3092\n",
      "Epoch [26/100], Step [1350/1751], Loss: 1.2075\n",
      "Epoch [26/100], Step [1360/1751], Loss: 1.3101\n",
      "Epoch [26/100], Step [1370/1751], Loss: 1.3862\n",
      "Epoch [26/100], Step [1380/1751], Loss: 1.2521\n",
      "Epoch [26/100], Step [1390/1751], Loss: 1.3414\n",
      "Epoch [26/100], Step [1400/1751], Loss: 1.3530\n",
      "Epoch [26/100], Step [1410/1751], Loss: 1.2798\n",
      "Epoch [26/100], Step [1420/1751], Loss: 1.1967\n",
      "Epoch [26/100], Step [1430/1751], Loss: 1.2640\n",
      "Epoch [26/100], Step [1440/1751], Loss: 1.1266\n",
      "Epoch [26/100], Step [1450/1751], Loss: 1.1594\n",
      "Epoch [26/100], Step [1460/1751], Loss: 1.2554\n",
      "Epoch [26/100], Step [1470/1751], Loss: 1.3195\n",
      "Epoch [26/100], Step [1480/1751], Loss: 1.2287\n",
      "Epoch [26/100], Step [1490/1751], Loss: 1.1462\n",
      "Epoch [26/100], Step [1500/1751], Loss: 1.2811\n",
      "Epoch [26/100], Step [1510/1751], Loss: 1.2309\n",
      "Epoch [26/100], Step [1520/1751], Loss: 1.2440\n",
      "Epoch [26/100], Step [1530/1751], Loss: 1.1209\n",
      "Epoch [26/100], Step [1540/1751], Loss: 1.0910\n",
      "Epoch [26/100], Step [1550/1751], Loss: 1.1985\n",
      "Epoch [26/100], Step [1560/1751], Loss: 1.3722\n",
      "Epoch [26/100], Step [1570/1751], Loss: 1.4464\n",
      "Epoch [26/100], Step [1580/1751], Loss: 1.2149\n",
      "Epoch [26/100], Step [1590/1751], Loss: 1.1575\n",
      "Epoch [26/100], Step [1600/1751], Loss: 1.3336\n",
      "Epoch [26/100], Step [1610/1751], Loss: 1.1435\n",
      "Epoch [26/100], Step [1620/1751], Loss: 1.2193\n",
      "Epoch [26/100], Step [1630/1751], Loss: 1.3147\n",
      "Epoch [26/100], Step [1640/1751], Loss: 1.2955\n",
      "Epoch [26/100], Step [1650/1751], Loss: 1.2297\n",
      "Epoch [26/100], Step [1660/1751], Loss: 1.2915\n",
      "Epoch [26/100], Step [1670/1751], Loss: 1.4176\n",
      "Epoch [26/100], Step [1680/1751], Loss: 1.1549\n",
      "Epoch [26/100], Step [1690/1751], Loss: 1.3985\n",
      "Epoch [26/100], Step [1700/1751], Loss: 1.2577\n",
      "Epoch [26/100], Step [1710/1751], Loss: 1.0652\n",
      "Epoch [26/100], Step [1720/1751], Loss: 1.1912\n",
      "Epoch [26/100], Step [1730/1751], Loss: 1.2455\n",
      "Epoch [26/100], Step [1740/1751], Loss: 1.2178\n",
      "Epoch [26/100], Step [1750/1751], Loss: 1.2368\n",
      "Epoch [26/100], Average Loss: 1.2557, Time: 1639.2969s\n",
      "Epoch [27/100], Step [10/1751], Loss: 1.3006\n",
      "Epoch [27/100], Step [20/1751], Loss: 1.3023\n",
      "Epoch [27/100], Step [30/1751], Loss: 1.3029\n",
      "Epoch [27/100], Step [40/1751], Loss: 1.2541\n",
      "Epoch [27/100], Step [50/1751], Loss: 1.2582\n",
      "Epoch [27/100], Step [60/1751], Loss: 1.2339\n",
      "Epoch [27/100], Step [70/1751], Loss: 1.2845\n",
      "Epoch [27/100], Step [80/1751], Loss: 1.4421\n",
      "Epoch [27/100], Step [90/1751], Loss: 1.3232\n",
      "Epoch [27/100], Step [100/1751], Loss: 1.2659\n",
      "Epoch [27/100], Step [110/1751], Loss: 1.1722\n",
      "Epoch [27/100], Step [120/1751], Loss: 1.2309\n",
      "Epoch [27/100], Step [130/1751], Loss: 1.1758\n",
      "Epoch [27/100], Step [140/1751], Loss: 1.3271\n",
      "Epoch [27/100], Step [150/1751], Loss: 1.2808\n",
      "Epoch [27/100], Step [160/1751], Loss: 1.3428\n",
      "Epoch [27/100], Step [170/1751], Loss: 1.3312\n",
      "Epoch [27/100], Step [180/1751], Loss: 1.2471\n",
      "Epoch [27/100], Step [190/1751], Loss: 1.2597\n",
      "Epoch [27/100], Step [200/1751], Loss: 1.3267\n",
      "Epoch [27/100], Step [210/1751], Loss: 1.1044\n",
      "Epoch [27/100], Step [220/1751], Loss: 1.3230\n",
      "Epoch [27/100], Step [230/1751], Loss: 1.2759\n",
      "Epoch [27/100], Step [240/1751], Loss: 1.2444\n",
      "Epoch [27/100], Step [250/1751], Loss: 1.2186\n",
      "Epoch [27/100], Step [260/1751], Loss: 1.3009\n",
      "Epoch [27/100], Step [270/1751], Loss: 1.2027\n",
      "Epoch [27/100], Step [280/1751], Loss: 1.1568\n",
      "Epoch [27/100], Step [290/1751], Loss: 1.3623\n",
      "Epoch [27/100], Step [300/1751], Loss: 1.3107\n",
      "Epoch [27/100], Step [310/1751], Loss: 1.1879\n",
      "Epoch [27/100], Step [320/1751], Loss: 1.3515\n",
      "Epoch [27/100], Step [330/1751], Loss: 1.3572\n",
      "Epoch [27/100], Step [340/1751], Loss: 1.1469\n",
      "Epoch [27/100], Step [350/1751], Loss: 1.2856\n",
      "Epoch [27/100], Step [360/1751], Loss: 1.3922\n",
      "Epoch [27/100], Step [370/1751], Loss: 1.2276\n",
      "Epoch [27/100], Step [380/1751], Loss: 1.2241\n",
      "Epoch [27/100], Step [390/1751], Loss: 1.3065\n",
      "Epoch [27/100], Step [400/1751], Loss: 1.4457\n",
      "Epoch [27/100], Step [410/1751], Loss: 1.3897\n",
      "Epoch [27/100], Step [420/1751], Loss: 1.3426\n",
      "Epoch [27/100], Step [430/1751], Loss: 1.2512\n",
      "Epoch [27/100], Step [440/1751], Loss: 1.2187\n",
      "Epoch [27/100], Step [450/1751], Loss: 1.0420\n",
      "Epoch [27/100], Step [460/1751], Loss: 1.3758\n",
      "Epoch [27/100], Step [470/1751], Loss: 1.2584\n",
      "Epoch [27/100], Step [480/1751], Loss: 1.1705\n",
      "Epoch [27/100], Step [490/1751], Loss: 1.2383\n",
      "Epoch [27/100], Step [500/1751], Loss: 1.1489\n",
      "Epoch [27/100], Step [510/1751], Loss: 1.3026\n",
      "Epoch [27/100], Step [520/1751], Loss: 1.2485\n",
      "Epoch [27/100], Step [530/1751], Loss: 1.2333\n",
      "Epoch [27/100], Step [540/1751], Loss: 1.1542\n",
      "Epoch [27/100], Step [550/1751], Loss: 1.1992\n",
      "Epoch [27/100], Step [560/1751], Loss: 1.2549\n",
      "Epoch [27/100], Step [570/1751], Loss: 1.2109\n",
      "Epoch [27/100], Step [580/1751], Loss: 1.2228\n",
      "Epoch [27/100], Step [590/1751], Loss: 1.2029\n",
      "Epoch [27/100], Step [600/1751], Loss: 1.4157\n",
      "Epoch [27/100], Step [610/1751], Loss: 1.2047\n",
      "Epoch [27/100], Step [620/1751], Loss: 1.3408\n",
      "Epoch [27/100], Step [630/1751], Loss: 1.2431\n",
      "Epoch [27/100], Step [640/1751], Loss: 1.2850\n",
      "Epoch [27/100], Step [650/1751], Loss: 1.4388\n",
      "Epoch [27/100], Step [660/1751], Loss: 1.3527\n",
      "Epoch [27/100], Step [670/1751], Loss: 1.1215\n",
      "Epoch [27/100], Step [680/1751], Loss: 1.2233\n",
      "Epoch [27/100], Step [690/1751], Loss: 1.2320\n",
      "Epoch [27/100], Step [700/1751], Loss: 1.3297\n",
      "Epoch [27/100], Step [710/1751], Loss: 1.0742\n",
      "Epoch [27/100], Step [720/1751], Loss: 1.1890\n",
      "Epoch [27/100], Step [730/1751], Loss: 1.2478\n",
      "Epoch [27/100], Step [740/1751], Loss: 1.2871\n",
      "Epoch [27/100], Step [750/1751], Loss: 1.1563\n",
      "Epoch [27/100], Step [760/1751], Loss: 1.1650\n",
      "Epoch [27/100], Step [770/1751], Loss: 1.1598\n",
      "Epoch [27/100], Step [780/1751], Loss: 1.4213\n",
      "Epoch [27/100], Step [790/1751], Loss: 1.2540\n",
      "Epoch [27/100], Step [800/1751], Loss: 1.1686\n",
      "Epoch [27/100], Step [810/1751], Loss: 1.1469\n",
      "Epoch [27/100], Step [820/1751], Loss: 1.1826\n",
      "Epoch [27/100], Step [830/1751], Loss: 1.2937\n",
      "Epoch [27/100], Step [840/1751], Loss: 1.1510\n",
      "Epoch [27/100], Step [850/1751], Loss: 1.2357\n",
      "Epoch [27/100], Step [860/1751], Loss: 1.1503\n",
      "Epoch [27/100], Step [870/1751], Loss: 1.2364\n",
      "Epoch [27/100], Step [880/1751], Loss: 1.1696\n",
      "Epoch [27/100], Step [890/1751], Loss: 1.2337\n",
      "Epoch [27/100], Step [900/1751], Loss: 1.3704\n",
      "Epoch [27/100], Step [910/1751], Loss: 1.1466\n",
      "Epoch [27/100], Step [920/1751], Loss: 1.2537\n",
      "Epoch [27/100], Step [930/1751], Loss: 1.2071\n",
      "Epoch [27/100], Step [940/1751], Loss: 1.2672\n",
      "Epoch [27/100], Step [950/1751], Loss: 1.2999\n",
      "Epoch [27/100], Step [960/1751], Loss: 1.2629\n",
      "Epoch [27/100], Step [970/1751], Loss: 1.3054\n",
      "Epoch [27/100], Step [980/1751], Loss: 1.0952\n",
      "Epoch [27/100], Step [990/1751], Loss: 1.4147\n",
      "Epoch [27/100], Step [1000/1751], Loss: 1.2940\n",
      "Epoch [27/100], Step [1010/1751], Loss: 1.1922\n",
      "Epoch [27/100], Step [1020/1751], Loss: 1.2915\n",
      "Epoch [27/100], Step [1030/1751], Loss: 1.1905\n",
      "Epoch [27/100], Step [1040/1751], Loss: 1.1721\n",
      "Epoch [27/100], Step [1050/1751], Loss: 1.4455\n",
      "Epoch [27/100], Step [1060/1751], Loss: 1.2135\n",
      "Epoch [27/100], Step [1070/1751], Loss: 1.3396\n",
      "Epoch [27/100], Step [1080/1751], Loss: 1.2977\n",
      "Epoch [27/100], Step [1090/1751], Loss: 1.3322\n",
      "Epoch [27/100], Step [1100/1751], Loss: 1.3529\n",
      "Epoch [27/100], Step [1110/1751], Loss: 1.2383\n",
      "Epoch [27/100], Step [1120/1751], Loss: 1.3044\n",
      "Epoch [27/100], Step [1130/1751], Loss: 1.3557\n",
      "Epoch [27/100], Step [1140/1751], Loss: 1.2080\n",
      "Epoch [27/100], Step [1150/1751], Loss: 1.2908\n",
      "Epoch [27/100], Step [1160/1751], Loss: 1.2190\n",
      "Epoch [27/100], Step [1170/1751], Loss: 1.0141\n",
      "Epoch [27/100], Step [1180/1751], Loss: 1.2661\n",
      "Epoch [27/100], Step [1190/1751], Loss: 1.3374\n",
      "Epoch [27/100], Step [1200/1751], Loss: 1.2376\n",
      "Epoch [27/100], Step [1210/1751], Loss: 1.1573\n",
      "Epoch [27/100], Step [1220/1751], Loss: 1.2525\n",
      "Epoch [27/100], Step [1230/1751], Loss: 1.2130\n",
      "Epoch [27/100], Step [1240/1751], Loss: 1.2717\n",
      "Epoch [27/100], Step [1250/1751], Loss: 1.1558\n",
      "Epoch [27/100], Step [1260/1751], Loss: 1.2195\n",
      "Epoch [27/100], Step [1270/1751], Loss: 1.2067\n",
      "Epoch [27/100], Step [1280/1751], Loss: 1.3032\n",
      "Epoch [27/100], Step [1290/1751], Loss: 1.2841\n",
      "Epoch [27/100], Step [1300/1751], Loss: 1.2075\n",
      "Epoch [27/100], Step [1310/1751], Loss: 1.2481\n",
      "Epoch [27/100], Step [1320/1751], Loss: 1.2258\n",
      "Epoch [27/100], Step [1330/1751], Loss: 1.3096\n",
      "Epoch [27/100], Step [1340/1751], Loss: 1.3752\n",
      "Epoch [27/100], Step [1350/1751], Loss: 1.2548\n",
      "Epoch [27/100], Step [1360/1751], Loss: 1.2631\n",
      "Epoch [27/100], Step [1370/1751], Loss: 1.3307\n",
      "Epoch [27/100], Step [1380/1751], Loss: 1.3581\n",
      "Epoch [27/100], Step [1390/1751], Loss: 1.2327\n",
      "Epoch [27/100], Step [1400/1751], Loss: 1.3779\n",
      "Epoch [27/100], Step [1410/1751], Loss: 1.2845\n",
      "Epoch [27/100], Step [1420/1751], Loss: 1.1468\n",
      "Epoch [27/100], Step [1430/1751], Loss: 1.3878\n",
      "Epoch [27/100], Step [1440/1751], Loss: 1.2182\n",
      "Epoch [27/100], Step [1450/1751], Loss: 1.3650\n",
      "Epoch [27/100], Step [1460/1751], Loss: 1.2219\n",
      "Epoch [27/100], Step [1470/1751], Loss: 1.2582\n",
      "Epoch [27/100], Step [1480/1751], Loss: 1.2544\n",
      "Epoch [27/100], Step [1490/1751], Loss: 1.0657\n",
      "Epoch [27/100], Step [1500/1751], Loss: 1.1780\n",
      "Epoch [27/100], Step [1510/1751], Loss: 1.2174\n",
      "Epoch [27/100], Step [1520/1751], Loss: 1.0648\n",
      "Epoch [27/100], Step [1530/1751], Loss: 1.3085\n",
      "Epoch [27/100], Step [1540/1751], Loss: 1.2475\n",
      "Epoch [27/100], Step [1550/1751], Loss: 1.0927\n",
      "Epoch [27/100], Step [1560/1751], Loss: 1.2033\n",
      "Epoch [27/100], Step [1570/1751], Loss: 1.3250\n",
      "Epoch [27/100], Step [1580/1751], Loss: 1.3126\n",
      "Epoch [27/100], Step [1590/1751], Loss: 1.0898\n",
      "Epoch [27/100], Step [1600/1751], Loss: 1.3887\n",
      "Epoch [27/100], Step [1610/1751], Loss: 1.3221\n",
      "Epoch [27/100], Step [1620/1751], Loss: 1.2760\n",
      "Epoch [27/100], Step [1630/1751], Loss: 1.2480\n",
      "Epoch [27/100], Step [1640/1751], Loss: 1.2771\n",
      "Epoch [27/100], Step [1650/1751], Loss: 1.3831\n",
      "Epoch [27/100], Step [1660/1751], Loss: 1.2922\n",
      "Epoch [27/100], Step [1670/1751], Loss: 1.2892\n",
      "Epoch [27/100], Step [1680/1751], Loss: 1.2053\n",
      "Epoch [27/100], Step [1690/1751], Loss: 1.1719\n",
      "Epoch [27/100], Step [1700/1751], Loss: 1.1767\n",
      "Epoch [27/100], Step [1710/1751], Loss: 1.2119\n",
      "Epoch [27/100], Step [1720/1751], Loss: 1.2735\n",
      "Epoch [27/100], Step [1730/1751], Loss: 1.3493\n",
      "Epoch [27/100], Step [1740/1751], Loss: 1.3564\n",
      "Epoch [27/100], Step [1750/1751], Loss: 1.2369\n",
      "Epoch [27/100], Average Loss: 1.2515, Time: 1650.8649s\n",
      "Epoch [28/100], Step [10/1751], Loss: 1.4108\n",
      "Epoch [28/100], Step [20/1751], Loss: 1.5041\n",
      "Epoch [28/100], Step [30/1751], Loss: 1.2277\n",
      "Epoch [28/100], Step [40/1751], Loss: 1.4332\n",
      "Epoch [28/100], Step [50/1751], Loss: 1.2326\n",
      "Epoch [28/100], Step [60/1751], Loss: 1.4332\n",
      "Epoch [28/100], Step [70/1751], Loss: 1.2012\n",
      "Epoch [28/100], Step [80/1751], Loss: 1.4102\n",
      "Epoch [28/100], Step [90/1751], Loss: 1.3747\n",
      "Epoch [28/100], Step [100/1751], Loss: 1.2366\n",
      "Epoch [28/100], Step [110/1751], Loss: 1.2872\n",
      "Epoch [28/100], Step [120/1751], Loss: 1.3436\n",
      "Epoch [28/100], Step [130/1751], Loss: 1.1920\n",
      "Epoch [28/100], Step [140/1751], Loss: 1.1973\n",
      "Epoch [28/100], Step [150/1751], Loss: 1.3350\n",
      "Epoch [28/100], Step [160/1751], Loss: 1.3135\n",
      "Epoch [28/100], Step [170/1751], Loss: 1.0602\n",
      "Epoch [28/100], Step [180/1751], Loss: 1.4551\n",
      "Epoch [28/100], Step [190/1751], Loss: 1.1617\n",
      "Epoch [28/100], Step [200/1751], Loss: 1.2419\n",
      "Epoch [28/100], Step [210/1751], Loss: 1.2935\n",
      "Epoch [28/100], Step [220/1751], Loss: 1.1284\n",
      "Epoch [28/100], Step [230/1751], Loss: 1.2558\n",
      "Epoch [28/100], Step [240/1751], Loss: 1.1763\n",
      "Epoch [28/100], Step [250/1751], Loss: 1.2849\n",
      "Epoch [28/100], Step [260/1751], Loss: 1.2060\n",
      "Epoch [28/100], Step [270/1751], Loss: 1.4298\n",
      "Epoch [28/100], Step [280/1751], Loss: 1.2011\n",
      "Epoch [28/100], Step [290/1751], Loss: 1.1289\n",
      "Epoch [28/100], Step [300/1751], Loss: 1.1953\n",
      "Epoch [28/100], Step [310/1751], Loss: 1.3287\n",
      "Epoch [28/100], Step [320/1751], Loss: 1.2736\n",
      "Epoch [28/100], Step [330/1751], Loss: 1.2728\n",
      "Epoch [28/100], Step [340/1751], Loss: 1.1545\n",
      "Epoch [28/100], Step [350/1751], Loss: 1.1212\n",
      "Epoch [28/100], Step [360/1751], Loss: 1.2056\n",
      "Epoch [28/100], Step [370/1751], Loss: 1.1795\n",
      "Epoch [28/100], Step [380/1751], Loss: 1.2720\n",
      "Epoch [28/100], Step [390/1751], Loss: 1.3019\n",
      "Epoch [28/100], Step [400/1751], Loss: 1.1523\n",
      "Epoch [28/100], Step [410/1751], Loss: 1.1749\n",
      "Epoch [28/100], Step [420/1751], Loss: 1.1285\n",
      "Epoch [28/100], Step [430/1751], Loss: 1.2763\n",
      "Epoch [28/100], Step [440/1751], Loss: 1.1364\n",
      "Epoch [28/100], Step [450/1751], Loss: 1.2073\n",
      "Epoch [28/100], Step [460/1751], Loss: 1.2275\n",
      "Epoch [28/100], Step [470/1751], Loss: 1.3457\n",
      "Epoch [28/100], Step [480/1751], Loss: 1.2786\n",
      "Epoch [28/100], Step [490/1751], Loss: 1.1554\n",
      "Epoch [28/100], Step [500/1751], Loss: 1.1078\n",
      "Epoch [28/100], Step [510/1751], Loss: 1.2459\n",
      "Epoch [28/100], Step [520/1751], Loss: 1.2906\n",
      "Epoch [28/100], Step [530/1751], Loss: 1.3103\n",
      "Epoch [28/100], Step [540/1751], Loss: 1.4300\n",
      "Epoch [28/100], Step [550/1751], Loss: 1.2614\n",
      "Epoch [28/100], Step [560/1751], Loss: 1.2320\n",
      "Epoch [28/100], Step [570/1751], Loss: 1.2161\n",
      "Epoch [28/100], Step [580/1751], Loss: 1.2072\n",
      "Epoch [28/100], Step [590/1751], Loss: 1.3465\n",
      "Epoch [28/100], Step [600/1751], Loss: 1.2800\n",
      "Epoch [28/100], Step [610/1751], Loss: 1.3854\n",
      "Epoch [28/100], Step [620/1751], Loss: 1.4063\n",
      "Epoch [28/100], Step [630/1751], Loss: 1.2330\n",
      "Epoch [28/100], Step [640/1751], Loss: 1.1379\n",
      "Epoch [28/100], Step [650/1751], Loss: 1.2844\n",
      "Epoch [28/100], Step [660/1751], Loss: 1.2985\n",
      "Epoch [28/100], Step [670/1751], Loss: 1.3119\n",
      "Epoch [28/100], Step [680/1751], Loss: 1.2508\n",
      "Epoch [28/100], Step [690/1751], Loss: 1.3478\n",
      "Epoch [28/100], Step [700/1751], Loss: 1.2918\n",
      "Epoch [28/100], Step [710/1751], Loss: 1.1324\n",
      "Epoch [28/100], Step [720/1751], Loss: 1.2109\n",
      "Epoch [28/100], Step [730/1751], Loss: 1.2156\n",
      "Epoch [28/100], Step [740/1751], Loss: 1.2138\n",
      "Epoch [28/100], Step [750/1751], Loss: 1.1400\n",
      "Epoch [28/100], Step [760/1751], Loss: 1.2266\n",
      "Epoch [28/100], Step [770/1751], Loss: 1.2590\n",
      "Epoch [28/100], Step [780/1751], Loss: 1.3689\n",
      "Epoch [28/100], Step [790/1751], Loss: 1.2011\n",
      "Epoch [28/100], Step [800/1751], Loss: 1.3015\n",
      "Epoch [28/100], Step [810/1751], Loss: 1.2092\n",
      "Epoch [28/100], Step [820/1751], Loss: 1.2324\n",
      "Epoch [28/100], Step [830/1751], Loss: 1.2635\n",
      "Epoch [28/100], Step [840/1751], Loss: 1.3533\n",
      "Epoch [28/100], Step [850/1751], Loss: 1.2806\n",
      "Epoch [28/100], Step [860/1751], Loss: 1.1013\n",
      "Epoch [28/100], Step [870/1751], Loss: 1.2694\n",
      "Epoch [28/100], Step [880/1751], Loss: 1.1828\n",
      "Epoch [28/100], Step [890/1751], Loss: 1.0831\n",
      "Epoch [28/100], Step [900/1751], Loss: 1.1194\n",
      "Epoch [28/100], Step [910/1751], Loss: 1.3967\n",
      "Epoch [28/100], Step [920/1751], Loss: 1.1778\n",
      "Epoch [28/100], Step [930/1751], Loss: 1.4513\n",
      "Epoch [28/100], Step [940/1751], Loss: 1.2941\n",
      "Epoch [28/100], Step [950/1751], Loss: 1.2377\n",
      "Epoch [28/100], Step [960/1751], Loss: 1.2103\n",
      "Epoch [28/100], Step [970/1751], Loss: 1.1979\n",
      "Epoch [28/100], Step [980/1751], Loss: 1.2724\n",
      "Epoch [28/100], Step [990/1751], Loss: 1.2783\n",
      "Epoch [28/100], Step [1000/1751], Loss: 1.3109\n",
      "Epoch [28/100], Step [1010/1751], Loss: 1.1250\n",
      "Epoch [28/100], Step [1020/1751], Loss: 1.2232\n",
      "Epoch [28/100], Step [1030/1751], Loss: 1.2356\n",
      "Epoch [28/100], Step [1040/1751], Loss: 1.2269\n",
      "Epoch [28/100], Step [1050/1751], Loss: 1.4287\n",
      "Epoch [28/100], Step [1060/1751], Loss: 1.1792\n",
      "Epoch [28/100], Step [1070/1751], Loss: 1.2512\n",
      "Epoch [28/100], Step [1080/1751], Loss: 1.2446\n",
      "Epoch [28/100], Step [1090/1751], Loss: 1.3507\n",
      "Epoch [28/100], Step [1100/1751], Loss: 1.2652\n",
      "Epoch [28/100], Step [1110/1751], Loss: 1.2512\n",
      "Epoch [28/100], Step [1120/1751], Loss: 1.2118\n",
      "Epoch [28/100], Step [1130/1751], Loss: 1.2410\n",
      "Epoch [28/100], Step [1140/1751], Loss: 1.1637\n",
      "Epoch [28/100], Step [1150/1751], Loss: 1.2108\n",
      "Epoch [28/100], Step [1160/1751], Loss: 1.3045\n",
      "Epoch [28/100], Step [1170/1751], Loss: 1.3235\n",
      "Epoch [28/100], Step [1180/1751], Loss: 1.2518\n",
      "Epoch [28/100], Step [1190/1751], Loss: 1.2486\n",
      "Epoch [28/100], Step [1200/1751], Loss: 1.3445\n",
      "Epoch [28/100], Step [1210/1751], Loss: 1.3281\n",
      "Epoch [28/100], Step [1220/1751], Loss: 1.1263\n",
      "Epoch [28/100], Step [1230/1751], Loss: 1.2712\n",
      "Epoch [28/100], Step [1240/1751], Loss: 1.3208\n",
      "Epoch [28/100], Step [1250/1751], Loss: 1.1990\n",
      "Epoch [28/100], Step [1260/1751], Loss: 1.3432\n",
      "Epoch [28/100], Step [1270/1751], Loss: 1.3243\n",
      "Epoch [28/100], Step [1280/1751], Loss: 1.3832\n",
      "Epoch [28/100], Step [1290/1751], Loss: 1.1724\n",
      "Epoch [28/100], Step [1300/1751], Loss: 1.4705\n",
      "Epoch [28/100], Step [1310/1751], Loss: 1.1476\n",
      "Epoch [28/100], Step [1320/1751], Loss: 1.1741\n",
      "Epoch [28/100], Step [1330/1751], Loss: 1.3750\n",
      "Epoch [28/100], Step [1340/1751], Loss: 1.0842\n",
      "Epoch [28/100], Step [1350/1751], Loss: 1.1179\n",
      "Epoch [28/100], Step [1360/1751], Loss: 1.1736\n",
      "Epoch [28/100], Step [1370/1751], Loss: 1.2680\n",
      "Epoch [28/100], Step [1380/1751], Loss: 1.2915\n",
      "Epoch [28/100], Step [1390/1751], Loss: 1.2445\n",
      "Epoch [28/100], Step [1400/1751], Loss: 1.4489\n",
      "Epoch [28/100], Step [1410/1751], Loss: 1.3139\n",
      "Epoch [28/100], Step [1420/1751], Loss: 1.1975\n",
      "Epoch [28/100], Step [1430/1751], Loss: 1.2644\n",
      "Epoch [28/100], Step [1440/1751], Loss: 1.0627\n",
      "Epoch [28/100], Step [1450/1751], Loss: 1.1531\n",
      "Epoch [28/100], Step [1460/1751], Loss: 1.2747\n",
      "Epoch [28/100], Step [1470/1751], Loss: 1.1649\n",
      "Epoch [28/100], Step [1480/1751], Loss: 1.2466\n",
      "Epoch [28/100], Step [1490/1751], Loss: 1.2399\n",
      "Epoch [28/100], Step [1500/1751], Loss: 1.4336\n",
      "Epoch [28/100], Step [1510/1751], Loss: 1.4525\n",
      "Epoch [28/100], Step [1520/1751], Loss: 1.3507\n",
      "Epoch [28/100], Step [1530/1751], Loss: 1.2858\n",
      "Epoch [28/100], Step [1540/1751], Loss: 1.3832\n",
      "Epoch [28/100], Step [1550/1751], Loss: 1.0492\n",
      "Epoch [28/100], Step [1560/1751], Loss: 1.2501\n",
      "Epoch [28/100], Step [1570/1751], Loss: 1.4929\n",
      "Epoch [28/100], Step [1580/1751], Loss: 1.3618\n",
      "Epoch [28/100], Step [1590/1751], Loss: 1.1189\n",
      "Epoch [28/100], Step [1600/1751], Loss: 1.3520\n",
      "Epoch [28/100], Step [1610/1751], Loss: 1.2843\n",
      "Epoch [28/100], Step [1620/1751], Loss: 1.1722\n",
      "Epoch [28/100], Step [1630/1751], Loss: 1.3028\n",
      "Epoch [28/100], Step [1640/1751], Loss: 1.3268\n",
      "Epoch [28/100], Step [1650/1751], Loss: 1.3082\n",
      "Epoch [28/100], Step [1660/1751], Loss: 1.1657\n",
      "Epoch [28/100], Step [1670/1751], Loss: 1.2416\n",
      "Epoch [28/100], Step [1680/1751], Loss: 1.2880\n",
      "Epoch [28/100], Step [1690/1751], Loss: 1.0784\n",
      "Epoch [28/100], Step [1700/1751], Loss: 1.2213\n",
      "Epoch [28/100], Step [1710/1751], Loss: 1.3100\n",
      "Epoch [28/100], Step [1720/1751], Loss: 1.3973\n",
      "Epoch [28/100], Step [1730/1751], Loss: 1.2234\n",
      "Epoch [28/100], Step [1740/1751], Loss: 1.1640\n",
      "Epoch [28/100], Step [1750/1751], Loss: 1.3295\n",
      "Epoch [28/100], Average Loss: 1.2472, Time: 1649.9170s\n",
      "Epoch [29/100], Step [10/1751], Loss: 1.2206\n",
      "Epoch [29/100], Step [20/1751], Loss: 1.1055\n",
      "Epoch [29/100], Step [30/1751], Loss: 1.3896\n",
      "Epoch [29/100], Step [40/1751], Loss: 1.3641\n",
      "Epoch [29/100], Step [50/1751], Loss: 1.2775\n",
      "Epoch [29/100], Step [60/1751], Loss: 1.0174\n",
      "Epoch [29/100], Step [70/1751], Loss: 1.1556\n",
      "Epoch [29/100], Step [80/1751], Loss: 1.3744\n",
      "Epoch [29/100], Step [90/1751], Loss: 1.4068\n",
      "Epoch [29/100], Step [100/1751], Loss: 1.1813\n",
      "Epoch [29/100], Step [110/1751], Loss: 1.2211\n",
      "Epoch [29/100], Step [120/1751], Loss: 1.2511\n",
      "Epoch [29/100], Step [130/1751], Loss: 1.2678\n",
      "Epoch [29/100], Step [140/1751], Loss: 1.1939\n",
      "Epoch [29/100], Step [150/1751], Loss: 1.2388\n",
      "Epoch [29/100], Step [160/1751], Loss: 1.1769\n",
      "Epoch [29/100], Step [170/1751], Loss: 1.1810\n",
      "Epoch [29/100], Step [180/1751], Loss: 1.2586\n",
      "Epoch [29/100], Step [190/1751], Loss: 1.1993\n",
      "Epoch [29/100], Step [200/1751], Loss: 1.4374\n",
      "Epoch [29/100], Step [210/1751], Loss: 1.2775\n",
      "Epoch [29/100], Step [220/1751], Loss: 1.2473\n",
      "Epoch [29/100], Step [230/1751], Loss: 1.2126\n",
      "Epoch [29/100], Step [240/1751], Loss: 1.1751\n",
      "Epoch [29/100], Step [250/1751], Loss: 1.3709\n",
      "Epoch [29/100], Step [260/1751], Loss: 1.3266\n",
      "Epoch [29/100], Step [270/1751], Loss: 1.3116\n",
      "Epoch [29/100], Step [280/1751], Loss: 1.0672\n",
      "Epoch [29/100], Step [290/1751], Loss: 1.2314\n",
      "Epoch [29/100], Step [300/1751], Loss: 1.4593\n",
      "Epoch [29/100], Step [310/1751], Loss: 1.3733\n",
      "Epoch [29/100], Step [320/1751], Loss: 1.3036\n",
      "Epoch [29/100], Step [330/1751], Loss: 1.3020\n",
      "Epoch [29/100], Step [340/1751], Loss: 1.3388\n",
      "Epoch [29/100], Step [350/1751], Loss: 1.3518\n",
      "Epoch [29/100], Step [360/1751], Loss: 1.2087\n",
      "Epoch [29/100], Step [370/1751], Loss: 1.1795\n",
      "Epoch [29/100], Step [380/1751], Loss: 1.5288\n",
      "Epoch [29/100], Step [390/1751], Loss: 1.2699\n",
      "Epoch [29/100], Step [400/1751], Loss: 1.2463\n",
      "Epoch [29/100], Step [410/1751], Loss: 1.3007\n",
      "Epoch [29/100], Step [420/1751], Loss: 1.3437\n",
      "Epoch [29/100], Step [430/1751], Loss: 1.2202\n",
      "Epoch [29/100], Step [440/1751], Loss: 1.0497\n",
      "Epoch [29/100], Step [450/1751], Loss: 1.1055\n",
      "Epoch [29/100], Step [460/1751], Loss: 1.2369\n",
      "Epoch [29/100], Step [470/1751], Loss: 1.2193\n",
      "Epoch [29/100], Step [480/1751], Loss: 1.1678\n",
      "Epoch [29/100], Step [490/1751], Loss: 1.4022\n",
      "Epoch [29/100], Step [500/1751], Loss: 1.1399\n",
      "Epoch [29/100], Step [510/1751], Loss: 1.1978\n",
      "Epoch [29/100], Step [520/1751], Loss: 1.2583\n",
      "Epoch [29/100], Step [530/1751], Loss: 1.1484\n",
      "Epoch [29/100], Step [540/1751], Loss: 1.0867\n",
      "Epoch [29/100], Step [550/1751], Loss: 1.1936\n",
      "Epoch [29/100], Step [560/1751], Loss: 1.2231\n",
      "Epoch [29/100], Step [570/1751], Loss: 1.2717\n",
      "Epoch [29/100], Step [580/1751], Loss: 1.2485\n",
      "Epoch [29/100], Step [590/1751], Loss: 1.2197\n",
      "Epoch [29/100], Step [600/1751], Loss: 1.2576\n",
      "Epoch [29/100], Step [610/1751], Loss: 1.3038\n",
      "Epoch [29/100], Step [620/1751], Loss: 1.1312\n",
      "Epoch [29/100], Step [630/1751], Loss: 1.2461\n",
      "Epoch [29/100], Step [640/1751], Loss: 1.1778\n",
      "Epoch [29/100], Step [650/1751], Loss: 1.1250\n",
      "Epoch [29/100], Step [660/1751], Loss: 1.1959\n",
      "Epoch [29/100], Step [670/1751], Loss: 1.2652\n",
      "Epoch [29/100], Step [680/1751], Loss: 1.2210\n",
      "Epoch [29/100], Step [690/1751], Loss: 1.1693\n",
      "Epoch [29/100], Step [700/1751], Loss: 1.3175\n",
      "Epoch [29/100], Step [710/1751], Loss: 1.2500\n",
      "Epoch [29/100], Step [720/1751], Loss: 1.0872\n",
      "Epoch [29/100], Step [730/1751], Loss: 1.4053\n",
      "Epoch [29/100], Step [740/1751], Loss: 1.1939\n",
      "Epoch [29/100], Step [750/1751], Loss: 1.2110\n",
      "Epoch [29/100], Step [760/1751], Loss: 1.2922\n",
      "Epoch [29/100], Step [770/1751], Loss: 1.2480\n",
      "Epoch [29/100], Step [780/1751], Loss: 1.1642\n",
      "Epoch [29/100], Step [790/1751], Loss: 1.1658\n",
      "Epoch [29/100], Step [800/1751], Loss: 1.2265\n",
      "Epoch [29/100], Step [810/1751], Loss: 1.2340\n",
      "Epoch [29/100], Step [820/1751], Loss: 1.3210\n",
      "Epoch [29/100], Step [830/1751], Loss: 1.2944\n",
      "Epoch [29/100], Step [840/1751], Loss: 1.1687\n",
      "Epoch [29/100], Step [850/1751], Loss: 1.2319\n",
      "Epoch [29/100], Step [860/1751], Loss: 1.2525\n",
      "Epoch [29/100], Step [870/1751], Loss: 1.2120\n",
      "Epoch [29/100], Step [880/1751], Loss: 1.2545\n",
      "Epoch [29/100], Step [890/1751], Loss: 1.2485\n",
      "Epoch [29/100], Step [900/1751], Loss: 1.1930\n",
      "Epoch [29/100], Step [910/1751], Loss: 1.1843\n",
      "Epoch [29/100], Step [920/1751], Loss: 1.3655\n",
      "Epoch [29/100], Step [930/1751], Loss: 1.2202\n",
      "Epoch [29/100], Step [940/1751], Loss: 1.2653\n",
      "Epoch [29/100], Step [950/1751], Loss: 1.2536\n",
      "Epoch [29/100], Step [960/1751], Loss: 1.2495\n",
      "Epoch [29/100], Step [970/1751], Loss: 1.3189\n",
      "Epoch [29/100], Step [980/1751], Loss: 1.1584\n",
      "Epoch [29/100], Step [990/1751], Loss: 1.2112\n",
      "Epoch [29/100], Step [1000/1751], Loss: 1.3023\n",
      "Epoch [29/100], Step [1010/1751], Loss: 1.2250\n",
      "Epoch [29/100], Step [1020/1751], Loss: 1.2262\n",
      "Epoch [29/100], Step [1030/1751], Loss: 1.0612\n",
      "Epoch [29/100], Step [1040/1751], Loss: 1.3514\n",
      "Epoch [29/100], Step [1050/1751], Loss: 1.1330\n",
      "Epoch [29/100], Step [1060/1751], Loss: 1.2359\n",
      "Epoch [29/100], Step [1070/1751], Loss: 1.0752\n",
      "Epoch [29/100], Step [1080/1751], Loss: 1.2847\n",
      "Epoch [29/100], Step [1090/1751], Loss: 1.2458\n",
      "Epoch [29/100], Step [1100/1751], Loss: 1.1071\n",
      "Epoch [29/100], Step [1110/1751], Loss: 1.3682\n",
      "Epoch [29/100], Step [1120/1751], Loss: 1.2357\n",
      "Epoch [29/100], Step [1130/1751], Loss: 1.3466\n",
      "Epoch [29/100], Step [1140/1751], Loss: 1.1593\n",
      "Epoch [29/100], Step [1150/1751], Loss: 1.2127\n",
      "Epoch [29/100], Step [1160/1751], Loss: 1.3226\n",
      "Epoch [29/100], Step [1170/1751], Loss: 1.2535\n",
      "Epoch [29/100], Step [1180/1751], Loss: 1.1025\n",
      "Epoch [29/100], Step [1190/1751], Loss: 1.1318\n",
      "Epoch [29/100], Step [1200/1751], Loss: 1.3901\n",
      "Epoch [29/100], Step [1210/1751], Loss: 1.1694\n",
      "Epoch [29/100], Step [1220/1751], Loss: 1.4578\n",
      "Epoch [29/100], Step [1230/1751], Loss: 1.2737\n",
      "Epoch [29/100], Step [1240/1751], Loss: 1.2729\n",
      "Epoch [29/100], Step [1250/1751], Loss: 1.2103\n",
      "Epoch [29/100], Step [1260/1751], Loss: 1.1809\n",
      "Epoch [29/100], Step [1270/1751], Loss: 1.2484\n",
      "Epoch [29/100], Step [1280/1751], Loss: 1.2965\n",
      "Epoch [29/100], Step [1290/1751], Loss: 1.2266\n",
      "Epoch [29/100], Step [1300/1751], Loss: 1.2231\n",
      "Epoch [29/100], Step [1310/1751], Loss: 1.3228\n",
      "Epoch [29/100], Step [1320/1751], Loss: 1.2584\n",
      "Epoch [29/100], Step [1330/1751], Loss: 1.1328\n",
      "Epoch [29/100], Step [1340/1751], Loss: 1.1352\n",
      "Epoch [29/100], Step [1350/1751], Loss: 1.3698\n",
      "Epoch [29/100], Step [1360/1751], Loss: 1.2712\n",
      "Epoch [29/100], Step [1370/1751], Loss: 1.2383\n",
      "Epoch [29/100], Step [1380/1751], Loss: 1.2243\n",
      "Epoch [29/100], Step [1390/1751], Loss: 1.2352\n",
      "Epoch [29/100], Step [1400/1751], Loss: 1.1278\n",
      "Epoch [29/100], Step [1410/1751], Loss: 1.2727\n",
      "Epoch [29/100], Step [1420/1751], Loss: 1.1670\n",
      "Epoch [29/100], Step [1430/1751], Loss: 1.1865\n",
      "Epoch [29/100], Step [1440/1751], Loss: 1.2520\n",
      "Epoch [29/100], Step [1450/1751], Loss: 1.2001\n",
      "Epoch [29/100], Step [1460/1751], Loss: 1.1673\n",
      "Epoch [29/100], Step [1470/1751], Loss: 1.2567\n",
      "Epoch [29/100], Step [1480/1751], Loss: 1.2803\n",
      "Epoch [29/100], Step [1490/1751], Loss: 1.2572\n",
      "Epoch [29/100], Step [1500/1751], Loss: 1.1116\n",
      "Epoch [29/100], Step [1510/1751], Loss: 1.3532\n",
      "Epoch [29/100], Step [1520/1751], Loss: 1.3315\n",
      "Epoch [29/100], Step [1530/1751], Loss: 1.1867\n",
      "Epoch [29/100], Step [1540/1751], Loss: 1.2064\n",
      "Epoch [29/100], Step [1550/1751], Loss: 1.3585\n",
      "Epoch [29/100], Step [1560/1751], Loss: 1.1648\n",
      "Epoch [29/100], Step [1570/1751], Loss: 1.2856\n",
      "Epoch [29/100], Step [1580/1751], Loss: 1.0824\n",
      "Epoch [29/100], Step [1590/1751], Loss: 1.3348\n",
      "Epoch [29/100], Step [1600/1751], Loss: 1.1434\n",
      "Epoch [29/100], Step [1610/1751], Loss: 1.1909\n",
      "Epoch [29/100], Step [1620/1751], Loss: 1.3826\n",
      "Epoch [29/100], Step [1630/1751], Loss: 1.2824\n",
      "Epoch [29/100], Step [1640/1751], Loss: 1.4474\n",
      "Epoch [29/100], Step [1650/1751], Loss: 1.1563\n",
      "Epoch [29/100], Step [1660/1751], Loss: 1.3513\n",
      "Epoch [29/100], Step [1670/1751], Loss: 1.2492\n",
      "Epoch [29/100], Step [1680/1751], Loss: 1.1253\n",
      "Epoch [29/100], Step [1690/1751], Loss: 1.2691\n",
      "Epoch [29/100], Step [1700/1751], Loss: 1.3517\n",
      "Epoch [29/100], Step [1710/1751], Loss: 1.2152\n",
      "Epoch [29/100], Step [1720/1751], Loss: 1.1863\n",
      "Epoch [29/100], Step [1730/1751], Loss: 1.1294\n",
      "Epoch [29/100], Step [1740/1751], Loss: 1.0336\n",
      "Epoch [29/100], Step [1750/1751], Loss: 1.1547\n",
      "Epoch [29/100], Average Loss: 1.2440, Time: 1649.8742s\n",
      "Epoch [30/100], Step [10/1751], Loss: 1.3354\n",
      "Epoch [30/100], Step [20/1751], Loss: 1.4162\n",
      "Epoch [30/100], Step [30/1751], Loss: 1.3204\n",
      "Epoch [30/100], Step [40/1751], Loss: 1.1661\n",
      "Epoch [30/100], Step [50/1751], Loss: 1.0233\n",
      "Epoch [30/100], Step [60/1751], Loss: 1.1395\n",
      "Epoch [30/100], Step [70/1751], Loss: 1.3296\n",
      "Epoch [30/100], Step [80/1751], Loss: 1.1563\n",
      "Epoch [30/100], Step [90/1751], Loss: 1.2616\n",
      "Epoch [30/100], Step [100/1751], Loss: 1.1485\n",
      "Epoch [30/100], Step [110/1751], Loss: 1.0901\n",
      "Epoch [30/100], Step [120/1751], Loss: 1.3084\n",
      "Epoch [30/100], Step [130/1751], Loss: 1.1913\n",
      "Epoch [30/100], Step [140/1751], Loss: 1.0997\n",
      "Epoch [30/100], Step [150/1751], Loss: 1.2049\n",
      "Epoch [30/100], Step [160/1751], Loss: 1.3082\n",
      "Epoch [30/100], Step [170/1751], Loss: 1.1330\n",
      "Epoch [30/100], Step [180/1751], Loss: 1.3365\n",
      "Epoch [30/100], Step [190/1751], Loss: 1.0534\n",
      "Epoch [30/100], Step [200/1751], Loss: 1.2032\n",
      "Epoch [30/100], Step [210/1751], Loss: 1.3179\n",
      "Epoch [30/100], Step [220/1751], Loss: 1.0840\n",
      "Epoch [30/100], Step [230/1751], Loss: 1.2666\n",
      "Epoch [30/100], Step [240/1751], Loss: 1.1692\n",
      "Epoch [30/100], Step [250/1751], Loss: 1.2471\n",
      "Epoch [30/100], Step [260/1751], Loss: 1.4241\n",
      "Epoch [30/100], Step [270/1751], Loss: 1.2971\n",
      "Epoch [30/100], Step [280/1751], Loss: 1.2923\n",
      "Epoch [30/100], Step [290/1751], Loss: 1.2066\n",
      "Epoch [30/100], Step [300/1751], Loss: 1.2424\n",
      "Epoch [30/100], Step [310/1751], Loss: 1.2788\n",
      "Epoch [30/100], Step [320/1751], Loss: 1.1609\n",
      "Epoch [30/100], Step [330/1751], Loss: 1.1309\n",
      "Epoch [30/100], Step [340/1751], Loss: 1.1264\n",
      "Epoch [30/100], Step [350/1751], Loss: 1.2141\n",
      "Epoch [30/100], Step [360/1751], Loss: 1.0416\n",
      "Epoch [30/100], Step [370/1751], Loss: 1.1671\n",
      "Epoch [30/100], Step [380/1751], Loss: 1.2401\n",
      "Epoch [30/100], Step [390/1751], Loss: 1.3824\n",
      "Epoch [30/100], Step [400/1751], Loss: 1.2247\n",
      "Epoch [30/100], Step [410/1751], Loss: 1.1665\n",
      "Epoch [30/100], Step [420/1751], Loss: 1.2560\n",
      "Epoch [30/100], Step [430/1751], Loss: 1.1623\n",
      "Epoch [30/100], Step [440/1751], Loss: 1.2192\n",
      "Epoch [30/100], Step [450/1751], Loss: 1.2696\n",
      "Epoch [30/100], Step [460/1751], Loss: 1.1760\n",
      "Epoch [30/100], Step [470/1751], Loss: 1.3846\n",
      "Epoch [30/100], Step [480/1751], Loss: 1.3308\n",
      "Epoch [30/100], Step [490/1751], Loss: 1.3816\n",
      "Epoch [30/100], Step [500/1751], Loss: 1.2871\n",
      "Epoch [30/100], Step [510/1751], Loss: 1.1913\n",
      "Epoch [30/100], Step [520/1751], Loss: 1.2247\n",
      "Epoch [30/100], Step [530/1751], Loss: 1.2519\n",
      "Epoch [30/100], Step [540/1751], Loss: 1.0714\n",
      "Epoch [30/100], Step [550/1751], Loss: 1.3074\n",
      "Epoch [30/100], Step [560/1751], Loss: 1.2910\n",
      "Epoch [30/100], Step [570/1751], Loss: 1.2717\n",
      "Epoch [30/100], Step [580/1751], Loss: 1.4085\n",
      "Epoch [30/100], Step [590/1751], Loss: 1.3047\n",
      "Epoch [30/100], Step [600/1751], Loss: 1.2411\n",
      "Epoch [30/100], Step [610/1751], Loss: 1.2029\n",
      "Epoch [30/100], Step [620/1751], Loss: 1.2026\n",
      "Epoch [30/100], Step [630/1751], Loss: 1.2415\n",
      "Epoch [30/100], Step [640/1751], Loss: 1.3106\n",
      "Epoch [30/100], Step [650/1751], Loss: 1.3752\n",
      "Epoch [30/100], Step [660/1751], Loss: 1.2803\n",
      "Epoch [30/100], Step [670/1751], Loss: 1.2652\n",
      "Epoch [30/100], Step [680/1751], Loss: 1.2645\n",
      "Epoch [30/100], Step [690/1751], Loss: 1.1362\n",
      "Epoch [30/100], Step [700/1751], Loss: 1.2562\n",
      "Epoch [30/100], Step [710/1751], Loss: 1.1979\n",
      "Epoch [30/100], Step [720/1751], Loss: 1.2554\n",
      "Epoch [30/100], Step [730/1751], Loss: 1.3439\n",
      "Epoch [30/100], Step [740/1751], Loss: 1.3514\n",
      "Epoch [30/100], Step [750/1751], Loss: 1.1153\n",
      "Epoch [30/100], Step [760/1751], Loss: 1.2609\n",
      "Epoch [30/100], Step [770/1751], Loss: 1.4599\n",
      "Epoch [30/100], Step [780/1751], Loss: 1.2568\n",
      "Epoch [30/100], Step [790/1751], Loss: 1.1479\n",
      "Epoch [30/100], Step [800/1751], Loss: 1.0915\n",
      "Epoch [30/100], Step [810/1751], Loss: 1.4806\n",
      "Epoch [30/100], Step [820/1751], Loss: 1.2269\n",
      "Epoch [30/100], Step [830/1751], Loss: 1.3449\n",
      "Epoch [30/100], Step [840/1751], Loss: 1.1748\n",
      "Epoch [30/100], Step [850/1751], Loss: 1.3681\n",
      "Epoch [30/100], Step [860/1751], Loss: 1.2078\n",
      "Epoch [30/100], Step [870/1751], Loss: 1.0961\n",
      "Epoch [30/100], Step [880/1751], Loss: 1.2096\n",
      "Epoch [30/100], Step [890/1751], Loss: 1.1950\n",
      "Epoch [30/100], Step [900/1751], Loss: 1.0666\n",
      "Epoch [30/100], Step [910/1751], Loss: 1.1269\n",
      "Epoch [30/100], Step [920/1751], Loss: 1.2416\n",
      "Epoch [30/100], Step [930/1751], Loss: 1.3363\n",
      "Epoch [30/100], Step [940/1751], Loss: 1.1880\n",
      "Epoch [30/100], Step [950/1751], Loss: 1.3267\n",
      "Epoch [30/100], Step [960/1751], Loss: 1.2467\n",
      "Epoch [30/100], Step [970/1751], Loss: 1.2237\n",
      "Epoch [30/100], Step [980/1751], Loss: 1.2303\n",
      "Epoch [30/100], Step [990/1751], Loss: 1.2857\n",
      "Epoch [30/100], Step [1000/1751], Loss: 1.3216\n",
      "Epoch [30/100], Step [1010/1751], Loss: 1.2592\n",
      "Epoch [30/100], Step [1020/1751], Loss: 1.3552\n",
      "Epoch [30/100], Step [1030/1751], Loss: 1.3730\n",
      "Epoch [30/100], Step [1040/1751], Loss: 1.3741\n",
      "Epoch [30/100], Step [1050/1751], Loss: 1.3802\n",
      "Epoch [30/100], Step [1060/1751], Loss: 1.2572\n",
      "Epoch [30/100], Step [1070/1751], Loss: 1.2482\n",
      "Epoch [30/100], Step [1080/1751], Loss: 1.3460\n",
      "Epoch [30/100], Step [1090/1751], Loss: 1.2557\n",
      "Epoch [30/100], Step [1100/1751], Loss: 1.2025\n",
      "Epoch [30/100], Step [1110/1751], Loss: 1.1170\n",
      "Epoch [30/100], Step [1120/1751], Loss: 1.2427\n",
      "Epoch [30/100], Step [1130/1751], Loss: 1.2335\n",
      "Epoch [30/100], Step [1140/1751], Loss: 1.2096\n",
      "Epoch [30/100], Step [1150/1751], Loss: 1.1960\n",
      "Epoch [30/100], Step [1160/1751], Loss: 1.2427\n",
      "Epoch [30/100], Step [1170/1751], Loss: 1.2452\n",
      "Epoch [30/100], Step [1180/1751], Loss: 1.2621\n",
      "Epoch [30/100], Step [1190/1751], Loss: 1.1637\n",
      "Epoch [30/100], Step [1200/1751], Loss: 1.1651\n",
      "Epoch [30/100], Step [1210/1751], Loss: 1.0601\n",
      "Epoch [30/100], Step [1220/1751], Loss: 1.2664\n",
      "Epoch [30/100], Step [1230/1751], Loss: 1.1514\n",
      "Epoch [30/100], Step [1240/1751], Loss: 1.2211\n",
      "Epoch [30/100], Step [1250/1751], Loss: 1.1734\n",
      "Epoch [30/100], Step [1260/1751], Loss: 1.2703\n",
      "Epoch [30/100], Step [1270/1751], Loss: 1.3334\n",
      "Epoch [30/100], Step [1280/1751], Loss: 1.1023\n",
      "Epoch [30/100], Step [1290/1751], Loss: 1.2774\n",
      "Epoch [30/100], Step [1300/1751], Loss: 1.0917\n",
      "Epoch [30/100], Step [1310/1751], Loss: 1.1611\n",
      "Epoch [30/100], Step [1320/1751], Loss: 1.3630\n",
      "Epoch [30/100], Step [1330/1751], Loss: 1.2548\n",
      "Epoch [30/100], Step [1340/1751], Loss: 1.0761\n",
      "Epoch [30/100], Step [1350/1751], Loss: 1.2183\n",
      "Epoch [30/100], Step [1360/1751], Loss: 1.1366\n",
      "Epoch [30/100], Step [1370/1751], Loss: 1.1476\n",
      "Epoch [30/100], Step [1380/1751], Loss: 1.3123\n",
      "Epoch [30/100], Step [1390/1751], Loss: 1.3125\n",
      "Epoch [30/100], Step [1400/1751], Loss: 1.2235\n",
      "Epoch [30/100], Step [1410/1751], Loss: 1.1341\n",
      "Epoch [30/100], Step [1420/1751], Loss: 1.2863\n",
      "Epoch [30/100], Step [1430/1751], Loss: 1.3333\n",
      "Epoch [30/100], Step [1440/1751], Loss: 1.1781\n",
      "Epoch [30/100], Step [1450/1751], Loss: 1.1052\n",
      "Epoch [30/100], Step [1460/1751], Loss: 1.1757\n",
      "Epoch [30/100], Step [1470/1751], Loss: 1.1457\n",
      "Epoch [30/100], Step [1480/1751], Loss: 1.3253\n",
      "Epoch [30/100], Step [1490/1751], Loss: 1.1461\n",
      "Epoch [30/100], Step [1500/1751], Loss: 1.1690\n",
      "Epoch [30/100], Step [1510/1751], Loss: 1.1991\n",
      "Epoch [30/100], Step [1520/1751], Loss: 1.2955\n",
      "Epoch [30/100], Step [1530/1751], Loss: 1.3352\n",
      "Epoch [30/100], Step [1540/1751], Loss: 1.1945\n",
      "Epoch [30/100], Step [1550/1751], Loss: 1.2199\n",
      "Epoch [30/100], Step [1560/1751], Loss: 1.2673\n",
      "Epoch [30/100], Step [1570/1751], Loss: 1.2538\n",
      "Epoch [30/100], Step [1580/1751], Loss: 1.2421\n",
      "Epoch [30/100], Step [1590/1751], Loss: 1.3664\n",
      "Epoch [30/100], Step [1600/1751], Loss: 1.2477\n",
      "Epoch [30/100], Step [1610/1751], Loss: 1.2387\n",
      "Epoch [30/100], Step [1620/1751], Loss: 1.2414\n",
      "Epoch [30/100], Step [1630/1751], Loss: 1.2236\n",
      "Epoch [30/100], Step [1640/1751], Loss: 1.3215\n",
      "Epoch [30/100], Step [1650/1751], Loss: 1.1629\n",
      "Epoch [30/100], Step [1660/1751], Loss: 1.2771\n",
      "Epoch [30/100], Step [1670/1751], Loss: 1.1471\n",
      "Epoch [30/100], Step [1680/1751], Loss: 1.2968\n",
      "Epoch [30/100], Step [1690/1751], Loss: 1.4254\n",
      "Epoch [30/100], Step [1700/1751], Loss: 1.2443\n",
      "Epoch [30/100], Step [1710/1751], Loss: 1.3144\n",
      "Epoch [30/100], Step [1720/1751], Loss: 1.2831\n",
      "Epoch [30/100], Step [1730/1751], Loss: 1.2698\n",
      "Epoch [30/100], Step [1740/1751], Loss: 1.2460\n",
      "Epoch [30/100], Step [1750/1751], Loss: 1.3511\n",
      "Epoch [30/100], Average Loss: 1.2426, Time: 1648.6137s\n",
      "Epoch [31/100], Step [10/1751], Loss: 1.2565\n",
      "Epoch [31/100], Step [20/1751], Loss: 1.3235\n",
      "Epoch [31/100], Step [30/1751], Loss: 1.4263\n",
      "Epoch [31/100], Step [40/1751], Loss: 1.3288\n",
      "Epoch [31/100], Step [50/1751], Loss: 1.1935\n",
      "Epoch [31/100], Step [60/1751], Loss: 1.1818\n",
      "Epoch [31/100], Step [70/1751], Loss: 1.1690\n",
      "Epoch [31/100], Step [80/1751], Loss: 1.1244\n",
      "Epoch [31/100], Step [90/1751], Loss: 1.2094\n",
      "Epoch [31/100], Step [100/1751], Loss: 1.2647\n",
      "Epoch [31/100], Step [110/1751], Loss: 1.3287\n",
      "Epoch [31/100], Step [120/1751], Loss: 1.2994\n",
      "Epoch [31/100], Step [130/1751], Loss: 1.3445\n",
      "Epoch [31/100], Step [140/1751], Loss: 1.3288\n",
      "Epoch [31/100], Step [150/1751], Loss: 1.2739\n",
      "Epoch [31/100], Step [160/1751], Loss: 1.2756\n",
      "Epoch [31/100], Step [170/1751], Loss: 1.1466\n",
      "Epoch [31/100], Step [180/1751], Loss: 1.2331\n",
      "Epoch [31/100], Step [190/1751], Loss: 1.1622\n",
      "Epoch [31/100], Step [200/1751], Loss: 1.3247\n",
      "Epoch [31/100], Step [210/1751], Loss: 1.2605\n",
      "Epoch [31/100], Step [220/1751], Loss: 1.1488\n",
      "Epoch [31/100], Step [230/1751], Loss: 1.0308\n",
      "Epoch [31/100], Step [240/1751], Loss: 1.3057\n",
      "Epoch [31/100], Step [250/1751], Loss: 1.1931\n",
      "Epoch [31/100], Step [260/1751], Loss: 1.2214\n",
      "Epoch [31/100], Step [270/1751], Loss: 1.2190\n",
      "Epoch [31/100], Step [280/1751], Loss: 1.3492\n",
      "Epoch [31/100], Step [290/1751], Loss: 1.2769\n",
      "Epoch [31/100], Step [300/1751], Loss: 1.3045\n",
      "Epoch [31/100], Step [310/1751], Loss: 1.3437\n",
      "Epoch [31/100], Step [320/1751], Loss: 1.1409\n",
      "Epoch [31/100], Step [330/1751], Loss: 1.1035\n",
      "Epoch [31/100], Step [340/1751], Loss: 1.3729\n",
      "Epoch [31/100], Step [350/1751], Loss: 1.4560\n",
      "Epoch [31/100], Step [360/1751], Loss: 1.3059\n",
      "Epoch [31/100], Step [370/1751], Loss: 1.1090\n",
      "Epoch [31/100], Step [380/1751], Loss: 1.1810\n",
      "Epoch [31/100], Step [390/1751], Loss: 1.1955\n",
      "Epoch [31/100], Step [400/1751], Loss: 1.1492\n",
      "Epoch [31/100], Step [410/1751], Loss: 1.3083\n",
      "Epoch [31/100], Step [420/1751], Loss: 1.1529\n",
      "Epoch [31/100], Step [430/1751], Loss: 1.2901\n",
      "Epoch [31/100], Step [440/1751], Loss: 1.1049\n",
      "Epoch [31/100], Step [450/1751], Loss: 1.2260\n",
      "Epoch [31/100], Step [460/1751], Loss: 1.2918\n",
      "Epoch [31/100], Step [470/1751], Loss: 1.2165\n",
      "Epoch [31/100], Step [480/1751], Loss: 1.1955\n",
      "Epoch [31/100], Step [490/1751], Loss: 1.2637\n",
      "Epoch [31/100], Step [500/1751], Loss: 1.2006\n",
      "Epoch [31/100], Step [510/1751], Loss: 1.1452\n",
      "Epoch [31/100], Step [520/1751], Loss: 1.3204\n",
      "Epoch [31/100], Step [530/1751], Loss: 1.1500\n",
      "Epoch [31/100], Step [540/1751], Loss: 1.3600\n",
      "Epoch [31/100], Step [550/1751], Loss: 1.3850\n",
      "Epoch [31/100], Step [560/1751], Loss: 1.3348\n",
      "Epoch [31/100], Step [570/1751], Loss: 1.1498\n",
      "Epoch [31/100], Step [580/1751], Loss: 1.2363\n",
      "Epoch [31/100], Step [590/1751], Loss: 1.2209\n",
      "Epoch [31/100], Step [600/1751], Loss: 1.3887\n",
      "Epoch [31/100], Step [610/1751], Loss: 1.1204\n",
      "Epoch [31/100], Step [620/1751], Loss: 1.0686\n",
      "Epoch [31/100], Step [630/1751], Loss: 1.2415\n",
      "Epoch [31/100], Step [640/1751], Loss: 1.1967\n",
      "Epoch [31/100], Step [650/1751], Loss: 1.1679\n",
      "Epoch [31/100], Step [660/1751], Loss: 1.3317\n",
      "Epoch [31/100], Step [670/1751], Loss: 1.2507\n",
      "Epoch [31/100], Step [680/1751], Loss: 1.3200\n",
      "Epoch [31/100], Step [690/1751], Loss: 1.1507\n",
      "Epoch [31/100], Step [700/1751], Loss: 1.3465\n",
      "Epoch [31/100], Step [710/1751], Loss: 1.2795\n",
      "Epoch [31/100], Step [720/1751], Loss: 1.3962\n",
      "Epoch [31/100], Step [730/1751], Loss: 1.2443\n",
      "Epoch [31/100], Step [740/1751], Loss: 1.1309\n",
      "Epoch [31/100], Step [750/1751], Loss: 1.1060\n",
      "Epoch [31/100], Step [760/1751], Loss: 1.1787\n",
      "Epoch [31/100], Step [770/1751], Loss: 1.3966\n",
      "Epoch [31/100], Step [780/1751], Loss: 1.3123\n",
      "Epoch [31/100], Step [790/1751], Loss: 1.1576\n",
      "Epoch [31/100], Step [800/1751], Loss: 1.2569\n",
      "Epoch [31/100], Step [810/1751], Loss: 1.1080\n",
      "Epoch [31/100], Step [820/1751], Loss: 1.1830\n",
      "Epoch [31/100], Step [830/1751], Loss: 1.1960\n",
      "Epoch [31/100], Step [840/1751], Loss: 1.1756\n",
      "Epoch [31/100], Step [850/1751], Loss: 1.1787\n",
      "Epoch [31/100], Step [860/1751], Loss: 1.2019\n",
      "Epoch [31/100], Step [870/1751], Loss: 1.0718\n",
      "Epoch [31/100], Step [880/1751], Loss: 1.1655\n",
      "Epoch [31/100], Step [890/1751], Loss: 1.2163\n",
      "Epoch [31/100], Step [900/1751], Loss: 1.3016\n",
      "Epoch [31/100], Step [910/1751], Loss: 1.1575\n",
      "Epoch [31/100], Step [920/1751], Loss: 1.1331\n",
      "Epoch [31/100], Step [930/1751], Loss: 1.0573\n",
      "Epoch [31/100], Step [940/1751], Loss: 1.2076\n",
      "Epoch [31/100], Step [950/1751], Loss: 1.3846\n",
      "Epoch [31/100], Step [960/1751], Loss: 1.2617\n",
      "Epoch [31/100], Step [970/1751], Loss: 1.2154\n",
      "Epoch [31/100], Step [980/1751], Loss: 1.2440\n",
      "Epoch [31/100], Step [990/1751], Loss: 1.1392\n",
      "Epoch [31/100], Step [1000/1751], Loss: 1.2342\n",
      "Epoch [31/100], Step [1010/1751], Loss: 1.2130\n",
      "Epoch [31/100], Step [1020/1751], Loss: 1.2722\n",
      "Epoch [31/100], Step [1030/1751], Loss: 1.2472\n",
      "Epoch [31/100], Step [1040/1751], Loss: 1.0992\n",
      "Epoch [31/100], Step [1050/1751], Loss: 1.2605\n",
      "Epoch [31/100], Step [1060/1751], Loss: 1.2607\n",
      "Epoch [31/100], Step [1070/1751], Loss: 1.1306\n",
      "Epoch [31/100], Step [1080/1751], Loss: 1.3388\n",
      "Epoch [31/100], Step [1090/1751], Loss: 1.2727\n",
      "Epoch [31/100], Step [1100/1751], Loss: 1.1544\n",
      "Epoch [31/100], Step [1110/1751], Loss: 1.1463\n",
      "Epoch [31/100], Step [1120/1751], Loss: 1.3129\n",
      "Epoch [31/100], Step [1130/1751], Loss: 1.2999\n",
      "Epoch [31/100], Step [1140/1751], Loss: 1.3488\n",
      "Epoch [31/100], Step [1150/1751], Loss: 1.2601\n",
      "Epoch [31/100], Step [1160/1751], Loss: 1.1193\n",
      "Epoch [31/100], Step [1170/1751], Loss: 1.2232\n",
      "Epoch [31/100], Step [1180/1751], Loss: 1.2375\n",
      "Epoch [31/100], Step [1190/1751], Loss: 1.2262\n",
      "Epoch [31/100], Step [1200/1751], Loss: 1.3005\n",
      "Epoch [31/100], Step [1210/1751], Loss: 1.2545\n",
      "Epoch [31/100], Step [1220/1751], Loss: 1.2561\n",
      "Epoch [31/100], Step [1230/1751], Loss: 1.2329\n",
      "Epoch [31/100], Step [1240/1751], Loss: 1.2103\n",
      "Epoch [31/100], Step [1250/1751], Loss: 1.3223\n",
      "Epoch [31/100], Step [1260/1751], Loss: 1.4559\n",
      "Epoch [31/100], Step [1270/1751], Loss: 1.1516\n",
      "Epoch [31/100], Step [1280/1751], Loss: 1.1793\n",
      "Epoch [31/100], Step [1290/1751], Loss: 1.4520\n",
      "Epoch [31/100], Step [1300/1751], Loss: 1.2439\n",
      "Epoch [31/100], Step [1310/1751], Loss: 1.3430\n",
      "Epoch [31/100], Step [1320/1751], Loss: 1.1886\n",
      "Epoch [31/100], Step [1330/1751], Loss: 1.2258\n",
      "Epoch [31/100], Step [1340/1751], Loss: 1.1831\n",
      "Epoch [31/100], Step [1350/1751], Loss: 1.2394\n",
      "Epoch [31/100], Step [1360/1751], Loss: 1.2974\n",
      "Epoch [31/100], Step [1370/1751], Loss: 1.2579\n",
      "Epoch [31/100], Step [1380/1751], Loss: 1.2962\n",
      "Epoch [31/100], Step [1390/1751], Loss: 1.1313\n",
      "Epoch [31/100], Step [1400/1751], Loss: 1.3745\n",
      "Epoch [31/100], Step [1410/1751], Loss: 1.3937\n",
      "Epoch [31/100], Step [1420/1751], Loss: 1.2497\n",
      "Epoch [31/100], Step [1430/1751], Loss: 1.3122\n",
      "Epoch [31/100], Step [1440/1751], Loss: 1.2946\n",
      "Epoch [31/100], Step [1450/1751], Loss: 1.2290\n",
      "Epoch [31/100], Step [1460/1751], Loss: 1.2070\n",
      "Epoch [31/100], Step [1470/1751], Loss: 1.2055\n",
      "Epoch [31/100], Step [1480/1751], Loss: 1.4189\n",
      "Epoch [31/100], Step [1490/1751], Loss: 1.2316\n",
      "Epoch [31/100], Step [1500/1751], Loss: 1.2706\n",
      "Epoch [31/100], Step [1510/1751], Loss: 1.2910\n",
      "Epoch [31/100], Step [1520/1751], Loss: 1.3358\n",
      "Epoch [31/100], Step [1530/1751], Loss: 1.4280\n",
      "Epoch [31/100], Step [1540/1751], Loss: 1.1687\n",
      "Epoch [31/100], Step [1550/1751], Loss: 1.3148\n",
      "Epoch [31/100], Step [1560/1751], Loss: 1.2271\n",
      "Epoch [31/100], Step [1570/1751], Loss: 1.3168\n",
      "Epoch [31/100], Step [1580/1751], Loss: 1.2204\n",
      "Epoch [31/100], Step [1590/1751], Loss: 1.3486\n",
      "Epoch [31/100], Step [1600/1751], Loss: 1.3327\n",
      "Epoch [31/100], Step [1610/1751], Loss: 1.2191\n",
      "Epoch [31/100], Step [1620/1751], Loss: 1.2095\n",
      "Epoch [31/100], Step [1630/1751], Loss: 1.1159\n",
      "Epoch [31/100], Step [1640/1751], Loss: 1.1505\n",
      "Epoch [31/100], Step [1650/1751], Loss: 1.3018\n",
      "Epoch [31/100], Step [1660/1751], Loss: 1.2938\n",
      "Epoch [31/100], Step [1670/1751], Loss: 1.4052\n",
      "Epoch [31/100], Step [1680/1751], Loss: 1.3458\n",
      "Epoch [31/100], Step [1690/1751], Loss: 1.2218\n",
      "Epoch [31/100], Step [1700/1751], Loss: 1.3030\n",
      "Epoch [31/100], Step [1710/1751], Loss: 1.3127\n",
      "Epoch [31/100], Step [1720/1751], Loss: 1.1711\n",
      "Epoch [31/100], Step [1730/1751], Loss: 1.1753\n",
      "Epoch [31/100], Step [1740/1751], Loss: 1.1706\n",
      "Epoch [31/100], Step [1750/1751], Loss: 1.3169\n",
      "Epoch [31/100], Average Loss: 1.2394, Time: 1649.8380s\n",
      "Epoch [32/100], Step [10/1751], Loss: 1.2869\n",
      "Epoch [32/100], Step [20/1751], Loss: 1.2286\n",
      "Epoch [32/100], Step [30/1751], Loss: 1.3382\n",
      "Epoch [32/100], Step [40/1751], Loss: 1.1863\n",
      "Epoch [32/100], Step [50/1751], Loss: 1.3102\n",
      "Epoch [32/100], Step [60/1751], Loss: 1.1784\n",
      "Epoch [32/100], Step [70/1751], Loss: 1.2715\n",
      "Epoch [32/100], Step [80/1751], Loss: 1.3451\n",
      "Epoch [32/100], Step [90/1751], Loss: 1.3317\n",
      "Epoch [32/100], Step [100/1751], Loss: 1.3295\n",
      "Epoch [32/100], Step [110/1751], Loss: 1.2500\n",
      "Epoch [32/100], Step [120/1751], Loss: 1.1034\n",
      "Epoch [32/100], Step [130/1751], Loss: 1.2258\n",
      "Epoch [32/100], Step [140/1751], Loss: 1.2602\n",
      "Epoch [32/100], Step [150/1751], Loss: 1.2083\n",
      "Epoch [32/100], Step [160/1751], Loss: 1.2362\n",
      "Epoch [32/100], Step [170/1751], Loss: 1.1591\n",
      "Epoch [32/100], Step [180/1751], Loss: 1.2689\n",
      "Epoch [32/100], Step [190/1751], Loss: 1.2706\n",
      "Epoch [32/100], Step [200/1751], Loss: 1.2272\n",
      "Epoch [32/100], Step [210/1751], Loss: 1.1839\n",
      "Epoch [32/100], Step [220/1751], Loss: 1.2761\n",
      "Epoch [32/100], Step [230/1751], Loss: 1.3017\n",
      "Epoch [32/100], Step [240/1751], Loss: 1.1870\n",
      "Epoch [32/100], Step [250/1751], Loss: 1.3251\n",
      "Epoch [32/100], Step [260/1751], Loss: 1.0700\n",
      "Epoch [32/100], Step [270/1751], Loss: 1.2279\n",
      "Epoch [32/100], Step [280/1751], Loss: 1.1939\n",
      "Epoch [32/100], Step [290/1751], Loss: 1.1632\n",
      "Epoch [32/100], Step [300/1751], Loss: 1.3241\n",
      "Epoch [32/100], Step [310/1751], Loss: 1.2338\n",
      "Epoch [32/100], Step [320/1751], Loss: 1.2384\n",
      "Epoch [32/100], Step [330/1751], Loss: 1.2735\n",
      "Epoch [32/100], Step [340/1751], Loss: 1.1758\n",
      "Epoch [32/100], Step [350/1751], Loss: 1.1629\n",
      "Epoch [32/100], Step [360/1751], Loss: 1.3638\n",
      "Epoch [32/100], Step [370/1751], Loss: 1.2927\n",
      "Epoch [32/100], Step [380/1751], Loss: 1.1245\n",
      "Epoch [32/100], Step [390/1751], Loss: 1.1403\n",
      "Epoch [32/100], Step [400/1751], Loss: 1.3338\n",
      "Epoch [32/100], Step [410/1751], Loss: 1.2108\n",
      "Epoch [32/100], Step [420/1751], Loss: 1.1864\n",
      "Epoch [32/100], Step [430/1751], Loss: 1.2472\n",
      "Epoch [32/100], Step [440/1751], Loss: 1.2818\n",
      "Epoch [32/100], Step [450/1751], Loss: 1.1879\n",
      "Epoch [32/100], Step [460/1751], Loss: 1.2684\n",
      "Epoch [32/100], Step [470/1751], Loss: 1.2607\n",
      "Epoch [32/100], Step [480/1751], Loss: 1.2495\n",
      "Epoch [32/100], Step [490/1751], Loss: 1.0738\n",
      "Epoch [32/100], Step [500/1751], Loss: 1.2023\n",
      "Epoch [32/100], Step [510/1751], Loss: 1.3275\n",
      "Epoch [32/100], Step [520/1751], Loss: 1.2097\n",
      "Epoch [32/100], Step [530/1751], Loss: 1.1746\n",
      "Epoch [32/100], Step [540/1751], Loss: 1.1416\n",
      "Epoch [32/100], Step [550/1751], Loss: 1.1916\n",
      "Epoch [32/100], Step [560/1751], Loss: 1.3445\n",
      "Epoch [32/100], Step [570/1751], Loss: 1.2548\n",
      "Epoch [32/100], Step [580/1751], Loss: 1.1921\n",
      "Epoch [32/100], Step [590/1751], Loss: 1.3864\n",
      "Epoch [32/100], Step [600/1751], Loss: 1.3207\n",
      "Epoch [32/100], Step [610/1751], Loss: 1.2906\n",
      "Epoch [32/100], Step [620/1751], Loss: 1.2985\n",
      "Epoch [32/100], Step [630/1751], Loss: 1.2339\n",
      "Epoch [32/100], Step [640/1751], Loss: 1.2090\n",
      "Epoch [32/100], Step [650/1751], Loss: 1.1905\n",
      "Epoch [32/100], Step [660/1751], Loss: 1.2607\n",
      "Epoch [32/100], Step [670/1751], Loss: 1.4296\n",
      "Epoch [32/100], Step [680/1751], Loss: 1.2224\n",
      "Epoch [32/100], Step [690/1751], Loss: 1.3341\n",
      "Epoch [32/100], Step [700/1751], Loss: 1.4221\n",
      "Epoch [32/100], Step [710/1751], Loss: 1.2446\n",
      "Epoch [32/100], Step [720/1751], Loss: 1.1374\n",
      "Epoch [32/100], Step [730/1751], Loss: 1.1757\n",
      "Epoch [32/100], Step [740/1751], Loss: 1.3379\n",
      "Epoch [32/100], Step [750/1751], Loss: 1.2391\n",
      "Epoch [32/100], Step [760/1751], Loss: 1.2850\n",
      "Epoch [32/100], Step [770/1751], Loss: 1.2313\n",
      "Epoch [32/100], Step [780/1751], Loss: 1.2538\n",
      "Epoch [32/100], Step [790/1751], Loss: 1.2813\n",
      "Epoch [32/100], Step [800/1751], Loss: 1.3274\n",
      "Epoch [32/100], Step [810/1751], Loss: 1.1499\n",
      "Epoch [32/100], Step [820/1751], Loss: 1.3171\n",
      "Epoch [32/100], Step [830/1751], Loss: 1.1587\n",
      "Epoch [32/100], Step [840/1751], Loss: 1.1861\n",
      "Epoch [32/100], Step [850/1751], Loss: 1.3495\n",
      "Epoch [32/100], Step [860/1751], Loss: 1.2858\n",
      "Epoch [32/100], Step [870/1751], Loss: 1.3085\n",
      "Epoch [32/100], Step [880/1751], Loss: 1.0573\n",
      "Epoch [32/100], Step [890/1751], Loss: 1.1692\n",
      "Epoch [32/100], Step [900/1751], Loss: 1.1932\n",
      "Epoch [32/100], Step [910/1751], Loss: 1.2726\n",
      "Epoch [32/100], Step [920/1751], Loss: 1.2255\n",
      "Epoch [32/100], Step [930/1751], Loss: 1.3124\n",
      "Epoch [32/100], Step [940/1751], Loss: 1.3496\n",
      "Epoch [32/100], Step [950/1751], Loss: 1.1104\n",
      "Epoch [32/100], Step [960/1751], Loss: 1.3122\n",
      "Epoch [32/100], Step [970/1751], Loss: 1.1636\n",
      "Epoch [32/100], Step [980/1751], Loss: 1.3842\n",
      "Epoch [32/100], Step [990/1751], Loss: 1.1676\n",
      "Epoch [32/100], Step [1000/1751], Loss: 1.1771\n",
      "Epoch [32/100], Step [1010/1751], Loss: 1.2855\n",
      "Epoch [32/100], Step [1020/1751], Loss: 1.2777\n",
      "Epoch [32/100], Step [1030/1751], Loss: 1.2619\n",
      "Epoch [32/100], Step [1040/1751], Loss: 1.2604\n",
      "Epoch [32/100], Step [1050/1751], Loss: 1.2479\n",
      "Epoch [32/100], Step [1060/1751], Loss: 1.2443\n",
      "Epoch [32/100], Step [1070/1751], Loss: 1.3593\n",
      "Epoch [32/100], Step [1080/1751], Loss: 1.1479\n",
      "Epoch [32/100], Step [1090/1751], Loss: 1.1758\n",
      "Epoch [32/100], Step [1100/1751], Loss: 1.1893\n",
      "Epoch [32/100], Step [1110/1751], Loss: 1.2054\n",
      "Epoch [32/100], Step [1120/1751], Loss: 1.3869\n",
      "Epoch [32/100], Step [1130/1751], Loss: 1.3376\n",
      "Epoch [32/100], Step [1140/1751], Loss: 1.1756\n",
      "Epoch [32/100], Step [1150/1751], Loss: 1.2145\n",
      "Epoch [32/100], Step [1160/1751], Loss: 1.2660\n",
      "Epoch [32/100], Step [1170/1751], Loss: 1.2080\n",
      "Epoch [32/100], Step [1180/1751], Loss: 1.1184\n",
      "Epoch [32/100], Step [1190/1751], Loss: 1.3459\n",
      "Epoch [32/100], Step [1200/1751], Loss: 1.2921\n",
      "Epoch [32/100], Step [1210/1751], Loss: 1.2733\n",
      "Epoch [32/100], Step [1220/1751], Loss: 1.1019\n",
      "Epoch [32/100], Step [1230/1751], Loss: 1.2499\n",
      "Epoch [32/100], Step [1240/1751], Loss: 1.3691\n",
      "Epoch [32/100], Step [1250/1751], Loss: 1.1588\n",
      "Epoch [32/100], Step [1260/1751], Loss: 1.2084\n",
      "Epoch [32/100], Step [1270/1751], Loss: 1.2070\n",
      "Epoch [32/100], Step [1280/1751], Loss: 1.3999\n",
      "Epoch [32/100], Step [1290/1751], Loss: 1.2801\n",
      "Epoch [32/100], Step [1300/1751], Loss: 1.1261\n",
      "Epoch [32/100], Step [1310/1751], Loss: 1.2711\n",
      "Epoch [32/100], Step [1320/1751], Loss: 1.0927\n",
      "Epoch [32/100], Step [1330/1751], Loss: 1.2958\n",
      "Epoch [32/100], Step [1340/1751], Loss: 1.2227\n",
      "Epoch [32/100], Step [1350/1751], Loss: 1.1280\n",
      "Epoch [32/100], Step [1360/1751], Loss: 1.2751\n",
      "Epoch [32/100], Step [1370/1751], Loss: 1.2119\n",
      "Epoch [32/100], Step [1380/1751], Loss: 1.2155\n",
      "Epoch [32/100], Step [1390/1751], Loss: 1.3590\n",
      "Epoch [32/100], Step [1400/1751], Loss: 1.3804\n",
      "Epoch [32/100], Step [1410/1751], Loss: 1.3101\n",
      "Epoch [32/100], Step [1420/1751], Loss: 1.1582\n",
      "Epoch [32/100], Step [1430/1751], Loss: 1.2746\n",
      "Epoch [32/100], Step [1440/1751], Loss: 1.3492\n",
      "Epoch [32/100], Step [1450/1751], Loss: 1.2494\n",
      "Epoch [32/100], Step [1460/1751], Loss: 1.2841\n",
      "Epoch [32/100], Step [1470/1751], Loss: 1.1829\n",
      "Epoch [32/100], Step [1480/1751], Loss: 1.3453\n",
      "Epoch [32/100], Step [1490/1751], Loss: 1.3243\n",
      "Epoch [32/100], Step [1500/1751], Loss: 1.2141\n",
      "Epoch [32/100], Step [1510/1751], Loss: 1.2010\n",
      "Epoch [32/100], Step [1520/1751], Loss: 1.3555\n",
      "Epoch [32/100], Step [1530/1751], Loss: 1.2732\n",
      "Epoch [32/100], Step [1540/1751], Loss: 1.0842\n",
      "Epoch [32/100], Step [1550/1751], Loss: 1.3629\n",
      "Epoch [32/100], Step [1560/1751], Loss: 1.0869\n",
      "Epoch [32/100], Step [1570/1751], Loss: 1.2630\n",
      "Epoch [32/100], Step [1580/1751], Loss: 1.1807\n",
      "Epoch [32/100], Step [1590/1751], Loss: 1.1335\n",
      "Epoch [32/100], Step [1600/1751], Loss: 1.2016\n",
      "Epoch [32/100], Step [1610/1751], Loss: 1.3036\n",
      "Epoch [32/100], Step [1620/1751], Loss: 1.2165\n",
      "Epoch [32/100], Step [1630/1751], Loss: 1.3005\n",
      "Epoch [32/100], Step [1640/1751], Loss: 1.2646\n",
      "Epoch [32/100], Step [1650/1751], Loss: 1.1570\n",
      "Epoch [32/100], Step [1660/1751], Loss: 1.3212\n",
      "Epoch [32/100], Step [1670/1751], Loss: 1.2110\n",
      "Epoch [32/100], Step [1680/1751], Loss: 1.2575\n",
      "Epoch [32/100], Step [1690/1751], Loss: 1.1289\n",
      "Epoch [32/100], Step [1700/1751], Loss: 1.1424\n",
      "Epoch [32/100], Step [1710/1751], Loss: 1.3853\n",
      "Epoch [32/100], Step [1720/1751], Loss: 1.3178\n",
      "Epoch [32/100], Step [1730/1751], Loss: 1.2410\n",
      "Epoch [32/100], Step [1740/1751], Loss: 1.3854\n",
      "Epoch [32/100], Step [1750/1751], Loss: 1.2755\n",
      "Epoch [32/100], Average Loss: 1.2364, Time: 1649.9711s\n",
      "Epoch [33/100], Step [10/1751], Loss: 1.2842\n",
      "Epoch [33/100], Step [20/1751], Loss: 1.2781\n",
      "Epoch [33/100], Step [30/1751], Loss: 1.2589\n",
      "Epoch [33/100], Step [40/1751], Loss: 1.2174\n",
      "Epoch [33/100], Step [50/1751], Loss: 1.3643\n",
      "Epoch [33/100], Step [60/1751], Loss: 1.2445\n",
      "Epoch [33/100], Step [70/1751], Loss: 1.1763\n",
      "Epoch [33/100], Step [80/1751], Loss: 1.0927\n",
      "Epoch [33/100], Step [90/1751], Loss: 1.1740\n",
      "Epoch [33/100], Step [100/1751], Loss: 1.3847\n",
      "Epoch [33/100], Step [110/1751], Loss: 1.2852\n",
      "Epoch [33/100], Step [120/1751], Loss: 1.3341\n",
      "Epoch [33/100], Step [130/1751], Loss: 1.0692\n",
      "Epoch [33/100], Step [140/1751], Loss: 1.2870\n",
      "Epoch [33/100], Step [150/1751], Loss: 1.1005\n",
      "Epoch [33/100], Step [160/1751], Loss: 1.0684\n",
      "Epoch [33/100], Step [170/1751], Loss: 1.1868\n",
      "Epoch [33/100], Step [180/1751], Loss: 1.1795\n",
      "Epoch [33/100], Step [190/1751], Loss: 1.1769\n",
      "Epoch [33/100], Step [200/1751], Loss: 1.2142\n",
      "Epoch [33/100], Step [210/1751], Loss: 1.2258\n",
      "Epoch [33/100], Step [220/1751], Loss: 1.3430\n",
      "Epoch [33/100], Step [230/1751], Loss: 1.1201\n",
      "Epoch [33/100], Step [240/1751], Loss: 1.2931\n",
      "Epoch [33/100], Step [250/1751], Loss: 1.3723\n",
      "Epoch [33/100], Step [260/1751], Loss: 1.2612\n",
      "Epoch [33/100], Step [270/1751], Loss: 1.2275\n",
      "Epoch [33/100], Step [280/1751], Loss: 1.3074\n",
      "Epoch [33/100], Step [290/1751], Loss: 1.1014\n",
      "Epoch [33/100], Step [300/1751], Loss: 1.1360\n",
      "Epoch [33/100], Step [310/1751], Loss: 1.1519\n",
      "Epoch [33/100], Step [320/1751], Loss: 1.2795\n",
      "Epoch [33/100], Step [330/1751], Loss: 1.2335\n",
      "Epoch [33/100], Step [340/1751], Loss: 1.2883\n",
      "Epoch [33/100], Step [350/1751], Loss: 1.3057\n",
      "Epoch [33/100], Step [360/1751], Loss: 1.2850\n",
      "Epoch [33/100], Step [370/1751], Loss: 1.4146\n",
      "Epoch [33/100], Step [380/1751], Loss: 1.2159\n",
      "Epoch [33/100], Step [390/1751], Loss: 1.1716\n",
      "Epoch [33/100], Step [400/1751], Loss: 1.2146\n",
      "Epoch [33/100], Step [410/1751], Loss: 1.3194\n",
      "Epoch [33/100], Step [420/1751], Loss: 1.1113\n",
      "Epoch [33/100], Step [430/1751], Loss: 1.1118\n",
      "Epoch [33/100], Step [440/1751], Loss: 1.1733\n",
      "Epoch [33/100], Step [450/1751], Loss: 1.3244\n",
      "Epoch [33/100], Step [460/1751], Loss: 1.0343\n",
      "Epoch [33/100], Step [470/1751], Loss: 1.1707\n",
      "Epoch [33/100], Step [480/1751], Loss: 1.0999\n",
      "Epoch [33/100], Step [490/1751], Loss: 1.2551\n",
      "Epoch [33/100], Step [500/1751], Loss: 1.1918\n",
      "Epoch [33/100], Step [510/1751], Loss: 1.1078\n",
      "Epoch [33/100], Step [520/1751], Loss: 1.2867\n",
      "Epoch [33/100], Step [530/1751], Loss: 1.1572\n",
      "Epoch [33/100], Step [540/1751], Loss: 1.3367\n",
      "Epoch [33/100], Step [550/1751], Loss: 1.2250\n",
      "Epoch [33/100], Step [560/1751], Loss: 1.3486\n",
      "Epoch [33/100], Step [570/1751], Loss: 1.2639\n",
      "Epoch [33/100], Step [580/1751], Loss: 1.1715\n",
      "Epoch [33/100], Step [590/1751], Loss: 1.3238\n",
      "Epoch [33/100], Step [600/1751], Loss: 1.2928\n",
      "Epoch [33/100], Step [610/1751], Loss: 1.2588\n",
      "Epoch [33/100], Step [620/1751], Loss: 1.2221\n",
      "Epoch [33/100], Step [630/1751], Loss: 1.2142\n",
      "Epoch [33/100], Step [640/1751], Loss: 1.4465\n",
      "Epoch [33/100], Step [650/1751], Loss: 1.2240\n",
      "Epoch [33/100], Step [660/1751], Loss: 1.2272\n",
      "Epoch [33/100], Step [670/1751], Loss: 1.0235\n",
      "Epoch [33/100], Step [680/1751], Loss: 1.2458\n",
      "Epoch [33/100], Step [690/1751], Loss: 1.2985\n",
      "Epoch [33/100], Step [700/1751], Loss: 1.1955\n",
      "Epoch [33/100], Step [710/1751], Loss: 1.2707\n",
      "Epoch [33/100], Step [720/1751], Loss: 1.2652\n",
      "Epoch [33/100], Step [730/1751], Loss: 1.1684\n",
      "Epoch [33/100], Step [740/1751], Loss: 1.2292\n",
      "Epoch [33/100], Step [750/1751], Loss: 1.3142\n",
      "Epoch [33/100], Step [760/1751], Loss: 1.2131\n",
      "Epoch [33/100], Step [770/1751], Loss: 1.1916\n",
      "Epoch [33/100], Step [780/1751], Loss: 1.2391\n",
      "Epoch [33/100], Step [790/1751], Loss: 1.2759\n",
      "Epoch [33/100], Step [800/1751], Loss: 1.1619\n",
      "Epoch [33/100], Step [810/1751], Loss: 1.3612\n",
      "Epoch [33/100], Step [820/1751], Loss: 1.2260\n",
      "Epoch [33/100], Step [830/1751], Loss: 1.3611\n",
      "Epoch [33/100], Step [840/1751], Loss: 1.2287\n",
      "Epoch [33/100], Step [850/1751], Loss: 1.2974\n",
      "Epoch [33/100], Step [860/1751], Loss: 1.2750\n",
      "Epoch [33/100], Step [870/1751], Loss: 1.3116\n",
      "Epoch [33/100], Step [880/1751], Loss: 1.2043\n",
      "Epoch [33/100], Step [890/1751], Loss: 1.1994\n",
      "Epoch [33/100], Step [900/1751], Loss: 1.2017\n",
      "Epoch [33/100], Step [910/1751], Loss: 1.1418\n",
      "Epoch [33/100], Step [920/1751], Loss: 1.1032\n",
      "Epoch [33/100], Step [930/1751], Loss: 1.1708\n",
      "Epoch [33/100], Step [940/1751], Loss: 1.2955\n",
      "Epoch [33/100], Step [950/1751], Loss: 1.2921\n",
      "Epoch [33/100], Step [960/1751], Loss: 1.1238\n",
      "Epoch [33/100], Step [970/1751], Loss: 1.2448\n",
      "Epoch [33/100], Step [980/1751], Loss: 1.3001\n",
      "Epoch [33/100], Step [990/1751], Loss: 1.2051\n",
      "Epoch [33/100], Step [1000/1751], Loss: 1.2725\n",
      "Epoch [33/100], Step [1010/1751], Loss: 1.2624\n",
      "Epoch [33/100], Step [1020/1751], Loss: 1.1673\n",
      "Epoch [33/100], Step [1030/1751], Loss: 1.2433\n",
      "Epoch [33/100], Step [1040/1751], Loss: 1.2707\n",
      "Epoch [33/100], Step [1050/1751], Loss: 1.2211\n",
      "Epoch [33/100], Step [1060/1751], Loss: 1.1840\n",
      "Epoch [33/100], Step [1070/1751], Loss: 1.2374\n",
      "Epoch [33/100], Step [1080/1751], Loss: 1.1424\n",
      "Epoch [33/100], Step [1090/1751], Loss: 1.2557\n",
      "Epoch [33/100], Step [1100/1751], Loss: 1.3345\n",
      "Epoch [33/100], Step [1110/1751], Loss: 1.2884\n",
      "Epoch [33/100], Step [1120/1751], Loss: 1.1064\n",
      "Epoch [33/100], Step [1130/1751], Loss: 1.2933\n",
      "Epoch [33/100], Step [1140/1751], Loss: 1.1313\n",
      "Epoch [33/100], Step [1150/1751], Loss: 1.4784\n",
      "Epoch [33/100], Step [1160/1751], Loss: 1.0882\n",
      "Epoch [33/100], Step [1170/1751], Loss: 1.1059\n",
      "Epoch [33/100], Step [1180/1751], Loss: 1.1826\n",
      "Epoch [33/100], Step [1190/1751], Loss: 1.2798\n",
      "Epoch [33/100], Step [1200/1751], Loss: 1.1518\n",
      "Epoch [33/100], Step [1210/1751], Loss: 1.3065\n",
      "Epoch [33/100], Step [1220/1751], Loss: 1.2535\n",
      "Epoch [33/100], Step [1230/1751], Loss: 1.2225\n",
      "Epoch [33/100], Step [1240/1751], Loss: 1.1971\n",
      "Epoch [33/100], Step [1250/1751], Loss: 1.2841\n",
      "Epoch [33/100], Step [1260/1751], Loss: 1.1922\n",
      "Epoch [33/100], Step [1270/1751], Loss: 1.3017\n",
      "Epoch [33/100], Step [1280/1751], Loss: 1.2332\n",
      "Epoch [33/100], Step [1290/1751], Loss: 1.1236\n",
      "Epoch [33/100], Step [1300/1751], Loss: 1.1154\n",
      "Epoch [33/100], Step [1310/1751], Loss: 1.2139\n",
      "Epoch [33/100], Step [1320/1751], Loss: 1.2449\n",
      "Epoch [33/100], Step [1330/1751], Loss: 1.2026\n",
      "Epoch [33/100], Step [1340/1751], Loss: 1.1106\n",
      "Epoch [33/100], Step [1350/1751], Loss: 1.0447\n",
      "Epoch [33/100], Step [1360/1751], Loss: 1.3638\n",
      "Epoch [33/100], Step [1370/1751], Loss: 1.1632\n",
      "Epoch [33/100], Step [1380/1751], Loss: 1.2304\n",
      "Epoch [33/100], Step [1390/1751], Loss: 1.3436\n",
      "Epoch [33/100], Step [1400/1751], Loss: 1.2681\n",
      "Epoch [33/100], Step [1410/1751], Loss: 1.2740\n",
      "Epoch [33/100], Step [1420/1751], Loss: 1.3274\n",
      "Epoch [33/100], Step [1430/1751], Loss: 1.3647\n",
      "Epoch [33/100], Step [1440/1751], Loss: 1.2292\n",
      "Epoch [33/100], Step [1450/1751], Loss: 1.3486\n",
      "Epoch [33/100], Step [1460/1751], Loss: 1.1506\n",
      "Epoch [33/100], Step [1470/1751], Loss: 1.3026\n",
      "Epoch [33/100], Step [1480/1751], Loss: 1.1056\n",
      "Epoch [33/100], Step [1490/1751], Loss: 1.0982\n",
      "Epoch [33/100], Step [1500/1751], Loss: 1.1540\n",
      "Epoch [33/100], Step [1510/1751], Loss: 1.3147\n",
      "Epoch [33/100], Step [1520/1751], Loss: 1.0948\n",
      "Epoch [33/100], Step [1530/1751], Loss: 1.3349\n",
      "Epoch [33/100], Step [1540/1751], Loss: 1.3977\n",
      "Epoch [33/100], Step [1550/1751], Loss: 1.3274\n",
      "Epoch [33/100], Step [1560/1751], Loss: 1.2298\n",
      "Epoch [33/100], Step [1570/1751], Loss: 1.3267\n",
      "Epoch [33/100], Step [1580/1751], Loss: 1.2213\n",
      "Epoch [33/100], Step [1590/1751], Loss: 1.2634\n",
      "Epoch [33/100], Step [1600/1751], Loss: 1.1808\n",
      "Epoch [33/100], Step [1610/1751], Loss: 1.2584\n",
      "Epoch [33/100], Step [1620/1751], Loss: 1.2556\n",
      "Epoch [33/100], Step [1630/1751], Loss: 1.1986\n",
      "Epoch [33/100], Step [1640/1751], Loss: 1.3310\n",
      "Epoch [33/100], Step [1650/1751], Loss: 1.2443\n",
      "Epoch [33/100], Step [1660/1751], Loss: 1.2321\n",
      "Epoch [33/100], Step [1670/1751], Loss: 1.2580\n",
      "Epoch [33/100], Step [1680/1751], Loss: 1.0210\n",
      "Epoch [33/100], Step [1690/1751], Loss: 1.1438\n",
      "Epoch [33/100], Step [1700/1751], Loss: 1.2287\n",
      "Epoch [33/100], Step [1710/1751], Loss: 1.2717\n",
      "Epoch [33/100], Step [1720/1751], Loss: 1.1705\n",
      "Epoch [33/100], Step [1730/1751], Loss: 1.3201\n",
      "Epoch [33/100], Step [1740/1751], Loss: 1.1701\n",
      "Epoch [33/100], Step [1750/1751], Loss: 1.3515\n",
      "Epoch [33/100], Average Loss: 1.2348, Time: 1649.4037s\n",
      "Epoch [34/100], Step [10/1751], Loss: 1.2562\n",
      "Epoch [34/100], Step [20/1751], Loss: 1.1886\n",
      "Epoch [34/100], Step [30/1751], Loss: 1.0559\n",
      "Epoch [34/100], Step [40/1751], Loss: 1.3286\n",
      "Epoch [34/100], Step [50/1751], Loss: 1.3229\n",
      "Epoch [34/100], Step [60/1751], Loss: 1.2927\n",
      "Epoch [34/100], Step [70/1751], Loss: 1.2509\n",
      "Epoch [34/100], Step [80/1751], Loss: 1.0795\n",
      "Epoch [34/100], Step [90/1751], Loss: 1.2930\n",
      "Epoch [34/100], Step [100/1751], Loss: 1.2776\n",
      "Epoch [34/100], Step [110/1751], Loss: 1.1098\n",
      "Epoch [34/100], Step [120/1751], Loss: 1.3268\n",
      "Epoch [34/100], Step [130/1751], Loss: 1.1332\n",
      "Epoch [34/100], Step [140/1751], Loss: 1.2839\n",
      "Epoch [34/100], Step [150/1751], Loss: 1.0319\n",
      "Epoch [34/100], Step [160/1751], Loss: 1.1191\n",
      "Epoch [34/100], Step [170/1751], Loss: 1.1213\n",
      "Epoch [34/100], Step [180/1751], Loss: 1.2218\n",
      "Epoch [34/100], Step [190/1751], Loss: 1.1269\n",
      "Epoch [34/100], Step [200/1751], Loss: 1.1563\n",
      "Epoch [34/100], Step [210/1751], Loss: 1.1915\n",
      "Epoch [34/100], Step [220/1751], Loss: 1.1601\n",
      "Epoch [34/100], Step [230/1751], Loss: 1.1785\n",
      "Epoch [34/100], Step [240/1751], Loss: 1.2684\n",
      "Epoch [34/100], Step [250/1751], Loss: 1.1786\n",
      "Epoch [34/100], Step [260/1751], Loss: 1.1517\n",
      "Epoch [34/100], Step [270/1751], Loss: 1.1880\n",
      "Epoch [34/100], Step [280/1751], Loss: 1.3556\n",
      "Epoch [34/100], Step [290/1751], Loss: 1.2779\n",
      "Epoch [34/100], Step [300/1751], Loss: 1.2319\n",
      "Epoch [34/100], Step [310/1751], Loss: 1.1373\n",
      "Epoch [34/100], Step [320/1751], Loss: 1.1792\n",
      "Epoch [34/100], Step [330/1751], Loss: 1.0652\n",
      "Epoch [34/100], Step [340/1751], Loss: 1.3271\n",
      "Epoch [34/100], Step [350/1751], Loss: 1.2687\n",
      "Epoch [34/100], Step [360/1751], Loss: 1.2470\n",
      "Epoch [34/100], Step [370/1751], Loss: 1.2369\n",
      "Epoch [34/100], Step [380/1751], Loss: 1.1566\n",
      "Epoch [34/100], Step [390/1751], Loss: 1.1740\n",
      "Epoch [34/100], Step [400/1751], Loss: 1.1139\n",
      "Epoch [34/100], Step [410/1751], Loss: 1.2087\n",
      "Epoch [34/100], Step [420/1751], Loss: 1.1892\n",
      "Epoch [34/100], Step [430/1751], Loss: 0.9947\n",
      "Epoch [34/100], Step [440/1751], Loss: 1.1744\n",
      "Epoch [34/100], Step [450/1751], Loss: 1.3112\n",
      "Epoch [34/100], Step [460/1751], Loss: 1.0729\n",
      "Epoch [34/100], Step [470/1751], Loss: 1.2581\n",
      "Epoch [34/100], Step [480/1751], Loss: 1.2166\n",
      "Epoch [34/100], Step [490/1751], Loss: 1.2945\n",
      "Epoch [34/100], Step [500/1751], Loss: 1.2238\n",
      "Epoch [34/100], Step [510/1751], Loss: 1.2073\n",
      "Epoch [34/100], Step [520/1751], Loss: 1.2845\n",
      "Epoch [34/100], Step [530/1751], Loss: 1.2445\n",
      "Epoch [34/100], Step [540/1751], Loss: 1.2907\n",
      "Epoch [34/100], Step [550/1751], Loss: 1.2688\n",
      "Epoch [34/100], Step [560/1751], Loss: 1.2607\n",
      "Epoch [34/100], Step [570/1751], Loss: 1.2874\n",
      "Epoch [34/100], Step [580/1751], Loss: 1.1843\n",
      "Epoch [34/100], Step [590/1751], Loss: 1.2934\n",
      "Epoch [34/100], Step [600/1751], Loss: 1.2546\n",
      "Epoch [34/100], Step [610/1751], Loss: 1.1776\n",
      "Epoch [34/100], Step [620/1751], Loss: 1.2517\n",
      "Epoch [34/100], Step [630/1751], Loss: 1.2063\n",
      "Epoch [34/100], Step [640/1751], Loss: 1.3252\n",
      "Epoch [34/100], Step [650/1751], Loss: 1.0716\n",
      "Epoch [34/100], Step [660/1751], Loss: 1.2181\n",
      "Epoch [34/100], Step [670/1751], Loss: 1.2771\n",
      "Epoch [34/100], Step [680/1751], Loss: 1.2963\n",
      "Epoch [34/100], Step [690/1751], Loss: 1.3290\n",
      "Epoch [34/100], Step [700/1751], Loss: 1.1671\n",
      "Epoch [34/100], Step [710/1751], Loss: 1.1506\n",
      "Epoch [34/100], Step [720/1751], Loss: 1.2972\n",
      "Epoch [34/100], Step [730/1751], Loss: 1.0856\n",
      "Epoch [34/100], Step [740/1751], Loss: 1.2794\n",
      "Epoch [34/100], Step [750/1751], Loss: 1.2007\n",
      "Epoch [34/100], Step [760/1751], Loss: 1.1744\n",
      "Epoch [34/100], Step [770/1751], Loss: 1.2116\n",
      "Epoch [34/100], Step [780/1751], Loss: 1.1059\n",
      "Epoch [34/100], Step [790/1751], Loss: 1.2208\n",
      "Epoch [34/100], Step [800/1751], Loss: 1.0470\n",
      "Epoch [34/100], Step [810/1751], Loss: 1.1997\n",
      "Epoch [34/100], Step [820/1751], Loss: 1.2513\n",
      "Epoch [34/100], Step [830/1751], Loss: 1.2034\n",
      "Epoch [34/100], Step [840/1751], Loss: 1.3671\n",
      "Epoch [34/100], Step [850/1751], Loss: 1.1097\n",
      "Epoch [34/100], Step [860/1751], Loss: 1.2323\n",
      "Epoch [34/100], Step [870/1751], Loss: 1.2391\n",
      "Epoch [34/100], Step [880/1751], Loss: 1.2190\n",
      "Epoch [34/100], Step [890/1751], Loss: 1.1881\n",
      "Epoch [34/100], Step [900/1751], Loss: 1.1465\n",
      "Epoch [34/100], Step [910/1751], Loss: 1.3155\n",
      "Epoch [34/100], Step [920/1751], Loss: 1.2632\n",
      "Epoch [34/100], Step [930/1751], Loss: 1.2351\n",
      "Epoch [34/100], Step [940/1751], Loss: 1.2389\n",
      "Epoch [34/100], Step [950/1751], Loss: 1.2485\n",
      "Epoch [34/100], Step [960/1751], Loss: 1.2479\n",
      "Epoch [34/100], Step [970/1751], Loss: 1.1773\n",
      "Epoch [34/100], Step [980/1751], Loss: 1.3757\n",
      "Epoch [34/100], Step [990/1751], Loss: 1.1934\n",
      "Epoch [34/100], Step [1000/1751], Loss: 1.2649\n",
      "Epoch [34/100], Step [1010/1751], Loss: 1.3282\n",
      "Epoch [34/100], Step [1020/1751], Loss: 1.3405\n",
      "Epoch [34/100], Step [1030/1751], Loss: 1.2775\n",
      "Epoch [34/100], Step [1040/1751], Loss: 1.3372\n",
      "Epoch [34/100], Step [1050/1751], Loss: 1.0992\n",
      "Epoch [34/100], Step [1060/1751], Loss: 1.1420\n",
      "Epoch [34/100], Step [1070/1751], Loss: 1.2801\n",
      "Epoch [34/100], Step [1080/1751], Loss: 1.2428\n",
      "Epoch [34/100], Step [1090/1751], Loss: 1.0475\n",
      "Epoch [34/100], Step [1100/1751], Loss: 1.3249\n",
      "Epoch [34/100], Step [1110/1751], Loss: 1.2752\n",
      "Epoch [34/100], Step [1120/1751], Loss: 1.2552\n",
      "Epoch [34/100], Step [1130/1751], Loss: 1.3083\n",
      "Epoch [34/100], Step [1140/1751], Loss: 1.3579\n",
      "Epoch [34/100], Step [1150/1751], Loss: 1.2097\n",
      "Epoch [34/100], Step [1160/1751], Loss: 1.2400\n",
      "Epoch [34/100], Step [1170/1751], Loss: 1.4772\n",
      "Epoch [34/100], Step [1180/1751], Loss: 1.1975\n",
      "Epoch [34/100], Step [1190/1751], Loss: 1.1931\n",
      "Epoch [34/100], Step [1200/1751], Loss: 1.3120\n",
      "Epoch [34/100], Step [1210/1751], Loss: 1.1758\n",
      "Epoch [34/100], Step [1220/1751], Loss: 1.1541\n",
      "Epoch [34/100], Step [1230/1751], Loss: 1.2476\n",
      "Epoch [34/100], Step [1240/1751], Loss: 1.2139\n",
      "Epoch [34/100], Step [1250/1751], Loss: 1.0640\n",
      "Epoch [34/100], Step [1260/1751], Loss: 1.2728\n",
      "Epoch [34/100], Step [1270/1751], Loss: 1.2996\n",
      "Epoch [34/100], Step [1280/1751], Loss: 1.2070\n",
      "Epoch [34/100], Step [1290/1751], Loss: 1.1007\n",
      "Epoch [34/100], Step [1300/1751], Loss: 1.1101\n",
      "Epoch [34/100], Step [1310/1751], Loss: 1.2040\n",
      "Epoch [34/100], Step [1320/1751], Loss: 1.3558\n",
      "Epoch [34/100], Step [1330/1751], Loss: 1.3245\n",
      "Epoch [34/100], Step [1340/1751], Loss: 1.3008\n",
      "Epoch [34/100], Step [1350/1751], Loss: 1.2958\n",
      "Epoch [34/100], Step [1360/1751], Loss: 1.3793\n",
      "Epoch [34/100], Step [1370/1751], Loss: 1.0932\n",
      "Epoch [34/100], Step [1380/1751], Loss: 1.2287\n",
      "Epoch [34/100], Step [1390/1751], Loss: 1.3270\n",
      "Epoch [34/100], Step [1400/1751], Loss: 1.3310\n",
      "Epoch [34/100], Step [1410/1751], Loss: 1.2973\n",
      "Epoch [34/100], Step [1420/1751], Loss: 1.2787\n",
      "Epoch [34/100], Step [1430/1751], Loss: 1.2004\n",
      "Epoch [34/100], Step [1440/1751], Loss: 1.1890\n",
      "Epoch [34/100], Step [1450/1751], Loss: 1.3178\n",
      "Epoch [34/100], Step [1460/1751], Loss: 1.2790\n",
      "Epoch [34/100], Step [1470/1751], Loss: 1.4052\n",
      "Epoch [34/100], Step [1480/1751], Loss: 1.3038\n",
      "Epoch [34/100], Step [1490/1751], Loss: 1.1820\n",
      "Epoch [34/100], Step [1500/1751], Loss: 1.1776\n",
      "Epoch [34/100], Step [1510/1751], Loss: 1.2592\n",
      "Epoch [34/100], Step [1520/1751], Loss: 1.3367\n",
      "Epoch [34/100], Step [1530/1751], Loss: 1.3982\n",
      "Epoch [34/100], Step [1540/1751], Loss: 1.2200\n",
      "Epoch [34/100], Step [1550/1751], Loss: 1.3660\n",
      "Epoch [34/100], Step [1560/1751], Loss: 1.3038\n",
      "Epoch [34/100], Step [1570/1751], Loss: 1.1470\n",
      "Epoch [34/100], Step [1580/1751], Loss: 1.2231\n",
      "Epoch [34/100], Step [1590/1751], Loss: 1.2358\n",
      "Epoch [34/100], Step [1600/1751], Loss: 1.1546\n",
      "Epoch [34/100], Step [1610/1751], Loss: 1.2791\n",
      "Epoch [34/100], Step [1620/1751], Loss: 1.4037\n",
      "Epoch [34/100], Step [1630/1751], Loss: 1.2800\n",
      "Epoch [34/100], Step [1640/1751], Loss: 1.1037\n",
      "Epoch [34/100], Step [1650/1751], Loss: 1.1651\n",
      "Epoch [34/100], Step [1660/1751], Loss: 1.1567\n",
      "Epoch [34/100], Step [1670/1751], Loss: 1.3038\n",
      "Epoch [34/100], Step [1680/1751], Loss: 1.2332\n",
      "Epoch [34/100], Step [1690/1751], Loss: 1.1451\n",
      "Epoch [34/100], Step [1700/1751], Loss: 1.2192\n",
      "Epoch [34/100], Step [1710/1751], Loss: 1.1289\n",
      "Epoch [34/100], Step [1720/1751], Loss: 1.4594\n",
      "Epoch [34/100], Step [1730/1751], Loss: 1.2187\n",
      "Epoch [34/100], Step [1740/1751], Loss: 1.3416\n",
      "Epoch [34/100], Step [1750/1751], Loss: 1.1182\n",
      "Epoch [34/100], Average Loss: 1.2332, Time: 1650.1769s\n",
      "Epoch [35/100], Step [10/1751], Loss: 1.2475\n",
      "Epoch [35/100], Step [20/1751], Loss: 1.2021\n",
      "Epoch [35/100], Step [30/1751], Loss: 1.1917\n",
      "Epoch [35/100], Step [40/1751], Loss: 1.1997\n",
      "Epoch [35/100], Step [50/1751], Loss: 1.3478\n",
      "Epoch [35/100], Step [60/1751], Loss: 1.2482\n",
      "Epoch [35/100], Step [70/1751], Loss: 1.1970\n",
      "Epoch [35/100], Step [80/1751], Loss: 1.2755\n",
      "Epoch [35/100], Step [90/1751], Loss: 1.2557\n",
      "Epoch [35/100], Step [100/1751], Loss: 1.2832\n",
      "Epoch [35/100], Step [110/1751], Loss: 1.2054\n",
      "Epoch [35/100], Step [120/1751], Loss: 1.2771\n",
      "Epoch [35/100], Step [130/1751], Loss: 1.1955\n",
      "Epoch [35/100], Step [140/1751], Loss: 1.3252\n",
      "Epoch [35/100], Step [150/1751], Loss: 1.3307\n",
      "Epoch [35/100], Step [160/1751], Loss: 1.3577\n",
      "Epoch [35/100], Step [170/1751], Loss: 1.4011\n",
      "Epoch [35/100], Step [180/1751], Loss: 1.0293\n",
      "Epoch [35/100], Step [190/1751], Loss: 1.1056\n",
      "Epoch [35/100], Step [200/1751], Loss: 1.2275\n",
      "Epoch [35/100], Step [210/1751], Loss: 1.1970\n",
      "Epoch [35/100], Step [220/1751], Loss: 1.4705\n",
      "Epoch [35/100], Step [230/1751], Loss: 1.3210\n",
      "Epoch [35/100], Step [240/1751], Loss: 1.2559\n",
      "Epoch [35/100], Step [250/1751], Loss: 1.2298\n",
      "Epoch [35/100], Step [260/1751], Loss: 1.2418\n",
      "Epoch [35/100], Step [270/1751], Loss: 1.1979\n",
      "Epoch [35/100], Step [280/1751], Loss: 1.3806\n",
      "Epoch [35/100], Step [290/1751], Loss: 1.3169\n",
      "Epoch [35/100], Step [300/1751], Loss: 1.1895\n",
      "Epoch [35/100], Step [310/1751], Loss: 1.0951\n",
      "Epoch [35/100], Step [320/1751], Loss: 1.2156\n",
      "Epoch [35/100], Step [330/1751], Loss: 1.2218\n",
      "Epoch [35/100], Step [340/1751], Loss: 1.0659\n",
      "Epoch [35/100], Step [350/1751], Loss: 1.2404\n",
      "Epoch [35/100], Step [360/1751], Loss: 1.1561\n",
      "Epoch [35/100], Step [370/1751], Loss: 1.1231\n",
      "Epoch [35/100], Step [380/1751], Loss: 1.2275\n",
      "Epoch [35/100], Step [390/1751], Loss: 1.1691\n",
      "Epoch [35/100], Step [400/1751], Loss: 1.3984\n",
      "Epoch [35/100], Step [410/1751], Loss: 1.1396\n",
      "Epoch [35/100], Step [420/1751], Loss: 1.1497\n",
      "Epoch [35/100], Step [430/1751], Loss: 1.2697\n",
      "Epoch [35/100], Step [440/1751], Loss: 1.1515\n",
      "Epoch [35/100], Step [450/1751], Loss: 1.2942\n",
      "Epoch [35/100], Step [460/1751], Loss: 1.4006\n",
      "Epoch [35/100], Step [470/1751], Loss: 1.0871\n",
      "Epoch [35/100], Step [480/1751], Loss: 1.1118\n",
      "Epoch [35/100], Step [490/1751], Loss: 1.1486\n",
      "Epoch [35/100], Step [500/1751], Loss: 1.3332\n",
      "Epoch [35/100], Step [510/1751], Loss: 1.1490\n",
      "Epoch [35/100], Step [520/1751], Loss: 1.1342\n",
      "Epoch [35/100], Step [530/1751], Loss: 1.1342\n",
      "Epoch [35/100], Step [540/1751], Loss: 1.0909\n",
      "Epoch [35/100], Step [550/1751], Loss: 1.2834\n",
      "Epoch [35/100], Step [560/1751], Loss: 1.2127\n",
      "Epoch [35/100], Step [570/1751], Loss: 1.1237\n",
      "Epoch [35/100], Step [580/1751], Loss: 1.2228\n",
      "Epoch [35/100], Step [590/1751], Loss: 1.3929\n",
      "Epoch [35/100], Step [600/1751], Loss: 1.2823\n",
      "Epoch [35/100], Step [610/1751], Loss: 1.1600\n",
      "Epoch [35/100], Step [620/1751], Loss: 1.1849\n",
      "Epoch [35/100], Step [630/1751], Loss: 1.3432\n",
      "Epoch [35/100], Step [640/1751], Loss: 1.2484\n",
      "Epoch [35/100], Step [650/1751], Loss: 1.2278\n",
      "Epoch [35/100], Step [660/1751], Loss: 1.2578\n",
      "Epoch [35/100], Step [670/1751], Loss: 1.1851\n",
      "Epoch [35/100], Step [680/1751], Loss: 1.2435\n",
      "Epoch [35/100], Step [690/1751], Loss: 1.2508\n",
      "Epoch [35/100], Step [700/1751], Loss: 1.0863\n",
      "Epoch [35/100], Step [710/1751], Loss: 1.1631\n",
      "Epoch [35/100], Step [720/1751], Loss: 1.2874\n",
      "Epoch [35/100], Step [730/1751], Loss: 1.1417\n",
      "Epoch [35/100], Step [740/1751], Loss: 1.2258\n",
      "Epoch [35/100], Step [750/1751], Loss: 1.0692\n",
      "Epoch [35/100], Step [760/1751], Loss: 1.2498\n",
      "Epoch [35/100], Step [770/1751], Loss: 1.2633\n",
      "Epoch [35/100], Step [780/1751], Loss: 1.3542\n",
      "Epoch [35/100], Step [790/1751], Loss: 1.3359\n",
      "Epoch [35/100], Step [800/1751], Loss: 1.2359\n",
      "Epoch [35/100], Step [810/1751], Loss: 1.1237\n",
      "Epoch [35/100], Step [820/1751], Loss: 1.2587\n",
      "Epoch [35/100], Step [830/1751], Loss: 1.3048\n",
      "Epoch [35/100], Step [840/1751], Loss: 1.1842\n",
      "Epoch [35/100], Step [850/1751], Loss: 1.1457\n",
      "Epoch [35/100], Step [860/1751], Loss: 1.3342\n",
      "Epoch [35/100], Step [870/1751], Loss: 1.2294\n",
      "Epoch [35/100], Step [880/1751], Loss: 1.2221\n",
      "Epoch [35/100], Step [890/1751], Loss: 1.2879\n",
      "Epoch [35/100], Step [900/1751], Loss: 1.2993\n",
      "Epoch [35/100], Step [910/1751], Loss: 1.3309\n",
      "Epoch [35/100], Step [920/1751], Loss: 1.1929\n",
      "Epoch [35/100], Step [930/1751], Loss: 1.1528\n",
      "Epoch [35/100], Step [940/1751], Loss: 1.2452\n",
      "Epoch [35/100], Step [950/1751], Loss: 1.2052\n",
      "Epoch [35/100], Step [960/1751], Loss: 1.2259\n",
      "Epoch [35/100], Step [970/1751], Loss: 1.2527\n",
      "Epoch [35/100], Step [980/1751], Loss: 0.9875\n",
      "Epoch [35/100], Step [990/1751], Loss: 1.3478\n",
      "Epoch [35/100], Step [1000/1751], Loss: 1.3199\n",
      "Epoch [35/100], Step [1010/1751], Loss: 1.1379\n",
      "Epoch [35/100], Step [1020/1751], Loss: 1.1131\n",
      "Epoch [35/100], Step [1030/1751], Loss: 1.1389\n",
      "Epoch [35/100], Step [1040/1751], Loss: 1.2830\n",
      "Epoch [35/100], Step [1050/1751], Loss: 1.3002\n",
      "Epoch [35/100], Step [1060/1751], Loss: 1.3661\n",
      "Epoch [35/100], Step [1070/1751], Loss: 1.2599\n",
      "Epoch [35/100], Step [1080/1751], Loss: 1.2613\n",
      "Epoch [35/100], Step [1090/1751], Loss: 1.1360\n",
      "Epoch [35/100], Step [1100/1751], Loss: 1.2821\n",
      "Epoch [35/100], Step [1110/1751], Loss: 1.3917\n",
      "Epoch [35/100], Step [1120/1751], Loss: 1.2089\n",
      "Epoch [35/100], Step [1130/1751], Loss: 1.2361\n",
      "Epoch [35/100], Step [1140/1751], Loss: 1.1760\n",
      "Epoch [35/100], Step [1150/1751], Loss: 1.1908\n",
      "Epoch [35/100], Step [1160/1751], Loss: 1.1689\n",
      "Epoch [35/100], Step [1170/1751], Loss: 1.2659\n",
      "Epoch [35/100], Step [1180/1751], Loss: 1.2388\n",
      "Epoch [35/100], Step [1190/1751], Loss: 1.3290\n",
      "Epoch [35/100], Step [1200/1751], Loss: 1.2194\n",
      "Epoch [35/100], Step [1210/1751], Loss: 1.3020\n",
      "Epoch [35/100], Step [1220/1751], Loss: 1.2508\n",
      "Epoch [35/100], Step [1230/1751], Loss: 1.2000\n",
      "Epoch [35/100], Step [1240/1751], Loss: 1.2481\n",
      "Epoch [35/100], Step [1250/1751], Loss: 1.2991\n",
      "Epoch [35/100], Step [1260/1751], Loss: 1.0049\n",
      "Epoch [35/100], Step [1270/1751], Loss: 1.3268\n",
      "Epoch [35/100], Step [1280/1751], Loss: 1.2227\n",
      "Epoch [35/100], Step [1290/1751], Loss: 1.2477\n",
      "Epoch [35/100], Step [1300/1751], Loss: 1.2683\n",
      "Epoch [35/100], Step [1310/1751], Loss: 1.2642\n",
      "Epoch [35/100], Step [1320/1751], Loss: 1.2773\n",
      "Epoch [35/100], Step [1330/1751], Loss: 1.1859\n",
      "Epoch [35/100], Step [1340/1751], Loss: 1.3538\n",
      "Epoch [35/100], Step [1350/1751], Loss: 1.2206\n",
      "Epoch [35/100], Step [1360/1751], Loss: 1.2272\n",
      "Epoch [35/100], Step [1370/1751], Loss: 1.0939\n",
      "Epoch [35/100], Step [1380/1751], Loss: 1.2142\n",
      "Epoch [35/100], Step [1390/1751], Loss: 1.2955\n",
      "Epoch [35/100], Step [1400/1751], Loss: 1.1099\n",
      "Epoch [35/100], Step [1410/1751], Loss: 1.3031\n",
      "Epoch [35/100], Step [1420/1751], Loss: 1.1840\n",
      "Epoch [35/100], Step [1430/1751], Loss: 1.2936\n",
      "Epoch [35/100], Step [1440/1751], Loss: 1.2039\n",
      "Epoch [35/100], Step [1450/1751], Loss: 1.1771\n",
      "Epoch [35/100], Step [1460/1751], Loss: 1.3014\n",
      "Epoch [35/100], Step [1470/1751], Loss: 1.3928\n",
      "Epoch [35/100], Step [1480/1751], Loss: 1.1274\n",
      "Epoch [35/100], Step [1490/1751], Loss: 1.3474\n",
      "Epoch [35/100], Step [1500/1751], Loss: 1.1734\n",
      "Epoch [35/100], Step [1510/1751], Loss: 1.2702\n",
      "Epoch [35/100], Step [1520/1751], Loss: 1.1491\n",
      "Epoch [35/100], Step [1530/1751], Loss: 1.1445\n",
      "Epoch [35/100], Step [1540/1751], Loss: 1.0959\n",
      "Epoch [35/100], Step [1550/1751], Loss: 1.2078\n",
      "Epoch [35/100], Step [1560/1751], Loss: 1.2275\n",
      "Epoch [35/100], Step [1570/1751], Loss: 1.1991\n",
      "Epoch [35/100], Step [1580/1751], Loss: 1.1307\n",
      "Epoch [35/100], Step [1590/1751], Loss: 1.2250\n",
      "Epoch [35/100], Step [1600/1751], Loss: 1.1829\n",
      "Epoch [35/100], Step [1610/1751], Loss: 1.3223\n",
      "Epoch [35/100], Step [1620/1751], Loss: 1.3512\n",
      "Epoch [35/100], Step [1630/1751], Loss: 1.3146\n",
      "Epoch [35/100], Step [1640/1751], Loss: 1.1442\n",
      "Epoch [35/100], Step [1650/1751], Loss: 1.2046\n",
      "Epoch [35/100], Step [1660/1751], Loss: 1.2994\n",
      "Epoch [35/100], Step [1670/1751], Loss: 1.4033\n",
      "Epoch [35/100], Step [1680/1751], Loss: 1.1861\n",
      "Epoch [35/100], Step [1690/1751], Loss: 1.1601\n",
      "Epoch [35/100], Step [1700/1751], Loss: 1.1666\n",
      "Epoch [35/100], Step [1710/1751], Loss: 1.2396\n",
      "Epoch [35/100], Step [1720/1751], Loss: 1.3135\n",
      "Epoch [35/100], Step [1730/1751], Loss: 1.2727\n",
      "Epoch [35/100], Step [1740/1751], Loss: 1.3347\n",
      "Epoch [35/100], Step [1750/1751], Loss: 1.1795\n",
      "Epoch [35/100], Average Loss: 1.2303, Time: 1649.5121s\n",
      "Epoch [36/100], Step [10/1751], Loss: 1.1349\n",
      "Epoch [36/100], Step [20/1751], Loss: 1.1659\n",
      "Epoch [36/100], Step [30/1751], Loss: 1.1624\n",
      "Epoch [36/100], Step [40/1751], Loss: 1.2420\n",
      "Epoch [36/100], Step [50/1751], Loss: 1.0891\n",
      "Epoch [36/100], Step [60/1751], Loss: 1.2038\n",
      "Epoch [36/100], Step [70/1751], Loss: 1.2273\n",
      "Epoch [36/100], Step [80/1751], Loss: 1.3956\n",
      "Epoch [36/100], Step [90/1751], Loss: 1.0952\n",
      "Epoch [36/100], Step [100/1751], Loss: 1.0066\n",
      "Epoch [36/100], Step [110/1751], Loss: 1.2211\n",
      "Epoch [36/100], Step [120/1751], Loss: 1.1113\n",
      "Epoch [36/100], Step [130/1751], Loss: 1.2311\n",
      "Epoch [36/100], Step [140/1751], Loss: 1.1952\n",
      "Epoch [36/100], Step [150/1751], Loss: 1.3213\n",
      "Epoch [36/100], Step [160/1751], Loss: 1.3723\n",
      "Epoch [36/100], Step [170/1751], Loss: 1.3039\n",
      "Epoch [36/100], Step [180/1751], Loss: 1.2189\n",
      "Epoch [36/100], Step [190/1751], Loss: 1.1565\n",
      "Epoch [36/100], Step [200/1751], Loss: 1.2109\n",
      "Epoch [36/100], Step [210/1751], Loss: 1.1498\n",
      "Epoch [36/100], Step [220/1751], Loss: 1.1894\n",
      "Epoch [36/100], Step [230/1751], Loss: 1.2983\n",
      "Epoch [36/100], Step [240/1751], Loss: 1.3085\n",
      "Epoch [36/100], Step [250/1751], Loss: 1.3256\n",
      "Epoch [36/100], Step [260/1751], Loss: 1.2201\n",
      "Epoch [36/100], Step [270/1751], Loss: 1.0988\n",
      "Epoch [36/100], Step [280/1751], Loss: 1.2192\n",
      "Epoch [36/100], Step [290/1751], Loss: 1.2058\n",
      "Epoch [36/100], Step [300/1751], Loss: 1.2333\n",
      "Epoch [36/100], Step [310/1751], Loss: 1.1685\n",
      "Epoch [36/100], Step [320/1751], Loss: 1.3010\n",
      "Epoch [36/100], Step [330/1751], Loss: 1.3538\n",
      "Epoch [36/100], Step [340/1751], Loss: 1.4586\n",
      "Epoch [36/100], Step [350/1751], Loss: 1.2466\n",
      "Epoch [36/100], Step [360/1751], Loss: 1.2261\n",
      "Epoch [36/100], Step [370/1751], Loss: 1.2910\n",
      "Epoch [36/100], Step [380/1751], Loss: 1.2260\n",
      "Epoch [36/100], Step [390/1751], Loss: 1.1978\n",
      "Epoch [36/100], Step [400/1751], Loss: 1.2892\n",
      "Epoch [36/100], Step [410/1751], Loss: 1.2325\n",
      "Epoch [36/100], Step [420/1751], Loss: 1.2677\n",
      "Epoch [36/100], Step [430/1751], Loss: 1.3081\n",
      "Epoch [36/100], Step [440/1751], Loss: 1.1417\n",
      "Epoch [36/100], Step [450/1751], Loss: 1.2456\n",
      "Epoch [36/100], Step [460/1751], Loss: 1.4010\n",
      "Epoch [36/100], Step [470/1751], Loss: 1.1957\n",
      "Epoch [36/100], Step [480/1751], Loss: 1.1333\n",
      "Epoch [36/100], Step [490/1751], Loss: 1.1576\n",
      "Epoch [36/100], Step [500/1751], Loss: 1.0808\n",
      "Epoch [36/100], Step [510/1751], Loss: 1.3387\n",
      "Epoch [36/100], Step [520/1751], Loss: 1.1588\n",
      "Epoch [36/100], Step [530/1751], Loss: 1.3692\n",
      "Epoch [36/100], Step [540/1751], Loss: 1.2830\n",
      "Epoch [36/100], Step [550/1751], Loss: 1.2013\n",
      "Epoch [36/100], Step [560/1751], Loss: 1.3247\n",
      "Epoch [36/100], Step [570/1751], Loss: 1.2311\n",
      "Epoch [36/100], Step [580/1751], Loss: 1.4135\n",
      "Epoch [36/100], Step [590/1751], Loss: 1.1349\n",
      "Epoch [36/100], Step [600/1751], Loss: 1.1430\n",
      "Epoch [36/100], Step [610/1751], Loss: 1.2135\n",
      "Epoch [36/100], Step [620/1751], Loss: 1.1917\n",
      "Epoch [36/100], Step [630/1751], Loss: 1.3378\n",
      "Epoch [36/100], Step [640/1751], Loss: 1.2034\n",
      "Epoch [36/100], Step [650/1751], Loss: 1.2070\n",
      "Epoch [36/100], Step [660/1751], Loss: 1.2231\n",
      "Epoch [36/100], Step [670/1751], Loss: 1.4216\n",
      "Epoch [36/100], Step [680/1751], Loss: 1.2747\n",
      "Epoch [36/100], Step [690/1751], Loss: 1.1471\n",
      "Epoch [36/100], Step [700/1751], Loss: 1.2673\n",
      "Epoch [36/100], Step [710/1751], Loss: 1.3096\n",
      "Epoch [36/100], Step [720/1751], Loss: 1.3788\n",
      "Epoch [36/100], Step [730/1751], Loss: 1.1292\n",
      "Epoch [36/100], Step [740/1751], Loss: 1.2676\n",
      "Epoch [36/100], Step [750/1751], Loss: 1.1164\n",
      "Epoch [36/100], Step [760/1751], Loss: 1.1915\n",
      "Epoch [36/100], Step [770/1751], Loss: 1.2015\n",
      "Epoch [36/100], Step [780/1751], Loss: 1.3088\n",
      "Epoch [36/100], Step [790/1751], Loss: 1.0557\n",
      "Epoch [36/100], Step [800/1751], Loss: 1.2602\n",
      "Epoch [36/100], Step [810/1751], Loss: 1.1957\n",
      "Epoch [36/100], Step [820/1751], Loss: 1.2715\n",
      "Epoch [36/100], Step [830/1751], Loss: 1.2668\n",
      "Epoch [36/100], Step [840/1751], Loss: 1.2917\n",
      "Epoch [36/100], Step [850/1751], Loss: 1.2821\n",
      "Epoch [36/100], Step [860/1751], Loss: 1.3274\n",
      "Epoch [36/100], Step [870/1751], Loss: 1.1511\n",
      "Epoch [36/100], Step [880/1751], Loss: 1.3181\n",
      "Epoch [36/100], Step [890/1751], Loss: 1.1908\n",
      "Epoch [36/100], Step [900/1751], Loss: 1.2129\n",
      "Epoch [36/100], Step [910/1751], Loss: 1.2520\n",
      "Epoch [36/100], Step [920/1751], Loss: 1.2754\n",
      "Epoch [36/100], Step [930/1751], Loss: 1.2669\n",
      "Epoch [36/100], Step [940/1751], Loss: 1.1875\n",
      "Epoch [36/100], Step [950/1751], Loss: 1.1937\n",
      "Epoch [36/100], Step [960/1751], Loss: 1.2075\n",
      "Epoch [36/100], Step [970/1751], Loss: 1.2809\n",
      "Epoch [36/100], Step [980/1751], Loss: 1.4846\n",
      "Epoch [36/100], Step [990/1751], Loss: 1.1757\n",
      "Epoch [36/100], Step [1000/1751], Loss: 1.1079\n",
      "Epoch [36/100], Step [1010/1751], Loss: 1.1979\n",
      "Epoch [36/100], Step [1020/1751], Loss: 1.2452\n",
      "Epoch [36/100], Step [1030/1751], Loss: 1.1341\n",
      "Epoch [36/100], Step [1040/1751], Loss: 1.3279\n",
      "Epoch [36/100], Step [1050/1751], Loss: 1.0626\n",
      "Epoch [36/100], Step [1060/1751], Loss: 1.1936\n",
      "Epoch [36/100], Step [1070/1751], Loss: 1.1305\n",
      "Epoch [36/100], Step [1080/1751], Loss: 1.0762\n",
      "Epoch [36/100], Step [1090/1751], Loss: 1.1927\n",
      "Epoch [36/100], Step [1100/1751], Loss: 1.3058\n",
      "Epoch [36/100], Step [1110/1751], Loss: 1.1586\n",
      "Epoch [36/100], Step [1120/1751], Loss: 1.1631\n",
      "Epoch [36/100], Step [1130/1751], Loss: 1.1548\n",
      "Epoch [36/100], Step [1140/1751], Loss: 1.1502\n",
      "Epoch [36/100], Step [1150/1751], Loss: 1.2017\n",
      "Epoch [36/100], Step [1160/1751], Loss: 1.1513\n",
      "Epoch [36/100], Step [1170/1751], Loss: 1.3040\n",
      "Epoch [36/100], Step [1180/1751], Loss: 1.1244\n",
      "Epoch [36/100], Step [1190/1751], Loss: 1.1702\n",
      "Epoch [36/100], Step [1200/1751], Loss: 1.1737\n",
      "Epoch [36/100], Step [1210/1751], Loss: 1.3169\n",
      "Epoch [36/100], Step [1220/1751], Loss: 1.2904\n",
      "Epoch [36/100], Step [1230/1751], Loss: 1.1318\n",
      "Epoch [36/100], Step [1240/1751], Loss: 1.0666\n",
      "Epoch [36/100], Step [1250/1751], Loss: 1.1526\n",
      "Epoch [36/100], Step [1260/1751], Loss: 1.3016\n",
      "Epoch [36/100], Step [1270/1751], Loss: 1.1355\n",
      "Epoch [36/100], Step [1280/1751], Loss: 1.4235\n",
      "Epoch [36/100], Step [1290/1751], Loss: 1.3010\n",
      "Epoch [36/100], Step [1300/1751], Loss: 1.1990\n",
      "Epoch [36/100], Step [1310/1751], Loss: 1.2870\n",
      "Epoch [36/100], Step [1320/1751], Loss: 1.3282\n",
      "Epoch [36/100], Step [1330/1751], Loss: 1.1454\n",
      "Epoch [36/100], Step [1340/1751], Loss: 1.3088\n",
      "Epoch [36/100], Step [1350/1751], Loss: 1.1859\n",
      "Epoch [36/100], Step [1360/1751], Loss: 1.2021\n",
      "Epoch [36/100], Step [1370/1751], Loss: 1.4510\n",
      "Epoch [36/100], Step [1380/1751], Loss: 1.2457\n",
      "Epoch [36/100], Step [1390/1751], Loss: 1.2602\n",
      "Epoch [36/100], Step [1400/1751], Loss: 1.2938\n",
      "Epoch [36/100], Step [1410/1751], Loss: 1.2343\n",
      "Epoch [36/100], Step [1420/1751], Loss: 1.3230\n",
      "Epoch [36/100], Step [1430/1751], Loss: 1.2242\n",
      "Epoch [36/100], Step [1440/1751], Loss: 1.3133\n",
      "Epoch [36/100], Step [1450/1751], Loss: 1.3516\n",
      "Epoch [36/100], Step [1460/1751], Loss: 1.1310\n",
      "Epoch [36/100], Step [1470/1751], Loss: 1.1878\n",
      "Epoch [36/100], Step [1480/1751], Loss: 1.2128\n",
      "Epoch [36/100], Step [1490/1751], Loss: 1.1829\n",
      "Epoch [36/100], Step [1500/1751], Loss: 1.2038\n",
      "Epoch [36/100], Step [1510/1751], Loss: 1.1451\n",
      "Epoch [36/100], Step [1520/1751], Loss: 1.0850\n",
      "Epoch [36/100], Step [1530/1751], Loss: 1.1621\n",
      "Epoch [36/100], Step [1540/1751], Loss: 1.1875\n",
      "Epoch [36/100], Step [1550/1751], Loss: 1.1330\n",
      "Epoch [36/100], Step [1560/1751], Loss: 1.2089\n",
      "Epoch [36/100], Step [1570/1751], Loss: 1.2686\n",
      "Epoch [36/100], Step [1580/1751], Loss: 1.4150\n",
      "Epoch [36/100], Step [1590/1751], Loss: 1.2175\n",
      "Epoch [36/100], Step [1600/1751], Loss: 1.1720\n",
      "Epoch [36/100], Step [1610/1751], Loss: 1.3727\n",
      "Epoch [36/100], Step [1620/1751], Loss: 1.0883\n",
      "Epoch [36/100], Step [1630/1751], Loss: 1.1547\n",
      "Epoch [36/100], Step [1640/1751], Loss: 1.2420\n",
      "Epoch [36/100], Step [1650/1751], Loss: 1.2024\n",
      "Epoch [36/100], Step [1660/1751], Loss: 1.1771\n",
      "Epoch [36/100], Step [1670/1751], Loss: 1.3180\n",
      "Epoch [36/100], Step [1680/1751], Loss: 1.2535\n",
      "Epoch [36/100], Step [1690/1751], Loss: 1.2397\n",
      "Epoch [36/100], Step [1700/1751], Loss: 1.2238\n",
      "Epoch [36/100], Step [1710/1751], Loss: 1.3212\n",
      "Epoch [36/100], Step [1720/1751], Loss: 1.2580\n",
      "Epoch [36/100], Step [1730/1751], Loss: 1.2297\n",
      "Epoch [36/100], Step [1740/1751], Loss: 1.1162\n",
      "Epoch [36/100], Step [1750/1751], Loss: 1.3121\n",
      "Epoch [36/100], Average Loss: 1.2293, Time: 1649.2956s\n",
      "Epoch [37/100], Step [10/1751], Loss: 1.2104\n",
      "Epoch [37/100], Step [20/1751], Loss: 1.1954\n",
      "Epoch [37/100], Step [30/1751], Loss: 1.3807\n",
      "Epoch [37/100], Step [40/1751], Loss: 1.1590\n",
      "Epoch [37/100], Step [50/1751], Loss: 1.2965\n",
      "Epoch [37/100], Step [60/1751], Loss: 1.2058\n",
      "Epoch [37/100], Step [70/1751], Loss: 1.3197\n",
      "Epoch [37/100], Step [80/1751], Loss: 1.2135\n",
      "Epoch [37/100], Step [90/1751], Loss: 1.2018\n",
      "Epoch [37/100], Step [100/1751], Loss: 1.2782\n",
      "Epoch [37/100], Step [110/1751], Loss: 1.2081\n",
      "Epoch [37/100], Step [120/1751], Loss: 1.1982\n",
      "Epoch [37/100], Step [130/1751], Loss: 1.4172\n",
      "Epoch [37/100], Step [140/1751], Loss: 1.0745\n",
      "Epoch [37/100], Step [150/1751], Loss: 1.2342\n",
      "Epoch [37/100], Step [160/1751], Loss: 1.1388\n",
      "Epoch [37/100], Step [170/1751], Loss: 1.0990\n",
      "Epoch [37/100], Step [180/1751], Loss: 1.0879\n",
      "Epoch [37/100], Step [190/1751], Loss: 1.1916\n",
      "Epoch [37/100], Step [200/1751], Loss: 1.2586\n",
      "Epoch [37/100], Step [210/1751], Loss: 1.0957\n",
      "Epoch [37/100], Step [220/1751], Loss: 1.2480\n",
      "Epoch [37/100], Step [230/1751], Loss: 1.2117\n",
      "Epoch [37/100], Step [240/1751], Loss: 1.2002\n",
      "Epoch [37/100], Step [250/1751], Loss: 1.2925\n",
      "Epoch [37/100], Step [260/1751], Loss: 1.3006\n",
      "Epoch [37/100], Step [270/1751], Loss: 1.2826\n",
      "Epoch [37/100], Step [280/1751], Loss: 1.2689\n",
      "Epoch [37/100], Step [290/1751], Loss: 1.2263\n",
      "Epoch [37/100], Step [300/1751], Loss: 1.2622\n",
      "Epoch [37/100], Step [310/1751], Loss: 1.1681\n",
      "Epoch [37/100], Step [320/1751], Loss: 1.1153\n",
      "Epoch [37/100], Step [330/1751], Loss: 1.2688\n",
      "Epoch [37/100], Step [340/1751], Loss: 1.2688\n",
      "Epoch [37/100], Step [350/1751], Loss: 1.0489\n",
      "Epoch [37/100], Step [360/1751], Loss: 1.2298\n",
      "Epoch [37/100], Step [370/1751], Loss: 1.2289\n",
      "Epoch [37/100], Step [380/1751], Loss: 1.2189\n",
      "Epoch [37/100], Step [390/1751], Loss: 1.2212\n",
      "Epoch [37/100], Step [400/1751], Loss: 1.2568\n",
      "Epoch [37/100], Step [410/1751], Loss: 1.2756\n",
      "Epoch [37/100], Step [420/1751], Loss: 1.0700\n",
      "Epoch [37/100], Step [430/1751], Loss: 1.3416\n",
      "Epoch [37/100], Step [440/1751], Loss: 1.4239\n",
      "Epoch [37/100], Step [450/1751], Loss: 1.3303\n",
      "Epoch [37/100], Step [460/1751], Loss: 1.1446\n",
      "Epoch [37/100], Step [470/1751], Loss: 1.2086\n",
      "Epoch [37/100], Step [480/1751], Loss: 1.2734\n",
      "Epoch [37/100], Step [490/1751], Loss: 1.2650\n",
      "Epoch [37/100], Step [500/1751], Loss: 1.2756\n",
      "Epoch [37/100], Step [510/1751], Loss: 1.1449\n",
      "Epoch [37/100], Step [520/1751], Loss: 1.2529\n",
      "Epoch [37/100], Step [530/1751], Loss: 1.2778\n",
      "Epoch [37/100], Step [540/1751], Loss: 1.0610\n",
      "Epoch [37/100], Step [550/1751], Loss: 1.1658\n",
      "Epoch [37/100], Step [560/1751], Loss: 1.1848\n",
      "Epoch [37/100], Step [570/1751], Loss: 1.3017\n",
      "Epoch [37/100], Step [580/1751], Loss: 1.1494\n",
      "Epoch [37/100], Step [590/1751], Loss: 1.2982\n",
      "Epoch [37/100], Step [600/1751], Loss: 1.2308\n",
      "Epoch [37/100], Step [610/1751], Loss: 1.3049\n",
      "Epoch [37/100], Step [620/1751], Loss: 1.1082\n",
      "Epoch [37/100], Step [630/1751], Loss: 1.1879\n",
      "Epoch [37/100], Step [640/1751], Loss: 1.3520\n",
      "Epoch [37/100], Step [650/1751], Loss: 1.2315\n",
      "Epoch [37/100], Step [660/1751], Loss: 1.3646\n",
      "Epoch [37/100], Step [670/1751], Loss: 1.1865\n",
      "Epoch [37/100], Step [680/1751], Loss: 1.1324\n",
      "Epoch [37/100], Step [690/1751], Loss: 1.3734\n",
      "Epoch [37/100], Step [700/1751], Loss: 1.2736\n",
      "Epoch [37/100], Step [710/1751], Loss: 1.1149\n",
      "Epoch [37/100], Step [720/1751], Loss: 1.3334\n",
      "Epoch [37/100], Step [730/1751], Loss: 1.1002\n",
      "Epoch [37/100], Step [740/1751], Loss: 1.3853\n",
      "Epoch [37/100], Step [750/1751], Loss: 1.1775\n",
      "Epoch [37/100], Step [760/1751], Loss: 1.2598\n",
      "Epoch [37/100], Step [770/1751], Loss: 1.2493\n",
      "Epoch [37/100], Step [780/1751], Loss: 1.3414\n",
      "Epoch [37/100], Step [790/1751], Loss: 1.3725\n",
      "Epoch [37/100], Step [800/1751], Loss: 1.3352\n",
      "Epoch [37/100], Step [810/1751], Loss: 1.2718\n",
      "Epoch [37/100], Step [820/1751], Loss: 1.3407\n",
      "Epoch [37/100], Step [830/1751], Loss: 1.3458\n",
      "Epoch [37/100], Step [840/1751], Loss: 1.2934\n",
      "Epoch [37/100], Step [850/1751], Loss: 1.2409\n",
      "Epoch [37/100], Step [860/1751], Loss: 1.1053\n",
      "Epoch [37/100], Step [870/1751], Loss: 1.2336\n",
      "Epoch [37/100], Step [880/1751], Loss: 1.1727\n",
      "Epoch [37/100], Step [890/1751], Loss: 1.1728\n",
      "Epoch [37/100], Step [900/1751], Loss: 1.2426\n",
      "Epoch [37/100], Step [910/1751], Loss: 1.1150\n",
      "Epoch [37/100], Step [920/1751], Loss: 1.2757\n",
      "Epoch [37/100], Step [930/1751], Loss: 1.2738\n",
      "Epoch [37/100], Step [940/1751], Loss: 1.2021\n",
      "Epoch [37/100], Step [950/1751], Loss: 1.2683\n",
      "Epoch [37/100], Step [960/1751], Loss: 1.1838\n",
      "Epoch [37/100], Step [970/1751], Loss: 1.1269\n",
      "Epoch [37/100], Step [980/1751], Loss: 1.1250\n",
      "Epoch [37/100], Step [990/1751], Loss: 1.1582\n",
      "Epoch [37/100], Step [1000/1751], Loss: 1.2725\n",
      "Epoch [37/100], Step [1010/1751], Loss: 1.2418\n",
      "Epoch [37/100], Step [1020/1751], Loss: 1.2162\n",
      "Epoch [37/100], Step [1030/1751], Loss: 1.2882\n",
      "Epoch [37/100], Step [1040/1751], Loss: 1.1687\n",
      "Epoch [37/100], Step [1050/1751], Loss: 1.3101\n",
      "Epoch [37/100], Step [1060/1751], Loss: 1.1587\n",
      "Epoch [37/100], Step [1070/1751], Loss: 1.3351\n",
      "Epoch [37/100], Step [1080/1751], Loss: 1.3141\n",
      "Epoch [37/100], Step [1090/1751], Loss: 1.1677\n",
      "Epoch [37/100], Step [1100/1751], Loss: 1.1573\n",
      "Epoch [37/100], Step [1110/1751], Loss: 1.1465\n",
      "Epoch [37/100], Step [1120/1751], Loss: 1.3230\n",
      "Epoch [37/100], Step [1130/1751], Loss: 1.2085\n",
      "Epoch [37/100], Step [1140/1751], Loss: 1.2923\n",
      "Epoch [37/100], Step [1150/1751], Loss: 1.1945\n",
      "Epoch [37/100], Step [1160/1751], Loss: 1.2684\n",
      "Epoch [37/100], Step [1170/1751], Loss: 1.1618\n",
      "Epoch [37/100], Step [1180/1751], Loss: 1.1777\n",
      "Epoch [37/100], Step [1190/1751], Loss: 1.2657\n",
      "Epoch [37/100], Step [1200/1751], Loss: 1.1836\n",
      "Epoch [37/100], Step [1210/1751], Loss: 1.3074\n",
      "Epoch [37/100], Step [1220/1751], Loss: 1.1656\n",
      "Epoch [37/100], Step [1230/1751], Loss: 1.0457\n",
      "Epoch [37/100], Step [1240/1751], Loss: 1.1514\n",
      "Epoch [37/100], Step [1250/1751], Loss: 1.0935\n",
      "Epoch [37/100], Step [1260/1751], Loss: 1.0264\n",
      "Epoch [37/100], Step [1270/1751], Loss: 1.2291\n",
      "Epoch [37/100], Step [1280/1751], Loss: 1.1387\n",
      "Epoch [37/100], Step [1290/1751], Loss: 1.1990\n",
      "Epoch [37/100], Step [1300/1751], Loss: 1.1748\n",
      "Epoch [37/100], Step [1310/1751], Loss: 0.9862\n",
      "Epoch [37/100], Step [1320/1751], Loss: 1.3093\n",
      "Epoch [37/100], Step [1330/1751], Loss: 1.3093\n",
      "Epoch [37/100], Step [1340/1751], Loss: 1.1225\n",
      "Epoch [37/100], Step [1350/1751], Loss: 1.2458\n",
      "Epoch [37/100], Step [1360/1751], Loss: 1.3046\n",
      "Epoch [37/100], Step [1370/1751], Loss: 1.2177\n",
      "Epoch [37/100], Step [1380/1751], Loss: 1.2854\n",
      "Epoch [37/100], Step [1390/1751], Loss: 1.1319\n",
      "Epoch [37/100], Step [1400/1751], Loss: 1.4100\n",
      "Epoch [37/100], Step [1410/1751], Loss: 1.0319\n",
      "Epoch [37/100], Step [1420/1751], Loss: 1.1699\n",
      "Epoch [37/100], Step [1430/1751], Loss: 1.3008\n",
      "Epoch [37/100], Step [1440/1751], Loss: 1.2805\n",
      "Epoch [37/100], Step [1450/1751], Loss: 1.2786\n",
      "Epoch [37/100], Step [1460/1751], Loss: 1.2480\n",
      "Epoch [37/100], Step [1470/1751], Loss: 1.1840\n",
      "Epoch [37/100], Step [1480/1751], Loss: 1.1912\n",
      "Epoch [37/100], Step [1490/1751], Loss: 1.2046\n",
      "Epoch [37/100], Step [1500/1751], Loss: 1.3625\n",
      "Epoch [37/100], Step [1510/1751], Loss: 1.2148\n",
      "Epoch [37/100], Step [1520/1751], Loss: 1.3562\n",
      "Epoch [37/100], Step [1530/1751], Loss: 1.0641\n",
      "Epoch [37/100], Step [1540/1751], Loss: 1.2243\n",
      "Epoch [37/100], Step [1550/1751], Loss: 1.2510\n",
      "Epoch [37/100], Step [1560/1751], Loss: 1.1747\n",
      "Epoch [37/100], Step [1570/1751], Loss: 1.2562\n",
      "Epoch [37/100], Step [1580/1751], Loss: 1.1624\n",
      "Epoch [37/100], Step [1590/1751], Loss: 1.1515\n",
      "Epoch [37/100], Step [1600/1751], Loss: 1.2915\n",
      "Epoch [37/100], Step [1610/1751], Loss: 1.2400\n",
      "Epoch [37/100], Step [1620/1751], Loss: 1.0855\n",
      "Epoch [37/100], Step [1630/1751], Loss: 1.2707\n",
      "Epoch [37/100], Step [1640/1751], Loss: 1.3240\n",
      "Epoch [37/100], Step [1650/1751], Loss: 1.3885\n",
      "Epoch [37/100], Step [1660/1751], Loss: 1.2796\n",
      "Epoch [37/100], Step [1670/1751], Loss: 1.4400\n",
      "Epoch [37/100], Step [1680/1751], Loss: 1.3923\n",
      "Epoch [37/100], Step [1690/1751], Loss: 1.3417\n",
      "Epoch [37/100], Step [1700/1751], Loss: 1.3121\n",
      "Epoch [37/100], Step [1710/1751], Loss: 1.2237\n",
      "Epoch [37/100], Step [1720/1751], Loss: 1.2213\n",
      "Epoch [37/100], Step [1730/1751], Loss: 1.3423\n",
      "Epoch [37/100], Step [1740/1751], Loss: 1.2041\n",
      "Epoch [37/100], Step [1750/1751], Loss: 1.4022\n",
      "Epoch [37/100], Average Loss: 1.2270, Time: 1649.4249s\n",
      "Epoch [38/100], Step [10/1751], Loss: 1.1940\n",
      "Epoch [38/100], Step [20/1751], Loss: 1.2845\n",
      "Epoch [38/100], Step [30/1751], Loss: 1.1346\n",
      "Epoch [38/100], Step [40/1751], Loss: 1.1431\n",
      "Epoch [38/100], Step [50/1751], Loss: 1.3874\n",
      "Epoch [38/100], Step [60/1751], Loss: 1.1405\n",
      "Epoch [38/100], Step [70/1751], Loss: 1.1981\n",
      "Epoch [38/100], Step [80/1751], Loss: 1.2904\n",
      "Epoch [38/100], Step [90/1751], Loss: 1.1262\n",
      "Epoch [38/100], Step [100/1751], Loss: 1.1521\n",
      "Epoch [38/100], Step [110/1751], Loss: 1.1764\n",
      "Epoch [38/100], Step [120/1751], Loss: 1.2067\n",
      "Epoch [38/100], Step [130/1751], Loss: 1.1849\n",
      "Epoch [38/100], Step [140/1751], Loss: 1.3559\n",
      "Epoch [38/100], Step [150/1751], Loss: 0.9987\n",
      "Epoch [38/100], Step [160/1751], Loss: 1.2055\n",
      "Epoch [38/100], Step [170/1751], Loss: 1.1099\n",
      "Epoch [38/100], Step [180/1751], Loss: 1.3163\n",
      "Epoch [38/100], Step [190/1751], Loss: 1.0945\n",
      "Epoch [38/100], Step [200/1751], Loss: 1.2417\n",
      "Epoch [38/100], Step [210/1751], Loss: 1.2234\n",
      "Epoch [38/100], Step [220/1751], Loss: 1.1205\n",
      "Epoch [38/100], Step [230/1751], Loss: 1.0550\n",
      "Epoch [38/100], Step [240/1751], Loss: 1.1286\n",
      "Epoch [38/100], Step [250/1751], Loss: 1.2112\n",
      "Epoch [38/100], Step [260/1751], Loss: 1.4045\n",
      "Epoch [38/100], Step [270/1751], Loss: 1.0840\n",
      "Epoch [38/100], Step [280/1751], Loss: 1.1684\n",
      "Epoch [38/100], Step [290/1751], Loss: 1.2683\n",
      "Epoch [38/100], Step [300/1751], Loss: 1.1768\n",
      "Epoch [38/100], Step [310/1751], Loss: 1.3108\n",
      "Epoch [38/100], Step [320/1751], Loss: 1.4482\n",
      "Epoch [38/100], Step [330/1751], Loss: 1.2159\n",
      "Epoch [38/100], Step [340/1751], Loss: 1.0668\n",
      "Epoch [38/100], Step [350/1751], Loss: 1.3142\n",
      "Epoch [38/100], Step [360/1751], Loss: 1.1475\n",
      "Epoch [38/100], Step [370/1751], Loss: 1.2520\n",
      "Epoch [38/100], Step [380/1751], Loss: 1.3381\n",
      "Epoch [38/100], Step [390/1751], Loss: 1.0537\n",
      "Epoch [38/100], Step [400/1751], Loss: 1.3850\n",
      "Epoch [38/100], Step [410/1751], Loss: 1.1781\n",
      "Epoch [38/100], Step [420/1751], Loss: 1.2166\n",
      "Epoch [38/100], Step [430/1751], Loss: 1.2124\n",
      "Epoch [38/100], Step [440/1751], Loss: 1.3248\n",
      "Epoch [38/100], Step [450/1751], Loss: 1.1477\n",
      "Epoch [38/100], Step [460/1751], Loss: 1.1068\n",
      "Epoch [38/100], Step [470/1751], Loss: 1.1279\n",
      "Epoch [38/100], Step [480/1751], Loss: 1.2156\n",
      "Epoch [38/100], Step [490/1751], Loss: 1.1303\n",
      "Epoch [38/100], Step [500/1751], Loss: 1.2798\n",
      "Epoch [38/100], Step [510/1751], Loss: 1.1675\n",
      "Epoch [38/100], Step [520/1751], Loss: 1.2500\n",
      "Epoch [38/100], Step [530/1751], Loss: 1.2020\n",
      "Epoch [38/100], Step [540/1751], Loss: 1.3539\n",
      "Epoch [38/100], Step [550/1751], Loss: 1.1509\n",
      "Epoch [38/100], Step [560/1751], Loss: 1.1895\n",
      "Epoch [38/100], Step [570/1751], Loss: 1.3083\n",
      "Epoch [38/100], Step [580/1751], Loss: 1.2723\n",
      "Epoch [38/100], Step [590/1751], Loss: 1.0883\n",
      "Epoch [38/100], Step [600/1751], Loss: 1.1351\n",
      "Epoch [38/100], Step [610/1751], Loss: 1.2527\n",
      "Epoch [38/100], Step [620/1751], Loss: 1.2779\n",
      "Epoch [38/100], Step [630/1751], Loss: 1.2616\n",
      "Epoch [38/100], Step [640/1751], Loss: 1.4071\n",
      "Epoch [38/100], Step [650/1751], Loss: 1.3054\n",
      "Epoch [38/100], Step [660/1751], Loss: 1.1563\n",
      "Epoch [38/100], Step [670/1751], Loss: 1.2015\n",
      "Epoch [38/100], Step [680/1751], Loss: 1.1948\n",
      "Epoch [38/100], Step [690/1751], Loss: 1.2934\n",
      "Epoch [38/100], Step [700/1751], Loss: 1.1525\n",
      "Epoch [38/100], Step [710/1751], Loss: 1.2663\n",
      "Epoch [38/100], Step [720/1751], Loss: 1.2138\n",
      "Epoch [38/100], Step [730/1751], Loss: 1.1307\n",
      "Epoch [38/100], Step [740/1751], Loss: 1.2882\n",
      "Epoch [38/100], Step [750/1751], Loss: 1.1853\n",
      "Epoch [38/100], Step [760/1751], Loss: 1.0861\n",
      "Epoch [38/100], Step [770/1751], Loss: 1.1981\n",
      "Epoch [38/100], Step [780/1751], Loss: 1.1875\n",
      "Epoch [38/100], Step [790/1751], Loss: 1.2727\n",
      "Epoch [38/100], Step [800/1751], Loss: 1.1431\n",
      "Epoch [38/100], Step [810/1751], Loss: 1.2482\n",
      "Epoch [38/100], Step [820/1751], Loss: 1.1341\n",
      "Epoch [38/100], Step [830/1751], Loss: 1.2746\n",
      "Epoch [38/100], Step [840/1751], Loss: 1.1636\n",
      "Epoch [38/100], Step [850/1751], Loss: 1.1881\n",
      "Epoch [38/100], Step [860/1751], Loss: 1.2487\n",
      "Epoch [38/100], Step [870/1751], Loss: 1.2994\n",
      "Epoch [38/100], Step [880/1751], Loss: 1.3204\n",
      "Epoch [38/100], Step [890/1751], Loss: 1.3116\n",
      "Epoch [38/100], Step [900/1751], Loss: 1.1523\n",
      "Epoch [38/100], Step [910/1751], Loss: 1.1818\n",
      "Epoch [38/100], Step [920/1751], Loss: 1.1569\n",
      "Epoch [38/100], Step [930/1751], Loss: 1.2089\n",
      "Epoch [38/100], Step [940/1751], Loss: 1.2015\n",
      "Epoch [38/100], Step [950/1751], Loss: 1.3202\n",
      "Epoch [38/100], Step [960/1751], Loss: 1.4033\n",
      "Epoch [38/100], Step [970/1751], Loss: 1.3053\n",
      "Epoch [38/100], Step [980/1751], Loss: 1.2380\n",
      "Epoch [38/100], Step [990/1751], Loss: 1.1979\n",
      "Epoch [38/100], Step [1000/1751], Loss: 1.1354\n",
      "Epoch [38/100], Step [1010/1751], Loss: 1.1923\n",
      "Epoch [38/100], Step [1020/1751], Loss: 1.4254\n",
      "Epoch [38/100], Step [1030/1751], Loss: 1.2997\n",
      "Epoch [38/100], Step [1040/1751], Loss: 1.2337\n",
      "Epoch [38/100], Step [1050/1751], Loss: 1.1023\n",
      "Epoch [38/100], Step [1060/1751], Loss: 1.2684\n",
      "Epoch [38/100], Step [1070/1751], Loss: 1.3475\n",
      "Epoch [38/100], Step [1080/1751], Loss: 1.2242\n",
      "Epoch [38/100], Step [1090/1751], Loss: 1.1834\n",
      "Epoch [38/100], Step [1100/1751], Loss: 1.3509\n",
      "Epoch [38/100], Step [1110/1751], Loss: 1.1520\n",
      "Epoch [38/100], Step [1120/1751], Loss: 1.2541\n",
      "Epoch [38/100], Step [1130/1751], Loss: 1.2066\n",
      "Epoch [38/100], Step [1140/1751], Loss: 1.2642\n",
      "Epoch [38/100], Step [1150/1751], Loss: 1.1166\n",
      "Epoch [38/100], Step [1160/1751], Loss: 1.0459\n",
      "Epoch [38/100], Step [1170/1751], Loss: 1.3048\n",
      "Epoch [38/100], Step [1180/1751], Loss: 1.1199\n",
      "Epoch [38/100], Step [1190/1751], Loss: 1.2468\n",
      "Epoch [38/100], Step [1200/1751], Loss: 1.2530\n",
      "Epoch [38/100], Step [1210/1751], Loss: 1.0816\n",
      "Epoch [38/100], Step [1220/1751], Loss: 1.2178\n",
      "Epoch [38/100], Step [1230/1751], Loss: 1.0580\n",
      "Epoch [38/100], Step [1240/1751], Loss: 1.2530\n",
      "Epoch [38/100], Step [1250/1751], Loss: 1.2504\n",
      "Epoch [38/100], Step [1260/1751], Loss: 1.2386\n",
      "Epoch [38/100], Step [1270/1751], Loss: 1.1699\n",
      "Epoch [38/100], Step [1280/1751], Loss: 1.2036\n",
      "Epoch [38/100], Step [1290/1751], Loss: 1.1436\n",
      "Epoch [38/100], Step [1300/1751], Loss: 1.1755\n",
      "Epoch [38/100], Step [1310/1751], Loss: 1.1736\n",
      "Epoch [38/100], Step [1320/1751], Loss: 1.1947\n",
      "Epoch [38/100], Step [1330/1751], Loss: 1.1668\n",
      "Epoch [38/100], Step [1340/1751], Loss: 1.3826\n",
      "Epoch [38/100], Step [1350/1751], Loss: 1.3177\n",
      "Epoch [38/100], Step [1360/1751], Loss: 1.2829\n",
      "Epoch [38/100], Step [1370/1751], Loss: 1.2117\n",
      "Epoch [38/100], Step [1380/1751], Loss: 1.3361\n",
      "Epoch [38/100], Step [1390/1751], Loss: 1.1987\n",
      "Epoch [38/100], Step [1400/1751], Loss: 1.1905\n",
      "Epoch [38/100], Step [1410/1751], Loss: 1.1835\n",
      "Epoch [38/100], Step [1420/1751], Loss: 1.1582\n",
      "Epoch [38/100], Step [1430/1751], Loss: 1.1733\n",
      "Epoch [38/100], Step [1440/1751], Loss: 1.2183\n",
      "Epoch [38/100], Step [1450/1751], Loss: 1.2085\n",
      "Epoch [38/100], Step [1460/1751], Loss: 1.2362\n",
      "Epoch [38/100], Step [1470/1751], Loss: 1.2733\n",
      "Epoch [38/100], Step [1480/1751], Loss: 1.3012\n",
      "Epoch [38/100], Step [1490/1751], Loss: 1.2410\n",
      "Epoch [38/100], Step [1500/1751], Loss: 1.2511\n",
      "Epoch [38/100], Step [1510/1751], Loss: 1.2126\n",
      "Epoch [38/100], Step [1520/1751], Loss: 1.1514\n",
      "Epoch [38/100], Step [1530/1751], Loss: 1.4217\n",
      "Epoch [38/100], Step [1540/1751], Loss: 1.3908\n",
      "Epoch [38/100], Step [1550/1751], Loss: 1.3492\n",
      "Epoch [38/100], Step [1560/1751], Loss: 1.1703\n",
      "Epoch [38/100], Step [1570/1751], Loss: 1.4027\n",
      "Epoch [38/100], Step [1580/1751], Loss: 1.3884\n",
      "Epoch [38/100], Step [1590/1751], Loss: 1.3116\n",
      "Epoch [38/100], Step [1600/1751], Loss: 1.2331\n",
      "Epoch [38/100], Step [1610/1751], Loss: 1.1872\n",
      "Epoch [38/100], Step [1620/1751], Loss: 1.1238\n",
      "Epoch [38/100], Step [1630/1751], Loss: 1.2243\n",
      "Epoch [38/100], Step [1640/1751], Loss: 1.3055\n",
      "Epoch [38/100], Step [1650/1751], Loss: 1.2020\n",
      "Epoch [38/100], Step [1660/1751], Loss: 1.1770\n",
      "Epoch [38/100], Step [1670/1751], Loss: 1.2645\n",
      "Epoch [38/100], Step [1680/1751], Loss: 1.1788\n",
      "Epoch [38/100], Step [1690/1751], Loss: 1.2298\n",
      "Epoch [38/100], Step [1700/1751], Loss: 1.0342\n",
      "Epoch [38/100], Step [1710/1751], Loss: 1.3469\n",
      "Epoch [38/100], Step [1720/1751], Loss: 1.1836\n",
      "Epoch [38/100], Step [1730/1751], Loss: 1.1613\n",
      "Epoch [38/100], Step [1740/1751], Loss: 1.1004\n",
      "Epoch [38/100], Step [1750/1751], Loss: 1.1307\n",
      "Epoch [38/100], Average Loss: 1.2249, Time: 1650.0911s\n",
      "Epoch [39/100], Step [10/1751], Loss: 1.2669\n",
      "Epoch [39/100], Step [20/1751], Loss: 1.0912\n",
      "Epoch [39/100], Step [30/1751], Loss: 1.2772\n",
      "Epoch [39/100], Step [40/1751], Loss: 1.3245\n",
      "Epoch [39/100], Step [50/1751], Loss: 1.1876\n",
      "Epoch [39/100], Step [60/1751], Loss: 1.2849\n",
      "Epoch [39/100], Step [70/1751], Loss: 1.0891\n",
      "Epoch [39/100], Step [80/1751], Loss: 1.1989\n",
      "Epoch [39/100], Step [90/1751], Loss: 1.1294\n",
      "Epoch [39/100], Step [100/1751], Loss: 1.1543\n",
      "Epoch [39/100], Step [110/1751], Loss: 1.1340\n",
      "Epoch [39/100], Step [120/1751], Loss: 1.1684\n",
      "Epoch [39/100], Step [130/1751], Loss: 1.1815\n",
      "Epoch [39/100], Step [140/1751], Loss: 1.2675\n",
      "Epoch [39/100], Step [150/1751], Loss: 1.1200\n",
      "Epoch [39/100], Step [160/1751], Loss: 1.1836\n",
      "Epoch [39/100], Step [170/1751], Loss: 1.1150\n",
      "Epoch [39/100], Step [180/1751], Loss: 1.1742\n",
      "Epoch [39/100], Step [190/1751], Loss: 1.2927\n",
      "Epoch [39/100], Step [200/1751], Loss: 1.2931\n",
      "Epoch [39/100], Step [210/1751], Loss: 1.2196\n",
      "Epoch [39/100], Step [220/1751], Loss: 1.1913\n",
      "Epoch [39/100], Step [230/1751], Loss: 1.2156\n",
      "Epoch [39/100], Step [240/1751], Loss: 1.1702\n",
      "Epoch [39/100], Step [250/1751], Loss: 1.4336\n",
      "Epoch [39/100], Step [260/1751], Loss: 1.2727\n",
      "Epoch [39/100], Step [270/1751], Loss: 1.1136\n",
      "Epoch [39/100], Step [280/1751], Loss: 1.2417\n",
      "Epoch [39/100], Step [290/1751], Loss: 1.2604\n",
      "Epoch [39/100], Step [300/1751], Loss: 1.0775\n",
      "Epoch [39/100], Step [310/1751], Loss: 1.3253\n",
      "Epoch [39/100], Step [320/1751], Loss: 1.3781\n",
      "Epoch [39/100], Step [330/1751], Loss: 1.1216\n",
      "Epoch [39/100], Step [340/1751], Loss: 1.3536\n",
      "Epoch [39/100], Step [350/1751], Loss: 1.2550\n",
      "Epoch [39/100], Step [360/1751], Loss: 1.1560\n",
      "Epoch [39/100], Step [370/1751], Loss: 1.0812\n",
      "Epoch [39/100], Step [380/1751], Loss: 1.3562\n",
      "Epoch [39/100], Step [390/1751], Loss: 1.2308\n",
      "Epoch [39/100], Step [400/1751], Loss: 1.0676\n",
      "Epoch [39/100], Step [410/1751], Loss: 1.3480\n",
      "Epoch [39/100], Step [420/1751], Loss: 1.2191\n",
      "Epoch [39/100], Step [430/1751], Loss: 1.3233\n",
      "Epoch [39/100], Step [440/1751], Loss: 1.3322\n",
      "Epoch [39/100], Step [450/1751], Loss: 1.2633\n",
      "Epoch [39/100], Step [460/1751], Loss: 1.2036\n",
      "Epoch [39/100], Step [470/1751], Loss: 1.0551\n",
      "Epoch [39/100], Step [480/1751], Loss: 1.1474\n",
      "Epoch [39/100], Step [490/1751], Loss: 1.3583\n",
      "Epoch [39/100], Step [500/1751], Loss: 1.1857\n",
      "Epoch [39/100], Step [510/1751], Loss: 1.2667\n",
      "Epoch [39/100], Step [520/1751], Loss: 1.2401\n",
      "Epoch [39/100], Step [530/1751], Loss: 1.2636\n",
      "Epoch [39/100], Step [540/1751], Loss: 1.2692\n",
      "Epoch [39/100], Step [550/1751], Loss: 1.1843\n",
      "Epoch [39/100], Step [560/1751], Loss: 1.2279\n",
      "Epoch [39/100], Step [570/1751], Loss: 1.3622\n",
      "Epoch [39/100], Step [580/1751], Loss: 1.3822\n",
      "Epoch [39/100], Step [590/1751], Loss: 1.2480\n",
      "Epoch [39/100], Step [600/1751], Loss: 1.3187\n",
      "Epoch [39/100], Step [610/1751], Loss: 1.3929\n",
      "Epoch [39/100], Step [620/1751], Loss: 1.2209\n",
      "Epoch [39/100], Step [630/1751], Loss: 1.2370\n",
      "Epoch [39/100], Step [640/1751], Loss: 1.2391\n",
      "Epoch [39/100], Step [650/1751], Loss: 1.2503\n",
      "Epoch [39/100], Step [660/1751], Loss: 1.3726\n",
      "Epoch [39/100], Step [670/1751], Loss: 1.1980\n",
      "Epoch [39/100], Step [680/1751], Loss: 1.2710\n",
      "Epoch [39/100], Step [690/1751], Loss: 1.0305\n",
      "Epoch [39/100], Step [700/1751], Loss: 1.2069\n",
      "Epoch [39/100], Step [710/1751], Loss: 1.2064\n",
      "Epoch [39/100], Step [720/1751], Loss: 1.1799\n",
      "Epoch [39/100], Step [730/1751], Loss: 1.3423\n",
      "Epoch [39/100], Step [740/1751], Loss: 1.1917\n",
      "Epoch [39/100], Step [750/1751], Loss: 1.1044\n",
      "Epoch [39/100], Step [760/1751], Loss: 1.2125\n",
      "Epoch [39/100], Step [770/1751], Loss: 1.2689\n",
      "Epoch [39/100], Step [780/1751], Loss: 1.1649\n",
      "Epoch [39/100], Step [790/1751], Loss: 1.2863\n",
      "Epoch [39/100], Step [800/1751], Loss: 1.3492\n",
      "Epoch [39/100], Step [810/1751], Loss: 1.2708\n",
      "Epoch [39/100], Step [820/1751], Loss: 1.1207\n",
      "Epoch [39/100], Step [830/1751], Loss: 1.1424\n",
      "Epoch [39/100], Step [840/1751], Loss: 1.2538\n",
      "Epoch [39/100], Step [850/1751], Loss: 1.3080\n",
      "Epoch [39/100], Step [860/1751], Loss: 1.0517\n",
      "Epoch [39/100], Step [870/1751], Loss: 1.1567\n",
      "Epoch [39/100], Step [880/1751], Loss: 1.1511\n",
      "Epoch [39/100], Step [890/1751], Loss: 1.2561\n",
      "Epoch [39/100], Step [900/1751], Loss: 1.2765\n",
      "Epoch [39/100], Step [910/1751], Loss: 1.1580\n",
      "Epoch [39/100], Step [920/1751], Loss: 1.2094\n",
      "Epoch [39/100], Step [930/1751], Loss: 1.2381\n",
      "Epoch [39/100], Step [940/1751], Loss: 1.3130\n",
      "Epoch [39/100], Step [950/1751], Loss: 1.2013\n",
      "Epoch [39/100], Step [960/1751], Loss: 1.2357\n",
      "Epoch [39/100], Step [970/1751], Loss: 1.2310\n",
      "Epoch [39/100], Step [980/1751], Loss: 1.2085\n",
      "Epoch [39/100], Step [990/1751], Loss: 1.2733\n",
      "Epoch [39/100], Step [1000/1751], Loss: 1.2316\n",
      "Epoch [39/100], Step [1010/1751], Loss: 1.2742\n",
      "Epoch [39/100], Step [1020/1751], Loss: 1.2968\n",
      "Epoch [39/100], Step [1030/1751], Loss: 1.3972\n",
      "Epoch [39/100], Step [1040/1751], Loss: 1.1799\n",
      "Epoch [39/100], Step [1050/1751], Loss: 1.1654\n",
      "Epoch [39/100], Step [1060/1751], Loss: 1.1826\n",
      "Epoch [39/100], Step [1070/1751], Loss: 1.3497\n",
      "Epoch [39/100], Step [1080/1751], Loss: 1.2625\n",
      "Epoch [39/100], Step [1090/1751], Loss: 1.2142\n",
      "Epoch [39/100], Step [1100/1751], Loss: 1.2890\n",
      "Epoch [39/100], Step [1110/1751], Loss: 1.2595\n",
      "Epoch [39/100], Step [1120/1751], Loss: 1.1850\n",
      "Epoch [39/100], Step [1130/1751], Loss: 1.1762\n",
      "Epoch [39/100], Step [1140/1751], Loss: 1.2535\n",
      "Epoch [39/100], Step [1150/1751], Loss: 1.2176\n",
      "Epoch [39/100], Step [1160/1751], Loss: 1.3160\n",
      "Epoch [39/100], Step [1170/1751], Loss: 1.1130\n",
      "Epoch [39/100], Step [1180/1751], Loss: 1.3089\n",
      "Epoch [39/100], Step [1190/1751], Loss: 1.1658\n",
      "Epoch [39/100], Step [1200/1751], Loss: 1.2595\n",
      "Epoch [39/100], Step [1210/1751], Loss: 1.2332\n",
      "Epoch [39/100], Step [1220/1751], Loss: 1.2012\n",
      "Epoch [39/100], Step [1230/1751], Loss: 1.0505\n",
      "Epoch [39/100], Step [1240/1751], Loss: 1.0761\n",
      "Epoch [39/100], Step [1250/1751], Loss: 1.2116\n",
      "Epoch [39/100], Step [1260/1751], Loss: 1.3927\n",
      "Epoch [39/100], Step [1270/1751], Loss: 1.0843\n",
      "Epoch [39/100], Step [1280/1751], Loss: 1.1105\n",
      "Epoch [39/100], Step [1290/1751], Loss: 1.2136\n",
      "Epoch [39/100], Step [1300/1751], Loss: 1.1276\n",
      "Epoch [39/100], Step [1310/1751], Loss: 1.3574\n",
      "Epoch [39/100], Step [1320/1751], Loss: 1.3492\n",
      "Epoch [39/100], Step [1330/1751], Loss: 1.1836\n",
      "Epoch [39/100], Step [1340/1751], Loss: 1.3977\n",
      "Epoch [39/100], Step [1350/1751], Loss: 1.1401\n",
      "Epoch [39/100], Step [1360/1751], Loss: 1.2575\n",
      "Epoch [39/100], Step [1370/1751], Loss: 1.1656\n",
      "Epoch [39/100], Step [1380/1751], Loss: 1.2376\n",
      "Epoch [39/100], Step [1390/1751], Loss: 1.4510\n",
      "Epoch [39/100], Step [1400/1751], Loss: 1.2513\n",
      "Epoch [39/100], Step [1410/1751], Loss: 1.0931\n",
      "Epoch [39/100], Step [1420/1751], Loss: 1.2729\n",
      "Epoch [39/100], Step [1430/1751], Loss: 1.2676\n",
      "Epoch [39/100], Step [1440/1751], Loss: 1.1441\n",
      "Epoch [39/100], Step [1450/1751], Loss: 1.1410\n",
      "Epoch [39/100], Step [1460/1751], Loss: 1.2849\n",
      "Epoch [39/100], Step [1470/1751], Loss: 1.1796\n",
      "Epoch [39/100], Step [1480/1751], Loss: 1.2014\n",
      "Epoch [39/100], Step [1490/1751], Loss: 1.2084\n",
      "Epoch [39/100], Step [1500/1751], Loss: 1.2331\n",
      "Epoch [39/100], Step [1510/1751], Loss: 1.2745\n",
      "Epoch [39/100], Step [1520/1751], Loss: 1.2770\n",
      "Epoch [39/100], Step [1530/1751], Loss: 1.0739\n",
      "Epoch [39/100], Step [1540/1751], Loss: 1.2450\n",
      "Epoch [39/100], Step [1550/1751], Loss: 1.1359\n",
      "Epoch [39/100], Step [1560/1751], Loss: 1.2748\n",
      "Epoch [39/100], Step [1570/1751], Loss: 1.2215\n",
      "Epoch [39/100], Step [1580/1751], Loss: 1.3033\n",
      "Epoch [39/100], Step [1590/1751], Loss: 1.2636\n",
      "Epoch [39/100], Step [1600/1751], Loss: 1.0933\n",
      "Epoch [39/100], Step [1610/1751], Loss: 1.3616\n",
      "Epoch [39/100], Step [1620/1751], Loss: 1.4209\n",
      "Epoch [39/100], Step [1630/1751], Loss: 1.2484\n",
      "Epoch [39/100], Step [1640/1751], Loss: 1.1220\n",
      "Epoch [39/100], Step [1650/1751], Loss: 1.1292\n",
      "Epoch [39/100], Step [1660/1751], Loss: 1.1744\n",
      "Epoch [39/100], Step [1670/1751], Loss: 1.1660\n",
      "Epoch [39/100], Step [1680/1751], Loss: 1.1782\n",
      "Epoch [39/100], Step [1690/1751], Loss: 1.3236\n",
      "Epoch [39/100], Step [1700/1751], Loss: 1.2723\n",
      "Epoch [39/100], Step [1710/1751], Loss: 1.1800\n",
      "Epoch [39/100], Step [1720/1751], Loss: 1.3174\n",
      "Epoch [39/100], Step [1730/1751], Loss: 1.1949\n",
      "Epoch [39/100], Step [1740/1751], Loss: 1.2700\n",
      "Epoch [39/100], Step [1750/1751], Loss: 1.1683\n",
      "Epoch [39/100], Average Loss: 1.2241, Time: 1649.9929s\n",
      "Epoch [40/100], Step [10/1751], Loss: 1.2312\n",
      "Epoch [40/100], Step [20/1751], Loss: 1.3064\n",
      "Epoch [40/100], Step [30/1751], Loss: 1.1139\n",
      "Epoch [40/100], Step [40/1751], Loss: 1.1977\n",
      "Epoch [40/100], Step [50/1751], Loss: 1.2048\n",
      "Epoch [40/100], Step [60/1751], Loss: 1.1620\n",
      "Epoch [40/100], Step [70/1751], Loss: 1.4201\n",
      "Epoch [40/100], Step [80/1751], Loss: 1.2453\n",
      "Epoch [40/100], Step [90/1751], Loss: 1.1929\n",
      "Epoch [40/100], Step [100/1751], Loss: 1.2091\n",
      "Epoch [40/100], Step [110/1751], Loss: 1.1058\n",
      "Epoch [40/100], Step [120/1751], Loss: 1.1965\n",
      "Epoch [40/100], Step [130/1751], Loss: 1.3396\n",
      "Epoch [40/100], Step [140/1751], Loss: 1.3444\n",
      "Epoch [40/100], Step [150/1751], Loss: 1.1327\n",
      "Epoch [40/100], Step [160/1751], Loss: 1.2101\n",
      "Epoch [40/100], Step [170/1751], Loss: 1.1120\n",
      "Epoch [40/100], Step [180/1751], Loss: 1.1400\n",
      "Epoch [40/100], Step [190/1751], Loss: 1.1246\n",
      "Epoch [40/100], Step [200/1751], Loss: 1.3725\n",
      "Epoch [40/100], Step [210/1751], Loss: 1.0620\n",
      "Epoch [40/100], Step [220/1751], Loss: 1.2373\n",
      "Epoch [40/100], Step [230/1751], Loss: 1.2050\n",
      "Epoch [40/100], Step [240/1751], Loss: 1.3175\n",
      "Epoch [40/100], Step [250/1751], Loss: 1.2442\n",
      "Epoch [40/100], Step [260/1751], Loss: 1.0875\n",
      "Epoch [40/100], Step [270/1751], Loss: 1.3019\n",
      "Epoch [40/100], Step [280/1751], Loss: 1.2882\n",
      "Epoch [40/100], Step [290/1751], Loss: 1.2989\n",
      "Epoch [40/100], Step [300/1751], Loss: 1.2800\n",
      "Epoch [40/100], Step [310/1751], Loss: 1.0349\n",
      "Epoch [40/100], Step [320/1751], Loss: 1.2454\n",
      "Epoch [40/100], Step [330/1751], Loss: 1.2825\n",
      "Epoch [40/100], Step [340/1751], Loss: 1.1861\n",
      "Epoch [40/100], Step [350/1751], Loss: 1.2154\n",
      "Epoch [40/100], Step [360/1751], Loss: 1.2087\n",
      "Epoch [40/100], Step [370/1751], Loss: 1.1111\n",
      "Epoch [40/100], Step [380/1751], Loss: 1.1869\n",
      "Epoch [40/100], Step [390/1751], Loss: 1.1852\n",
      "Epoch [40/100], Step [400/1751], Loss: 1.1207\n",
      "Epoch [40/100], Step [410/1751], Loss: 1.2348\n",
      "Epoch [40/100], Step [420/1751], Loss: 1.4239\n",
      "Epoch [40/100], Step [430/1751], Loss: 1.0800\n",
      "Epoch [40/100], Step [440/1751], Loss: 1.4481\n",
      "Epoch [40/100], Step [450/1751], Loss: 1.2171\n",
      "Epoch [40/100], Step [460/1751], Loss: 1.2911\n",
      "Epoch [40/100], Step [470/1751], Loss: 1.2531\n",
      "Epoch [40/100], Step [480/1751], Loss: 1.1791\n",
      "Epoch [40/100], Step [490/1751], Loss: 1.1222\n",
      "Epoch [40/100], Step [500/1751], Loss: 1.1964\n",
      "Epoch [40/100], Step [510/1751], Loss: 1.2611\n",
      "Epoch [40/100], Step [520/1751], Loss: 1.1533\n",
      "Epoch [40/100], Step [530/1751], Loss: 1.0999\n",
      "Epoch [40/100], Step [540/1751], Loss: 1.2293\n",
      "Epoch [40/100], Step [550/1751], Loss: 1.3312\n",
      "Epoch [40/100], Step [560/1751], Loss: 1.2005\n",
      "Epoch [40/100], Step [570/1751], Loss: 1.3296\n",
      "Epoch [40/100], Step [580/1751], Loss: 1.2565\n",
      "Epoch [40/100], Step [590/1751], Loss: 1.1255\n",
      "Epoch [40/100], Step [600/1751], Loss: 1.1538\n",
      "Epoch [40/100], Step [610/1751], Loss: 1.1589\n",
      "Epoch [40/100], Step [620/1751], Loss: 1.2779\n",
      "Epoch [40/100], Step [630/1751], Loss: 1.2704\n",
      "Epoch [40/100], Step [640/1751], Loss: 1.1623\n",
      "Epoch [40/100], Step [650/1751], Loss: 1.1815\n",
      "Epoch [40/100], Step [660/1751], Loss: 1.1469\n",
      "Epoch [40/100], Step [670/1751], Loss: 1.1946\n",
      "Epoch [40/100], Step [680/1751], Loss: 1.0693\n",
      "Epoch [40/100], Step [690/1751], Loss: 1.1140\n",
      "Epoch [40/100], Step [700/1751], Loss: 1.2812\n",
      "Epoch [40/100], Step [710/1751], Loss: 1.1017\n",
      "Epoch [40/100], Step [720/1751], Loss: 1.1600\n",
      "Epoch [40/100], Step [730/1751], Loss: 1.0512\n",
      "Epoch [40/100], Step [740/1751], Loss: 0.9987\n",
      "Epoch [40/100], Step [750/1751], Loss: 1.2804\n",
      "Epoch [40/100], Step [760/1751], Loss: 1.1365\n",
      "Epoch [40/100], Step [770/1751], Loss: 1.1154\n",
      "Epoch [40/100], Step [780/1751], Loss: 1.1636\n",
      "Epoch [40/100], Step [790/1751], Loss: 1.2737\n",
      "Epoch [40/100], Step [800/1751], Loss: 1.1154\n",
      "Epoch [40/100], Step [810/1751], Loss: 1.1905\n",
      "Epoch [40/100], Step [820/1751], Loss: 1.2197\n",
      "Epoch [40/100], Step [830/1751], Loss: 1.2742\n",
      "Epoch [40/100], Step [840/1751], Loss: 1.3969\n",
      "Epoch [40/100], Step [850/1751], Loss: 1.4396\n",
      "Epoch [40/100], Step [860/1751], Loss: 1.1305\n",
      "Epoch [40/100], Step [870/1751], Loss: 1.2073\n",
      "Epoch [40/100], Step [880/1751], Loss: 1.3957\n",
      "Epoch [40/100], Step [890/1751], Loss: 1.3025\n",
      "Epoch [40/100], Step [900/1751], Loss: 1.3357\n",
      "Epoch [40/100], Step [910/1751], Loss: 1.1641\n",
      "Epoch [40/100], Step [920/1751], Loss: 1.1577\n",
      "Epoch [40/100], Step [930/1751], Loss: 1.3519\n",
      "Epoch [40/100], Step [940/1751], Loss: 1.2532\n",
      "Epoch [40/100], Step [950/1751], Loss: 1.3681\n",
      "Epoch [40/100], Step [960/1751], Loss: 1.1101\n",
      "Epoch [40/100], Step [970/1751], Loss: 0.9799\n",
      "Epoch [40/100], Step [980/1751], Loss: 1.0268\n",
      "Epoch [40/100], Step [990/1751], Loss: 1.1741\n",
      "Epoch [40/100], Step [1000/1751], Loss: 1.2308\n",
      "Epoch [40/100], Step [1010/1751], Loss: 1.1985\n",
      "Epoch [40/100], Step [1020/1751], Loss: 1.2504\n",
      "Epoch [40/100], Step [1030/1751], Loss: 1.3534\n",
      "Epoch [40/100], Step [1040/1751], Loss: 1.1072\n",
      "Epoch [40/100], Step [1050/1751], Loss: 1.2222\n",
      "Epoch [40/100], Step [1060/1751], Loss: 1.2652\n",
      "Epoch [40/100], Step [1070/1751], Loss: 1.2001\n",
      "Epoch [40/100], Step [1080/1751], Loss: 1.2707\n",
      "Epoch [40/100], Step [1090/1751], Loss: 1.1687\n",
      "Epoch [40/100], Step [1100/1751], Loss: 1.2177\n",
      "Epoch [40/100], Step [1110/1751], Loss: 1.1925\n",
      "Epoch [40/100], Step [1120/1751], Loss: 1.0740\n",
      "Epoch [40/100], Step [1130/1751], Loss: 1.2309\n",
      "Epoch [40/100], Step [1140/1751], Loss: 1.1904\n",
      "Epoch [40/100], Step [1150/1751], Loss: 1.1379\n",
      "Epoch [40/100], Step [1160/1751], Loss: 1.2198\n",
      "Epoch [40/100], Step [1170/1751], Loss: 1.1973\n",
      "Epoch [40/100], Step [1180/1751], Loss: 1.2250\n",
      "Epoch [40/100], Step [1190/1751], Loss: 1.2034\n",
      "Epoch [40/100], Step [1200/1751], Loss: 1.2450\n",
      "Epoch [40/100], Step [1210/1751], Loss: 1.2815\n",
      "Epoch [40/100], Step [1220/1751], Loss: 1.1986\n",
      "Epoch [40/100], Step [1230/1751], Loss: 1.2448\n",
      "Epoch [40/100], Step [1240/1751], Loss: 1.2710\n",
      "Epoch [40/100], Step [1250/1751], Loss: 1.3008\n",
      "Epoch [40/100], Step [1260/1751], Loss: 1.2905\n",
      "Epoch [40/100], Step [1270/1751], Loss: 1.3032\n",
      "Epoch [40/100], Step [1280/1751], Loss: 1.3623\n",
      "Epoch [40/100], Step [1290/1751], Loss: 1.1282\n",
      "Epoch [40/100], Step [1300/1751], Loss: 1.2041\n",
      "Epoch [40/100], Step [1310/1751], Loss: 1.2174\n",
      "Epoch [40/100], Step [1320/1751], Loss: 1.4272\n",
      "Epoch [40/100], Step [1330/1751], Loss: 1.1439\n",
      "Epoch [40/100], Step [1340/1751], Loss: 1.0131\n",
      "Epoch [40/100], Step [1350/1751], Loss: 1.1071\n",
      "Epoch [40/100], Step [1360/1751], Loss: 1.1861\n",
      "Epoch [40/100], Step [1370/1751], Loss: 1.3030\n",
      "Epoch [40/100], Step [1380/1751], Loss: 1.1928\n",
      "Epoch [40/100], Step [1390/1751], Loss: 1.2583\n",
      "Epoch [40/100], Step [1400/1751], Loss: 1.4441\n",
      "Epoch [40/100], Step [1410/1751], Loss: 1.1678\n",
      "Epoch [40/100], Step [1420/1751], Loss: 1.2662\n",
      "Epoch [40/100], Step [1430/1751], Loss: 1.2678\n",
      "Epoch [40/100], Step [1440/1751], Loss: 1.2078\n",
      "Epoch [40/100], Step [1450/1751], Loss: 1.2749\n",
      "Epoch [40/100], Step [1460/1751], Loss: 1.3044\n",
      "Epoch [40/100], Step [1470/1751], Loss: 1.3104\n",
      "Epoch [40/100], Step [1480/1751], Loss: 1.3511\n",
      "Epoch [40/100], Step [1490/1751], Loss: 1.2420\n",
      "Epoch [40/100], Step [1500/1751], Loss: 1.2503\n",
      "Epoch [40/100], Step [1510/1751], Loss: 1.1958\n",
      "Epoch [40/100], Step [1520/1751], Loss: 1.2869\n",
      "Epoch [40/100], Step [1530/1751], Loss: 1.0532\n",
      "Epoch [40/100], Step [1540/1751], Loss: 1.2666\n",
      "Epoch [40/100], Step [1550/1751], Loss: 1.1055\n",
      "Epoch [40/100], Step [1560/1751], Loss: 1.1554\n",
      "Epoch [40/100], Step [1570/1751], Loss: 1.2375\n",
      "Epoch [40/100], Step [1580/1751], Loss: 1.3668\n",
      "Epoch [40/100], Step [1590/1751], Loss: 1.0882\n",
      "Epoch [40/100], Step [1600/1751], Loss: 1.3130\n",
      "Epoch [40/100], Step [1610/1751], Loss: 1.2964\n",
      "Epoch [40/100], Step [1620/1751], Loss: 1.2992\n",
      "Epoch [40/100], Step [1630/1751], Loss: 1.1068\n",
      "Epoch [40/100], Step [1640/1751], Loss: 1.1532\n",
      "Epoch [40/100], Step [1650/1751], Loss: 1.2861\n",
      "Epoch [40/100], Step [1660/1751], Loss: 1.2210\n",
      "Epoch [40/100], Step [1670/1751], Loss: 1.1801\n",
      "Epoch [40/100], Step [1680/1751], Loss: 1.2010\n",
      "Epoch [40/100], Step [1690/1751], Loss: 1.2101\n",
      "Epoch [40/100], Step [1700/1751], Loss: 1.2322\n",
      "Epoch [40/100], Step [1710/1751], Loss: 1.1001\n",
      "Epoch [40/100], Step [1720/1751], Loss: 1.1744\n",
      "Epoch [40/100], Step [1730/1751], Loss: 1.4195\n",
      "Epoch [40/100], Step [1740/1751], Loss: 1.1620\n",
      "Epoch [40/100], Step [1750/1751], Loss: 1.1264\n",
      "Epoch [40/100], Average Loss: 1.2224, Time: 1649.4044s\n",
      "Epoch [41/100], Step [10/1751], Loss: 1.0847\n",
      "Epoch [41/100], Step [20/1751], Loss: 1.3889\n",
      "Epoch [41/100], Step [30/1751], Loss: 1.3408\n",
      "Epoch [41/100], Step [40/1751], Loss: 1.3437\n",
      "Epoch [41/100], Step [50/1751], Loss: 1.3216\n",
      "Epoch [41/100], Step [60/1751], Loss: 1.1662\n",
      "Epoch [41/100], Step [70/1751], Loss: 1.3673\n",
      "Epoch [41/100], Step [80/1751], Loss: 1.3200\n",
      "Epoch [41/100], Step [90/1751], Loss: 1.1185\n",
      "Epoch [41/100], Step [100/1751], Loss: 1.2121\n",
      "Epoch [41/100], Step [110/1751], Loss: 1.1874\n",
      "Epoch [41/100], Step [120/1751], Loss: 1.3311\n",
      "Epoch [41/100], Step [130/1751], Loss: 1.1605\n",
      "Epoch [41/100], Step [140/1751], Loss: 1.1134\n",
      "Epoch [41/100], Step [150/1751], Loss: 1.1340\n",
      "Epoch [41/100], Step [160/1751], Loss: 1.1614\n",
      "Epoch [41/100], Step [170/1751], Loss: 1.3745\n",
      "Epoch [41/100], Step [180/1751], Loss: 1.1561\n",
      "Epoch [41/100], Step [190/1751], Loss: 1.2112\n",
      "Epoch [41/100], Step [200/1751], Loss: 1.1982\n",
      "Epoch [41/100], Step [210/1751], Loss: 1.1737\n",
      "Epoch [41/100], Step [220/1751], Loss: 1.1187\n",
      "Epoch [41/100], Step [230/1751], Loss: 1.2477\n",
      "Epoch [41/100], Step [240/1751], Loss: 1.1966\n",
      "Epoch [41/100], Step [250/1751], Loss: 1.0349\n",
      "Epoch [41/100], Step [260/1751], Loss: 1.1338\n",
      "Epoch [41/100], Step [270/1751], Loss: 1.2360\n",
      "Epoch [41/100], Step [280/1751], Loss: 1.1976\n",
      "Epoch [41/100], Step [290/1751], Loss: 1.1588\n",
      "Epoch [41/100], Step [300/1751], Loss: 1.4012\n",
      "Epoch [41/100], Step [310/1751], Loss: 1.0466\n",
      "Epoch [41/100], Step [320/1751], Loss: 1.1250\n",
      "Epoch [41/100], Step [330/1751], Loss: 1.1823\n",
      "Epoch [41/100], Step [340/1751], Loss: 1.3342\n",
      "Epoch [41/100], Step [350/1751], Loss: 1.2133\n",
      "Epoch [41/100], Step [360/1751], Loss: 1.1548\n",
      "Epoch [41/100], Step [370/1751], Loss: 1.3389\n",
      "Epoch [41/100], Step [380/1751], Loss: 1.0657\n",
      "Epoch [41/100], Step [390/1751], Loss: 1.2460\n",
      "Epoch [41/100], Step [400/1751], Loss: 1.1537\n",
      "Epoch [41/100], Step [410/1751], Loss: 1.2684\n",
      "Epoch [41/100], Step [420/1751], Loss: 1.2977\n",
      "Epoch [41/100], Step [430/1751], Loss: 1.1141\n",
      "Epoch [41/100], Step [440/1751], Loss: 1.2804\n",
      "Epoch [41/100], Step [450/1751], Loss: 1.2525\n",
      "Epoch [41/100], Step [460/1751], Loss: 1.3006\n",
      "Epoch [41/100], Step [470/1751], Loss: 1.3575\n",
      "Epoch [41/100], Step [480/1751], Loss: 1.3570\n",
      "Epoch [41/100], Step [490/1751], Loss: 1.1898\n",
      "Epoch [41/100], Step [500/1751], Loss: 1.2785\n",
      "Epoch [41/100], Step [510/1751], Loss: 1.2328\n",
      "Epoch [41/100], Step [520/1751], Loss: 1.2235\n",
      "Epoch [41/100], Step [530/1751], Loss: 1.2303\n",
      "Epoch [41/100], Step [540/1751], Loss: 1.1728\n",
      "Epoch [41/100], Step [550/1751], Loss: 1.1673\n",
      "Epoch [41/100], Step [560/1751], Loss: 1.1821\n",
      "Epoch [41/100], Step [570/1751], Loss: 1.1446\n",
      "Epoch [41/100], Step [580/1751], Loss: 1.2148\n",
      "Epoch [41/100], Step [590/1751], Loss: 1.2930\n",
      "Epoch [41/100], Step [600/1751], Loss: 1.0927\n",
      "Epoch [41/100], Step [610/1751], Loss: 1.1152\n",
      "Epoch [41/100], Step [620/1751], Loss: 1.3087\n",
      "Epoch [41/100], Step [630/1751], Loss: 1.2274\n",
      "Epoch [41/100], Step [640/1751], Loss: 1.1308\n",
      "Epoch [41/100], Step [650/1751], Loss: 1.1329\n",
      "Epoch [41/100], Step [660/1751], Loss: 1.2626\n",
      "Epoch [41/100], Step [670/1751], Loss: 1.3318\n",
      "Epoch [41/100], Step [680/1751], Loss: 1.1923\n",
      "Epoch [41/100], Step [690/1751], Loss: 1.2334\n",
      "Epoch [41/100], Step [700/1751], Loss: 1.3358\n",
      "Epoch [41/100], Step [710/1751], Loss: 1.2232\n",
      "Epoch [41/100], Step [720/1751], Loss: 1.2141\n",
      "Epoch [41/100], Step [730/1751], Loss: 1.2870\n",
      "Epoch [41/100], Step [740/1751], Loss: 1.3479\n",
      "Epoch [41/100], Step [750/1751], Loss: 1.2275\n",
      "Epoch [41/100], Step [760/1751], Loss: 1.2199\n",
      "Epoch [41/100], Step [770/1751], Loss: 1.2017\n",
      "Epoch [41/100], Step [780/1751], Loss: 1.2518\n",
      "Epoch [41/100], Step [790/1751], Loss: 1.2731\n",
      "Epoch [41/100], Step [800/1751], Loss: 1.1893\n",
      "Epoch [41/100], Step [810/1751], Loss: 1.3462\n",
      "Epoch [41/100], Step [820/1751], Loss: 1.2459\n",
      "Epoch [41/100], Step [830/1751], Loss: 1.3396\n",
      "Epoch [41/100], Step [840/1751], Loss: 1.3717\n",
      "Epoch [41/100], Step [850/1751], Loss: 1.1007\n",
      "Epoch [41/100], Step [860/1751], Loss: 1.2891\n",
      "Epoch [41/100], Step [870/1751], Loss: 1.0558\n",
      "Epoch [41/100], Step [880/1751], Loss: 1.1332\n",
      "Epoch [41/100], Step [890/1751], Loss: 1.1535\n",
      "Epoch [41/100], Step [900/1751], Loss: 1.0476\n",
      "Epoch [41/100], Step [910/1751], Loss: 1.2004\n",
      "Epoch [41/100], Step [920/1751], Loss: 1.1591\n",
      "Epoch [41/100], Step [930/1751], Loss: 1.1690\n",
      "Epoch [41/100], Step [940/1751], Loss: 1.1656\n",
      "Epoch [41/100], Step [950/1751], Loss: 1.1627\n",
      "Epoch [41/100], Step [960/1751], Loss: 1.1555\n",
      "Epoch [41/100], Step [970/1751], Loss: 1.2311\n",
      "Epoch [41/100], Step [980/1751], Loss: 1.4106\n",
      "Epoch [41/100], Step [990/1751], Loss: 1.1418\n",
      "Epoch [41/100], Step [1000/1751], Loss: 1.1732\n",
      "Epoch [41/100], Step [1010/1751], Loss: 1.2855\n",
      "Epoch [41/100], Step [1020/1751], Loss: 1.2615\n",
      "Epoch [41/100], Step [1030/1751], Loss: 1.2072\n",
      "Epoch [41/100], Step [1040/1751], Loss: 1.2391\n",
      "Epoch [41/100], Step [1050/1751], Loss: 1.1249\n",
      "Epoch [41/100], Step [1060/1751], Loss: 1.4056\n",
      "Epoch [41/100], Step [1070/1751], Loss: 1.1528\n",
      "Epoch [41/100], Step [1080/1751], Loss: 1.1562\n",
      "Epoch [41/100], Step [1090/1751], Loss: 1.2521\n",
      "Epoch [41/100], Step [1100/1751], Loss: 1.2371\n",
      "Epoch [41/100], Step [1110/1751], Loss: 1.3165\n",
      "Epoch [41/100], Step [1120/1751], Loss: 1.1121\n",
      "Epoch [41/100], Step [1130/1751], Loss: 1.2931\n",
      "Epoch [41/100], Step [1140/1751], Loss: 1.2276\n",
      "Epoch [41/100], Step [1150/1751], Loss: 1.1550\n",
      "Epoch [41/100], Step [1160/1751], Loss: 1.3454\n",
      "Epoch [41/100], Step [1170/1751], Loss: 1.1324\n",
      "Epoch [41/100], Step [1180/1751], Loss: 1.2189\n",
      "Epoch [41/100], Step [1190/1751], Loss: 1.1308\n",
      "Epoch [41/100], Step [1200/1751], Loss: 1.3253\n",
      "Epoch [41/100], Step [1210/1751], Loss: 1.3807\n",
      "Epoch [41/100], Step [1220/1751], Loss: 1.1423\n",
      "Epoch [41/100], Step [1230/1751], Loss: 1.2281\n",
      "Epoch [41/100], Step [1240/1751], Loss: 1.3317\n",
      "Epoch [41/100], Step [1250/1751], Loss: 1.2263\n",
      "Epoch [41/100], Step [1260/1751], Loss: 1.1984\n",
      "Epoch [41/100], Step [1270/1751], Loss: 1.3538\n",
      "Epoch [41/100], Step [1280/1751], Loss: 1.2906\n",
      "Epoch [41/100], Step [1290/1751], Loss: 1.1607\n",
      "Epoch [41/100], Step [1300/1751], Loss: 1.1444\n",
      "Epoch [41/100], Step [1310/1751], Loss: 1.2539\n",
      "Epoch [41/100], Step [1320/1751], Loss: 1.2423\n",
      "Epoch [41/100], Step [1330/1751], Loss: 1.2276\n",
      "Epoch [41/100], Step [1340/1751], Loss: 1.3220\n",
      "Epoch [41/100], Step [1350/1751], Loss: 1.2977\n",
      "Epoch [41/100], Step [1360/1751], Loss: 1.1437\n",
      "Epoch [41/100], Step [1370/1751], Loss: 1.2018\n",
      "Epoch [41/100], Step [1380/1751], Loss: 1.2292\n",
      "Epoch [41/100], Step [1390/1751], Loss: 1.2002\n",
      "Epoch [41/100], Step [1400/1751], Loss: 1.1732\n",
      "Epoch [41/100], Step [1410/1751], Loss: 1.2699\n",
      "Epoch [41/100], Step [1420/1751], Loss: 1.1123\n",
      "Epoch [41/100], Step [1430/1751], Loss: 1.1074\n",
      "Epoch [41/100], Step [1440/1751], Loss: 1.2757\n",
      "Epoch [41/100], Step [1450/1751], Loss: 1.3632\n",
      "Epoch [41/100], Step [1460/1751], Loss: 1.1478\n",
      "Epoch [41/100], Step [1470/1751], Loss: 1.0181\n",
      "Epoch [41/100], Step [1480/1751], Loss: 1.2055\n",
      "Epoch [41/100], Step [1490/1751], Loss: 1.3457\n",
      "Epoch [41/100], Step [1500/1751], Loss: 1.3391\n",
      "Epoch [41/100], Step [1510/1751], Loss: 1.3451\n",
      "Epoch [41/100], Step [1520/1751], Loss: 1.2597\n",
      "Epoch [41/100], Step [1530/1751], Loss: 1.1750\n",
      "Epoch [41/100], Step [1540/1751], Loss: 1.1156\n",
      "Epoch [41/100], Step [1550/1751], Loss: 1.2839\n",
      "Epoch [41/100], Step [1560/1751], Loss: 1.2000\n",
      "Epoch [41/100], Step [1570/1751], Loss: 1.1137\n",
      "Epoch [41/100], Step [1580/1751], Loss: 1.3145\n",
      "Epoch [41/100], Step [1590/1751], Loss: 1.1042\n",
      "Epoch [41/100], Step [1600/1751], Loss: 1.2087\n",
      "Epoch [41/100], Step [1610/1751], Loss: 1.2070\n",
      "Epoch [41/100], Step [1620/1751], Loss: 1.2650\n",
      "Epoch [41/100], Step [1630/1751], Loss: 1.3198\n",
      "Epoch [41/100], Step [1640/1751], Loss: 1.2792\n",
      "Epoch [41/100], Step [1650/1751], Loss: 1.1764\n",
      "Epoch [41/100], Step [1660/1751], Loss: 1.1866\n",
      "Epoch [41/100], Step [1670/1751], Loss: 1.2333\n",
      "Epoch [41/100], Step [1680/1751], Loss: 1.2570\n",
      "Epoch [41/100], Step [1690/1751], Loss: 1.1866\n",
      "Epoch [41/100], Step [1700/1751], Loss: 1.0651\n",
      "Epoch [41/100], Step [1710/1751], Loss: 1.1860\n",
      "Epoch [41/100], Step [1720/1751], Loss: 1.1357\n",
      "Epoch [41/100], Step [1730/1751], Loss: 1.1687\n",
      "Epoch [41/100], Step [1740/1751], Loss: 1.0620\n",
      "Epoch [41/100], Step [1750/1751], Loss: 1.3192\n",
      "Epoch [41/100], Average Loss: 1.2199, Time: 1642.3599s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00002\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 20\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Step [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00002)\n",
    "\n",
    "train_model(net, dataloader, criterion, optimizer, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/centar15-desktop1/Desktop/example_data/images-20250210T115939Z-001/images/100.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[219], line 18\u001b[0m\n\u001b[1;32m      8\u001b[0m net \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     10\u001b[0m transform2 \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     11\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),  \n\u001b[1;32m     12\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     16\u001b[0m ])\n\u001b[0;32m---> 18\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolderpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGBA\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Convert the image to RGB (remove alpha channel)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/LPCV_2025_T1/.venv/lib/python3.12/site-packages/PIL/Image.py:3465\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3462\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[1;32m   3464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3465\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3466\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/centar15-desktop1/Desktop/example_data/images-20250210T115939Z-001/images/100.jpg'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage as ski\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "folderpath = \"/home/centar15-desktop1/Desktop/example_data/images-20250210T115939Z-001/images/\"\n",
    "\n",
    "net = net.eval()\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4435, 0.3968, 0.3221], std=[0.2266, 0.2126, 0.2013]),  \n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Lambda(lambda x: x.unsqueeze(0).to(device))\n",
    "])\n",
    "\n",
    "image = Image.open(folderpath + f\"{idx}.jpg\")\n",
    "\n",
    "if image.mode == 'RGBA':\n",
    "    # Convert the image to RGB (remove alpha channel)\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "# image = ski.io.imread(folderpath + f\"{idx}.jpg\")\n",
    "\n",
    "# print(image.shape)\n",
    "\n",
    "# image = image[:,:, :3]\n",
    "# image2 = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "# print(image.shape)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "helper.print_probablities_from_output(net(transform2(image)), utils.GLOBAL_CLASSES, 10, 'V3_Small_trained')\n",
    "\n",
    "idx = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slika: 0\n",
      "Slika: 1000\n",
      "Slika: 2000\n",
      "Slika: 3000\n",
      "Slika: 4000\n",
      "Slika: 5000\n",
      "Slika: 6000\n",
      "Slika: 7000\n",
      "Slika: 8000\n",
      "Slika: 9000\n",
      "Slika: 10000\n",
      "Slika: 11000\n",
      "Slika: 12000\n",
      "Slika: 13000\n",
      "Slika: 14000\n",
      "Slika: 15000\n",
      "Slika: 16000\n",
      "Slika: 17000\n",
      "Slika: 18000\n",
      "Slika: 19000\n",
      "Slika: 20000\n",
      "Slika: 21000\n",
      "Slika: 22000\n",
      "Slika: 23000\n",
      "Accuracy: 0.5976218374343618\n"
     ]
    }
   ],
   "source": [
    "totalPost = 0\n",
    "accuracyPost = 0\n",
    "\n",
    "net = net.eval()\n",
    "\n",
    "dataset = DatasetReader.COCODataset(annotation_file='../../datasets/coco/annotations/instances_val2017.json',\n",
    "    image_dir= '../../datasets/coco/val2017',\n",
    "    target_classes=[s.lower() for s in utils.GLOBAL_CLASSES],\n",
    "    transform=transform)\n",
    "\n",
    "# batch_size = 256\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=11, prefetch_factor=4, persistent_workers=True)\n",
    "\n",
    "\n",
    "for i in range(0, len(dataset)):\n",
    "    totalPost += 1\n",
    "    image, label = dataset[i]\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    label = torch.tensor([label]).to(device)\n",
    "    output = net(image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    # print(f\"Predicted: {predicted.item()}, Actual: {label.item()}\")\n",
    "    if(i % 1000 == 0):\n",
    "        print(f\"Slika: {i}\")\n",
    "    if predicted.item() == label.item():\n",
    "        accuracyPost += 1\n",
    "    \n",
    "\n",
    "print(f\"Accuracy: {accuracyPost/totalPost}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
