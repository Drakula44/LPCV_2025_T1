{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 22:40:40,966 - root - INFO - AIMET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\onnxscript\\converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "<frozen abc>:106: FutureWarning: `NLLLoss2d` has been deprecated. Please use `NLLLoss` instead as a drop-in replacement and see https://pytorch.org/docs/main/nn.html#torch.nn.NLLLoss for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "PreprocessedMobileNetV2(\n",
      "  (mobilenet_v2): MobileNetV2(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (9): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (11): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (12): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (13): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (14): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (15): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (16): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (17): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (18): Conv2dNormActivation(\n",
      "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=1280, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "PreprocessedMobileNetV2(\n",
      "  (mobilenet_v2): MobileNetV2(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (9): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (11): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (12): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (13): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (14): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (15): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (16): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (17): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (18): Conv2dNormActivation(\n",
      "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=1280, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.cross_layer_equalization import equalize_model\n",
    "from torchvision import models\n",
    "import torch\n",
    "import MobileNetV2Pretrained\n",
    "import os\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from utils.input_getter import mug_image_getter\n",
    "\n",
    "# Load a pretrained model\n",
    "\n",
    "image_getter = mug_image_getter()\n",
    "mug_image = image_getter.get_input_torch()\n",
    "print(mug_image.shape)\n",
    "\n",
    "modelOriginal   = MobileNetV2Pretrained.PreprocessedMobileNetV2(64, r\"model/mobilenet_v2_coco.pth\")\n",
    "modelQuantized  = MobileNetV2Pretrained.PreprocessedMobileNetV2(64, r\"model/mobilenet_v2_coco.pth\")\n",
    "\n",
    "modelOriginal.eval()\n",
    "modelQuantized.eval()\n",
    "\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# model = torch.nn.Sequential(\n",
    "#     torch.nn.Flatten(),\n",
    "#     torch.nn.Linear(3 * 224 * 224, 1024),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(1024, 512),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(512, 256),\n",
    "#     torch.nn.ReLU()\n",
    "# )\n",
    "\n",
    "\n",
    "print(modelQuantized)\n",
    "print(modelOriginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 predictions for  on :\n",
      "31 Cup                   78.5%\n",
      "34 Bowl                   6.4%\n",
      "37 Orange                 1.2%\n",
      "33 Spoon                  1.2%\n",
      "41 Donut                  0.9%\n",
      "Top-5 predictions for  on :\n",
      "31 Cup                   78.5%\n",
      "34 Bowl                   6.4%\n",
      "37 Orange                 1.2%\n",
      "33 Spoon                  1.2%\n",
      "41 Donut                  0.9%\n"
     ]
    }
   ],
   "source": [
    "from utils import helper\n",
    "from dataset import utils as dsutils\n",
    "\n",
    "helper.print_probablities_from_output(modelOriginal(mug_image), dsutils.GLOBAL_CLASSES)\n",
    "helper.print_probablities_from_output(modelQuantized(mug_image), dsutils.GLOBAL_CLASSES)\n",
    "\n",
    "# print(modelOriginal(mug_image))\n",
    "# print(modelQuantized(mug_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLE Started\n",
      "2025-02-18 22:40:51,953 - ConnectedGraph - WARNING - Unable to isolate model outputs.\n",
      "Cross-Layer Equalization applied successfully!\n",
      "PreprocessedMobileNetV2(\n",
      "  (mobilenet_v2): MobileNetV2(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (10): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (11): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (12): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (13): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (14): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (15): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (16): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (17): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (18): Conv2dNormActivation(\n",
      "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=1280, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_num = 0\n",
    "\n",
    "# print(list(model.parameters())[param_num])\n",
    "# print(list(model.parameters())[param_num].shape)\n",
    "\n",
    "# Apply Cross-Layer Equalization (CLE)\n",
    "# This modifies the model in place\n",
    "\n",
    "input_shape = (1, 3, 224, 224)\n",
    "dummy_input = torch.randn(input_shape)\n",
    "\n",
    "print(\"CLE Started\")\n",
    "\n",
    "equalize_model(modelQuantized, dummy_input = dummy_input)\n",
    "\n",
    "print(\"Cross-Layer Equalization applied successfully!\")\n",
    "\n",
    "print(modelQuantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 predictions for  on :\n",
      "31 Cup                   78.5%\n",
      "34 Bowl                   6.4%\n",
      "37 Orange                 1.2%\n",
      "33 Spoon                  1.2%\n",
      "41 Donut                  0.9%\n",
      "Top-5 predictions for  on :\n",
      "31 Cup                   94.5%\n",
      "34 Bowl                   1.8%\n",
      "35 Banana                 0.4%\n",
      "37 Orange                 0.3%\n",
      "33 Spoon                  0.2%\n"
     ]
    }
   ],
   "source": [
    "helper.print_probablities_from_output(modelOriginal(mug_image), dsutils.GLOBAL_CLASSES)\n",
    "helper.print_probablities_from_output(modelQuantized(mug_image), dsutils.GLOBAL_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix posle nemam pojma kako radi\n",
    "\n",
    "# from aimet_torch.bias_correction import correct_bias\n",
    "# import torch\n",
    "\n",
    "# # Define your model (example: ResNet18)\n",
    "# model = models.resnet18(pretrained=True)\n",
    "\n",
    "# # Define a dummy dataset (batch size = 1, shape = [1, 3, 224, 224])\n",
    "# dummy_input = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "# # Apply Bias Correction\n",
    "# correct_bias(model)\n",
    "\n",
    "# print(\"Bias Correction applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 22:41:04,014 - ConnectedGraph - WARNING - Unable to isolate model outputs.\n",
      "2025-02-18 22:41:04,087 - Quant - INFO - No config file provided, defaulting to config file at c:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\aimet_common\\quantsim_config\\default_config.json\n",
      "2025-02-18 22:41:04,107 - Quant - INFO - Unsupported op type Squeeze\n",
      "2025-02-18 22:41:04,108 - Quant - INFO - Unsupported op type Mean\n",
      "2025-02-18 22:41:04,114 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n",
      "23043\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.quantsim import QuantizationSimModel\n",
    "from aimet_common.defs import QuantScheme\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dummy input to define the model input size\n",
    "dummy_input = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "# Step 1: Create QuantizationSimModel\n",
    "sim = QuantizationSimModel(modelQuantized, dummy_input=dummy_input,\n",
    "                                     quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                                     default_param_bw=8, default_output_bw=8)\n",
    "\n",
    "from dataset import DatasetReader\n",
    "\n",
    "datasetCOCO = DatasetReader.COCODataset(\n",
    "    annotation_file = r\"C:/Users/Pro/Documents/___FAKULTET___/LPCV/LPCV_2025_T1/data/annotations/annotations/instances_val2017.json\", \n",
    "    image_dir= r'C:/Users/Pro/Documents/___FAKULTET___/LPCV/LPCV_2025_T1/data/val2017/val2017',\n",
    "    target_classes=[s.lower() for s in dsutils.GLOBAL_CLASSES],\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]))\n",
    "\n",
    "dataloader = DataLoader(datasetCOCO, batch_size=1, shuffle=True)\n",
    "\n",
    "print(len(dataloader))\n",
    "\n",
    "\n",
    "# Step 2: Compute Encodings (calibration)\n",
    "def calibration_function(model, eval_iterations = 50, use_cuda = False):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        image, target = data\n",
    "        model(image)\n",
    "        if (i > eval_iterations):\n",
    "            break\n",
    "    print(eval_iterations)\n",
    "    print(\"cal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreprocessedMobileNetV2(\n",
      "  (mobilenet_v2): MobileNetV2(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): QuantizedConv2d(\n",
      "          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (1): Identity()\n",
      "        (2): QuantizedReLU(\n",
      "          (param_quantizers): ModuleDict()\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedConv2d(\n",
      "            32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              16, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            96, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            144, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            144, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (10): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (11): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (12): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (13): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (14): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 160, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (15): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (16): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (17): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 320, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (18): Conv2dNormActivation(\n",
      "        (0): QuantizedConv2d(\n",
      "          320, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (1): Identity()\n",
      "        (2): QuantizedReLU(\n",
      "          (param_quantizers): ModuleDict()\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): QuantizedDropout(\n",
      "        p=0.2, inplace=False\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (1): QuantizedLinear(\n",
      "        in_features=1280, out_features=64, bias=True\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(sim.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.compute_encodings(calibration_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sim.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.model(mug_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 22:41:21,216 - Utils - INFO - successfully created onnx model with 88/100 node names updated\n",
      "2025-02-18 22:41:21,294 - Quant - WARNING - The following layers were not found in the exported onnx model. Encodings for these layers will not appear in the exported encodings file, however it will continue to exist in torch encoding file:\n",
      "['mobilenet_v2.classifier.0']\n",
      "This can be due to several reasons:\n",
      "\t- The layer is set to quantize with float datatype, but was not exercised in compute encodings. Not an issue if the layer is not meant to be run.\n",
      "\t- The layer has valid encodings but was not seen while exporting to onnx using the dummy input provided in sim.export(). Ensure that the dummy input covers all layers.\n",
      "2025-02-18 22:41:21,295 - Quant - INFO - Layers excluded from quantization: []\n",
      "2025-02-18 22:41:21,296 - Quant - WARNING - \u001b[31;21mQuantsim export will stop exporting encodings for saving and loading in a future AIMET release.\n",
      "To export encodings for saving and loading, use QuantizationSimModel's save_encodings_to_json() utility instead.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Step 3: Export Quantized Model\n",
    "os.makedirs('quantized', exist_ok=True)\n",
    "sim.export(path='quantized', filename_prefix='ref_solution_quantized', dummy_input=dummy_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sim.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.model.mobilenet_v2.features[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.model(mug_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.print_probablities_from_output(sim.model(mug_image), dsutils.GLOBAL_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
